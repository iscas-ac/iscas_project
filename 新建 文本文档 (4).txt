{
  "data": {
    "repository": {
      "databaseId": 156018,
      "discussions": {
        "totalCount": 223,
        "nodes": [
          {
            "url": "https://github.com/redis/redis/discussions/13881",
            "number": 13881,
            "title": "Why does Pipeline.close() delete the connection?",
            "body": "I have a class written to interact with my Redis DB. In it, I thought about using a method for session handling (as I would do with other DBs) (exception handling is handled elsewhere):\r\n\r\n```\r\n    @contextlib.contextmanager\r\n    def _make_session(self) -> Generator[redis.client.Pipeline]:\r\n        session = self.client.pipeline()\r\n        try:\r\n            yield session\r\n        finally:\r\n            session.close()\r\n```\r\n\r\nImagine that I perform a **with** istruction to open the Pipeline, running one or more commands and executing them. e.g.:\r\n\r\n```\r\n            with self._make_session() as session:\r\n                return method(self, session, *args, **kwargs)\r\n```\r\n\r\nI noticed that the code above only works the first time in my backend, the latters raise a NoneType error. So I looked into the Pipeline code and found out that **close** calls the **reset** method that has this part at the end:\r\n\r\n```\r\n        if self.connection:\r\n            self.connection_pool.release(self.connection)\r\n            self.connection = None\r\n```\r\n\r\nWas this done for some reason in particular? It seems like that since the connection closes, from the second time I create a session it simply points to None. I had to quickly fix it like this:\r\n\r\n```\r\n    @contextlib.contextmanager\r\n    def _make_session(self) -> Generator[redis.client.Pipeline]:\r\n        yield self.client.pipeline()\r\n```\r\n\r\nDo you have any ideas of why?",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": "RESOLVED",
            "createdAt": "2025-03-25T05:41:27Z",
            "updatedAt": "2025-03-25T06:12:45Z",
            "closedAt": "2025-03-25T06:12:45Z",
            "closed": true,
            "author": {
              "login": "Frank995",
              "__typename": "User"
            },
            "category": {
              "name": "General",
              "description": "Chat about anything and everything here",
              "slug": "general"
            },
            "isAnswered": null,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "please move this issue to https://github.com/redis/redis-py/issues",
                  "createdAt": "2025-03-25T06:12:11Z",
                  "updatedAt": "2025-03-25T06:12:12Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13849",
            "number": 13849,
            "title": "What does the replicas never free the backlog for timeout mean?",
            "body": "Hi team.\r\nI noticed the comment in the Redis config file. I can't get the replicas never free the backlog for timeout.\r\nI think the replica should never have the backlog buffer. Am I right? The replica will not be replica after the failover.\r\n\r\n```c\r\n# After a master has no connected replicas for some time, the backlog will be\r\n# freed. The following option configures the amount of seconds that need to\r\n# elapse, starting from the time the last replica disconnected, for the backlog\r\n# buffer to be freed.\r\n#\r\n# Note that replicas never free the backlog for timeout, since they may be\r\n# promoted to masters later, and should be able to correctly \"partially\r\n# resynchronize\" with other replicas: hence they should always accumulate backlog.\r\n#\r\n# A value of 0 means to never release the backlog.\r\n#\r\n# repl-backlog-ttl 3600\r\n```\r\nThanks for the answer in advance.",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2025-03-10T09:00:54Z",
            "updatedAt": "2025-03-11T07:11:39Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "hnzhrh",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "i think The following sentence explains the reason\r\n\r\n>  Note that replicas never free the backlog for timeout, **since they may be**\r\n>  **promoted to masters later, and should be able to correctly \"partially**\r\n>  **resynchronize\" with other replicas: hence they should always accumulate backlog.**\r\n",
                  "createdAt": "2025-03-11T07:11:39Z",
                  "updatedAt": "2025-03-11T07:11:39Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "ShooterIT",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13826",
            "number": 13826,
            "title": "Rethinking the leader election process",
            "body": "We know that the current leader election is as following: https://redis.io/docs/latest/operate/oss_and_stack/reference/cluster-spec/ (Replica election and promotion). I have an idea about this stage, but I don't make sure whether I miss something or not. And I would give the idea in the following:\r\n**Masters reply to replica vote request**\r\n1. A replica would send the election request to the master with epoch, and **a random number**. And if other conditions is same, we think the replica with a bigger random number have a higher priority.\r\n2. A master only votes a single time for a given epoch, and refuses to vote for older epochs: every master has a lastVoteEpoch field and will refuse to vote again as long as the currentEpoch in the auth request packet is not greater than the lastVoteEpoch. When a master replies positively to a vote request, the lastVoteEpoch is updated accordingly, and safely stored on disk. But there is an exception. If a master vote for a replica, but the replica send a message to a master that the replica give up the election for currentEpoch, then the master could vote for another replica, which with a higher priority.\r\n3. When a replica send a election request to a master, and the master have voted. The master would tell it which replica gets its vote. If a replica have found that more than half of the master nodes have voted to other replicas and there are some replicas with higher priority. The replica would send messages to all the masters, who vote for the replica and tell them the replica give up the election for currentEpoch, and tells them the replica with the highest number, who also want to win the election, then these master would vote for the highest priority replica. \r\n4. The masters, who receive the give up message, would compare the priority that it receives from the give up replica with the replica election requests they receive. And then these masters would vote for the replicas with highest priority again. \r\n5. The replica, who receives the vote for more than one half of masters will win the election.\r\n6. A master votes for a replica only if the replica's master is flagged as FAIL.\r\n7. Auth requests with a currentEpoch that is less than the master currentEpoch are ignored. Because of this the master reply will always have the same currentEpoch as the auth request. If the same replica asks again to be voted, incrementing the currentEpoch, it is guaranteed that an old delayed reply from the master can not be accepted for the new vote.\r\n\r\n\r\nThanks for the opinions about the ideas before. ",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2025-02-24T02:26:15Z",
            "updatedAt": "2025-03-08T03:49:55Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "ss-torres",
              "__typename": "User"
            },
            "category": {
              "name": "General",
              "description": "Chat about anything and everything here",
              "slug": "general"
            },
            "isAnswered": null,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "Hi, what's the problem you're trying to solve, i.e. do you encounter some issues with current version?",
                  "createdAt": "2025-02-26T03:49:40Z",
                  "updatedAt": "2025-02-26T03:49:41Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "ShooterIT",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "Hi, @ShooterIT. For Raft Protocol, there are some cases that no candidate could get the majority votes. For example, there are five candidates, the two of the candidates get two votes, and one candidate get one vote. Then no candidate could get the majority votes. I have an idea that may help to get the majority votes in this case, so that it could speed up the election progress. I want to know whether this idea is taking into account all aspects?",
                        "createdAt": "2025-02-26T04:11:51Z",
                        "updatedAt": "2025-02-26T04:11:51Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "ss-torres",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                },
                {
                  "body": "@ShooterIT , you could refer to the following description.\r\n\r\nFor the Raft algorithm, the leader election process is as follows:\r\n1. Node Roles\r\nNodes in the Raft algorithm have three roles:\r\n(1) Leader: Responsible for handling client requests, managing log replication, and sending heartbeats.\r\n(2) Follower: Passively accepts logs and heartbeats from the Leader and does not initiate requests.\r\n(3) Candidate: During the election process, a Follower can transition to a Candidate and initiate an election.\r\n\r\n2. Election Trigger Conditions\r\nLeader elections are typically triggered under the following conditions:\r\n(1) Leader failure: A Follower does not receive a heartbeat from the Leader within a certain period (election timeout).\r\n(2) New node joining: A newly joined node may trigger an election.\r\n(3) Partition recovery: An election may be triggered after network partition recovery.\r\n\r\n3. Election Process\r\nThe election process consists of the following steps:\r\n(1) Follower Transitions to Candidate\r\n1.\tWhen a Follower does not receive a heartbeat from the Leader within the election timeout period, it considers the Leader to have failed.\r\n2.\tThe Follower transitions its role to Candidate and initiates a new election.\r\n(2) Candidate Initiates Voting\r\n1.\tThe Candidate first increments its term number (Term) by 1, indicating a new round of elections.\r\n2.\tThe Candidate sends a RequestVote RPC to other nodes in the cluster to request votes.\r\n3.\tThe Candidate votes for itself.\r\n(3) Other Nodes Vote\r\n1.\tUpon receiving the RequestVote RPC, nodes check the following conditions:\r\n•\tWhether the Candidate's Term is greater than or equal to their own Term.\r\n•\tWhether they have already voted for another Candidate.\r\n•\tWhether the Candidate's log is at least as up-to-date as their own.\r\n2.\tIf the conditions are met, the node votes for the Candidate and resets its election timeout.\r\n(4) Election Result\r\n1.\tIf the Candidate receives votes from a majority of nodes, it becomes the new Leader.\r\n2.\tThe new Leader immediately sends heartbeats to other nodes to prevent them from initiating new elections.\r\n(5) Election Failure\r\n1.\tIf the Candidate does not receive enough votes within the election timeout period, the election fails.\r\n2.\tThe Candidate waits for a random period before re-initiating the election (to avoid split votes caused by multiple nodes initiating elections simultaneously).\r\n\r\n4. Election Timeout\r\n(1) To prevent multiple nodes from initiating elections simultaneously, Raft uses randomized election timeout periods (typically 150ms-300ms).\r\n(2) Randomized timeout periods reduce the probability of election conflicts.\r\n\r\n5. Log Consistency Guarantee\r\n(1) During the election process, Raft ensures that only nodes with sufficiently up-to-date logs can become Leaders by comparing the Term and Index of logs.\r\n(2) This guarantees that the Leader's log contains all committed log entries, ensuring consistency.\r\n\r\n6. Election Optimization\r\n(1) Pre-Vote Mechanism: Before formally initiating an election, a Candidate can first send a Pre-Vote request to confirm whether it has a chance to win the election, avoiding unnecessary Term increments.\r\n(2) Leader Transfer: In certain situations, the Leader can proactively transfer leadership to another node to avoid frequent elections.\r\n\r\n\r\nModified Election Process:\r\nThe election process is divided into the following steps:\r\n(1) Follower Transitions to Candidate (unchanged)\r\n1.\tWhen a Follower does not receive a heartbeat from the Leader within the election timeout period, it considers the Leader to have failed.\r\n2.\tThe Follower transitions its role to Candidate and initiates a new election.\r\n(2) Candidate Initiates Voting (modified)\r\n1.\tThe Candidate first increments its term number (Term) by 1 and generates a random int64 number, randPriority, indicating a new round of elections.\r\n2.\tThe Candidate sends a RequestVote RPC to other nodes in the cluster (this RPC request carries the random number randPriority) to request votes.\r\n3.\tThe Candidate votes for itself.\r\n(3) Other Nodes Vote\r\n1.\tUpon receiving the RequestVote RPC, nodes check the following conditions:\r\n•\tWhether the Candidate's Term is greater than or equal to their own Term.\r\n•\tWhether they have already voted for another Candidate.\r\n•\tWhether the Candidate's log is at least as up-to-date as their own.\r\n2.\tIf the conditions are met, the node votes for the Candidate and resets its election timeout.\r\n3.\tUpon receiving the RequestVote RPC, nodes record the maximum value maxNodePriority (first by log offset, then by term, then by the random number randPriority) among Candidate nodes that meet the following conditions:\r\n•\tThe Candidate's Term is greater than or equal to their own Term.\r\n•\tThe Candidate's log offset is at least as up-to-date as their own.\r\n4.\tIf a node refuses to vote for the Candidate (conditions not met or already voted for another node), it will return a message informing the Candidate.\r\n(4) Voting Result (Receiving Rejected Votes)\r\n1.\tIf the Candidate receives rejected vote messages and the following conditions are met:\r\n•\tIf the message informs the voting node that the log is outdated,\r\n•\tIf the message informs the voting node that it has already voted for another node, then the Candidate node will record these nodes in a list (VoteOtherList) until the number of rejected nodes exceeds a majority (meaning the node cannot be elected according to the standard Raft algorithm, in which case it needs to self-rescue). If the Candidate's nodePriority is less than the recorded maxNodePriority or less than the maximum nodePriority in VoteOtherList, the node will initiate a specific node re-voting process.\r\n2.\tSpecific Node Re-Voting Process:\r\n•\tActively relinquish the Candidate role for this round.\r\n•\tSend a message to all nodes that voted for it, requesting them to re-vote (the message includes the Candidate node with the maximum nodePriority recorded, obtained from VoteOtherList and maxNodePriority).\r\n•\tUpon receiving this message, nodes compare the maxNodePriority in the message with the locally recorded maxNodePriority and send a vote to the node with the larger value.\r\n•\tThe specific node re-voting process aims to concentrate votes on the node with the highest nodePriority, thereby completing the election process.\r\n(5) Voting Result (Receiving an Accepted Vote)\r\n1.\tIf the Candidate receives votes from a majority of nodes, it becomes the new Leader.\r\n2.\tThe new Leader immediately sends heartbeats to other nodes to prevent them from initiating new elections.\r\n\r\n(6) Election Failure\r\n1.\tIf the Candidate does not receive enough votes within the election timeout period, the election fails.\r\n2.\tThe Candidate waits for a random period before re-initiating the election.\r\n\r\n\r\nLeader Election Examples\r\nExample 1: Raft Election Succeeds in One Round\r\nAssume a 5-node cluster (A, B, C, D, E):\r\n1.\tInitially, A is the Leader, and the other nodes are Followers.\r\n2.\tAfter A fails, B and C's election timeout expires, and they transition to Candidates.\r\n3.\tB and C initiate elections, sending RequestVote RPCs to other nodes.\r\n4.\tAssume B's log is more up-to-date than C's, and D and E vote for B.\r\n5.\tB receives 3 votes (including its own) and becomes the new Leader.\r\n\r\nExample 2: Raft Election Fails in One Round\r\n1.\tInitially, A is the Leader, and the other nodes are Followers.\r\n2.\tAfter A fails, B, C, D, and E's election timeout expires, and they transition to Candidates.\r\n3.\tB, C, D, and E initiate elections, sending RequestVote RPCs to other nodes. B, C, D, and E receive random numbers 100, 90, 80, and 70, respectively.\r\n4.\tAssume B, C, D, and E each receive 1 vote. Since C, D, and E each receive three rejected vote messages (exceeding a majority), these three nodes relinquish their Candidate roles for this round and send messages to themselves (these three nodes vote for themselves), instructing them to transfer their votes to node B.\r\n5.\tNode B receives 4 votes, exceeding a majority, and completes the election process.\r\n\r\nExample 3: Another Raft Scenario\r\n1.\tInitially, A is the Leader, and the other nodes are Followers.\r\n2.\tAfter A fails, B and C's election timeout expires, and they transition to Candidates.\r\n3.\tB and C initiate elections, sending RequestVote RPCs to other nodes, with random numbers 900 and 1000, respectively.\r\n4.\tAssume B and C have the same log offset and receive votes from D and E, respectively. Here, a heuristic approach can be considered: since node A cannot vote, both B and C believe that under the existing Raft algorithm logic, neither will become the Leader. Since node C has a larger offset, node C will not initiate specific node re-voting. Node B will relinquish its Candidate role for this round and instruct nodes B and D, which voted for it, to transfer their votes to node C.\r\n5.\tNode C receives 4 votes, exceeding a majority, and completes the election process.\r\n\r\nCorrectness of the Idea\r\nAt any moment, each node will only vote for one Candidate, so at any moment, there is at most one node with votes from a majority of nodes.\r\n\r\nEfficiency\r\nUnder normal Raft operation, the new modifications have no impact. However, when the Raft algorithm cannot elect a Leader in one election round, the new modifications may facilitate the election of a Leader.\r\n",
                  "createdAt": "2025-03-08T03:49:55Z",
                  "updatedAt": "2025-03-08T04:38:10Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "ss-torres",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13844",
            "number": 13844,
            "title": "Modify command to run on keys with pattern",
            "body": "Hi\r\n\r\nI want to modify a few set of commands so that it runs on the keys matching the pattern given as the key argument. I am looking at the begin_search and find_keys for those commands, but not sure how can I modify the json file so that the command behavior changes from a single key to pattern matched keys.\r\n\r\nThanks\r\nmwahal",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2025-03-05T19:48:35Z",
            "updatedAt": "2025-03-07T01:42:32Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "mwahal",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "you can refer to set.json and mset.json\r\n```\r\n# mset\r\n            {\r\n                \"flags\": [\r\n                    \"OW\",\r\n                    \"UPDATE\"\r\n                ],\r\n                \"begin_search\": {\r\n                    \"index\": {\r\n                        \"pos\": 1  <- first key position\r\n                    }\r\n                },\r\n                \"find_keys\": {\r\n                    \"range\": {\r\n                        \"lastkey\": -1, <- no limit\r\n                        \"step\": 2, <- second key: 1+2, third key: 1+2+2, ....\r\n                        \"limit\": 0\r\n                    }\r\n                }\r\n            }\r\n```\r\n```\r\n#set\r\n            {\r\n                \"begin_search\": {\r\n                    \"index\": {\r\n                        \"pos\": 1\r\n                    }\r\n                },\r\n                \"find_keys\": {\r\n                    \"range\": {\r\n                        \"lastkey\": 0, <- only one key\r\n                        \"step\": 1,\r\n                        \"limit\": 0\r\n                    }\r\n                }\r\n            }\r\n```",
                  "createdAt": "2025-03-06T01:25:15Z",
                  "updatedAt": "2025-03-06T01:25:17Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "Hello @sundb, thank you for the reply.\r\n\r\nActually, I didn't explain properly. I am not looking to pass multiple keys on the command line. I want to pass a pattern instead of a key and the operating function on the server should find all the matching keys and operate on each one. \r\n\r\nSay for example, I have done these operations\r\nSET user_key_1 \"1\"\r\nSET user_key_2 \"2\"\r\nSET user_key_3 \"3\"\r\n\r\nNow if I want to increment each one of them, I have to call\r\nINCR user_key_1\r\nINCR user_key_2\r\nINCR user_key_3\r\n\r\nWhat I want to do is call\r\nINCR user_key_ \r\n\r\nAnd the server should fetch all the matching keys with user_key_* and run INCR operation on it.\r\nBasically, a SCAN for pattern and then INCR in a for loop.\r\n\r\nThanks\r\nMudit",
                        "createdAt": "2025-03-06T23:19:22Z",
                        "updatedAt": "2025-03-06T23:19:23Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "mwahal",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "If so, you can use the original incr.json, because the json of the new command has not changed, you can just modify the INCR command to support what you want.",
                        "createdAt": "2025-03-07T01:42:32Z",
                        "updatedAt": "2025-03-07T01:42:33Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13743",
            "number": 13743,
            "title": "CVE-2024-46981 ，Does redis5.0.14 involve this vulnerability?",
            "body": "Does redis5.0.14 involve this vulnerability?",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2025-01-14T00:55:54Z",
            "updatedAt": "2025-02-26T00:08:40Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "xinsibo",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "@xinsibo yes, you can manually patch your own version if you need to.",
                  "createdAt": "2025-01-14T01:06:30Z",
                  "updatedAt": "2025-01-14T01:06:31Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "In scripting.c between  \"server.lua_scripts_mem = 0\" and \"lua_close(server.lua)\" add     \"lua_gc(server.lua, LUA_GCCOLLECT, 0)\"?",
                        "createdAt": "2025-01-14T01:09:47Z",
                        "updatedAt": "2025-01-14T01:09:48Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "xinsibo",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "yes, it's the only place.",
                        "createdAt": "2025-01-14T01:14:02Z",
                        "updatedAt": "2025-01-14T01:14:03Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Is the repair solution for redis 4.0 the same?",
                        "createdAt": "2025-01-14T01:52:50Z",
                        "updatedAt": "2025-01-14T01:52:50Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "xinsibo",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "yes.",
                        "createdAt": "2025-01-14T01:53:40Z",
                        "updatedAt": "2025-01-14T01:53:41Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "> Is the repair solution for redis 4.0 the same?\r\n\r\nIt seems that Redis@4.0.8 already has a periodic lua gc.   \r\n\r\nhttps://github.com/redis/redis/blob/4.0.8/src/scripting.c#L1371\r\n```\r\n /* Call the Lua garbage collector from time to time to avoid a\r\n     * full cycle performed by Lua, which adds too latency.\r\n     *\r\n     * The call is performed every LUA_GC_CYCLE_PERIOD executed commands\r\n     * (and for LUA_GC_CYCLE_PERIOD collection steps) because calling it\r\n     * for every command uses too much CPU. */\r\n    #define LUA_GC_CYCLE_PERIOD 50\r\n    {\r\n        static long gc_count = 0;\r\n\r\n        gc_count++;\r\n        if (gc_count == LUA_GC_CYCLE_PERIOD) {\r\n            lua_gc(lua,LUA_GCSTEP,LUA_GC_CYCLE_PERIOD);\r\n            gc_count = 0;\r\n        }\r\n    }\r\n```",
                        "createdAt": "2025-02-25T03:47:42Z",
                        "updatedAt": "2025-02-25T03:47:58Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "weimengxi",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@weimengxi please note that this period GC is not in the timer, but in the eval command, so `lua_close` also can be called without completing GC.",
                        "createdAt": "2025-02-26T00:08:40Z",
                        "updatedAt": "2025-02-26T00:08:41Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13808",
            "number": 13808,
            "title": "Red Lock Algorithm Rethinking",
            "body": "In order to acquire the lock, the client performs the following operations:\r\n\r\n1. It gets the current time in milliseconds.\r\n2. It tries to acquire the lock in all the N instances sequentially, using the same key name and random value in all the instances. During step 2, when setting the lock in each instance, the client uses a timeout which is small compared to the total lock auto-release time in order to acquire it. For example if the auto-release time is 10 seconds, the timeout could be in the ~ 5-50 milliseconds range. This prevents the client from remaining blocked for a long time trying to talk with a Redis node which is down: if an instance is not available, we should try to talk with the next instance ASAP.\r\n3. The client computes how much time elapsed in order to acquire the lock, by subtracting from the current time the timestamp obtained in step 1. If and only if the client was able to acquire the lock in the majority of the instances (at least 3), and the total time elapsed to acquire the lock is less than lock validity time, the lock is considered to be acquired.\r\n4. If the lock was acquired, its validity time is considered to be the initial validity time minus the time elapsed, as computed in step 3.\r\n5. If the client failed to acquire the lock for some reason (either it was not able to lock N/2+1 instances or the validity time is negative), it will try to unlock all the instances (even the instances it believed it was not able to lock).\r\n\r\n**The above five steps are how we should do to acquire a red lock. As what I know, we need to acquire all the N instances sequentially to avoid the dead lock. If we just want to avoid the dead lock, we could acquire the first N/2 locks sequentially, and we could get the last locks simultaneously. Because we have get the first N/2 locks, then if we could get one lock again, then we could get the red lock.**\r\n\r\nIt is nice for you to give me some opinions about this. And welcome to talk about it. ",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2025-02-17T01:58:38Z",
            "updatedAt": "2025-02-17T02:02:41Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "ss-torres",
              "__typename": "User"
            },
            "category": {
              "name": "General",
              "description": "Chat about anything and everything here",
              "slug": "general"
            },
            "isAnswered": null,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": []
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13801",
            "number": 13801,
            "title": "Is there a way to run unit tests (or any tests individually) without triggering the whole test suite?",
            "body": "I am very new to the redis project and I am trying to play with it. But everytime I run make tests it take a long time. I am wondering if it is possible to run individual tests?",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2025-02-13T23:44:34Z",
            "updatedAt": "2025-02-14T00:58:30Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "rgopikrishnan91",
              "__typename": "User"
            },
            "category": {
              "name": "General",
              "description": "Chat about anything and everything here",
              "slug": "general"
            },
            "isAnswered": null,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "you can use `--single` and `--only`.",
                  "createdAt": "2025-02-14T00:47:01Z",
                  "updatedAt": "2025-02-14T00:47:03Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "Thanks for your response. Would you mind giving an example that I can try against? \r\n`./runtest --host <host> --port <port> --tags -needs:repl`\r\n\r\nIn this would it be just about adding \r\n\r\n`./runtest --host <host> --port <port> --tags -needs:repl --single TEST_NAME --only` \r\n\r\nOr would it be different?\r\n",
                        "createdAt": "2025-02-14T00:53:39Z",
                        "updatedAt": "2025-02-14T00:53:40Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "rgopikrishnan91",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "e.g. `./runtest --host <host> --port <port> --tags -needs:repl --single unit/type/hash --only \"HSET/HLEN - Big hash creation\"`",
                        "createdAt": "2025-02-14T00:58:30Z",
                        "updatedAt": "2025-02-14T00:58:31Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13786",
            "number": 13786,
            "title": "can a redis cluster with replicas roll-restart seamlessly ?",
            "body": "hello!\r\ni have a 7.4.1-redis cluster composed of 3 master nodes and 3 replicas, each master has one replica.\r\nWe have a php app connected to the cluster through tls using phpredis extension.\r\nWhen i trigger the command\r\n```\r\ncluster failover\r\n```\r\non one replica node, i notice every time the php app shows error like\r\n```\r\nTimed out attempting to find data in the correct node!\r\n```\r\nhowever the timeouts configured on the app are like 2/3 seconds, the cluster has only ~7000keys, the election is nearly instant.\r\nSo is a cluster failover supposed to generate errors towards their client apps because it is not as seamless as i thought or is it just a driver/extension issue like a bad config in phpredis ou bad config within the app ?",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2025-02-04T18:41:13Z",
            "updatedAt": "2025-02-07T14:02:58Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "ginolegigot",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "could you show redis(both master and replica) log when you executed `cluster failover`?",
                  "createdAt": "2025-02-06T01:22:45Z",
                  "updatedAt": "2025-02-06T01:22:46Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "ShooterIT",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "hello!\r\nThanks for your reply, here the logs of an old master (the master before failover) (i masked the ip:6379) :\r\n![old_node_failover_logs](https://github.com/user-attachments/assets/53bde012-ff65-4476-891c-7a5ed2af2f40)\r\n\r\nHere the logs of the former replica/the new master (the master after failover):\r\n![new_node_failover_logs](https://github.com/user-attachments/assets/2c914abf-a5a4-4727-a728-2dc15a47e2c0)\r\n",
                  "createdAt": "2025-02-06T10:17:42Z",
                  "updatedAt": "2025-02-06T10:18:18Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "ginolegigot",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "from the logs, failover is very fast, only tens of milliseconds.\r\n\r\ni found a fix https://github.com/phpredis/phpredis/pull/2459 in phpredis repo, maybe you can have a look, what's you phpredis version",
                  "createdAt": "2025-02-06T14:15:05Z",
                  "updatedAt": "2025-02-06T14:15:06Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "ShooterIT",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "it's phpredis 6.1.0 so i dont think this works, and it's only when the failover is triggered rest of the time we have no issues",
                  "createdAt": "2025-02-06T14:30:24Z",
                  "updatedAt": "2025-02-06T14:30:24Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "ginolegigot",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "what's your `timeout` configuration",
                  "createdAt": "2025-02-06T14:43:10Z",
                  "updatedAt": "2025-02-06T14:43:10Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "ShooterIT",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "Thanks for your reply,\r\nOriginally the app user had cluster|slots and cluster|info permissions which were sufficient for the proper app run. I added the whole cluster rights to test some things",
                  "createdAt": "2025-02-06T15:11:11Z",
                  "updatedAt": "2025-02-10T10:40:26Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "ginolegigot",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "sorry, maybe I didn't express it clearly, i want to know the timeout configuration in your client side, i.e. phpredis configuration\r\n",
                  "createdAt": "2025-02-07T01:23:13Z",
                  "updatedAt": "2025-02-07T01:23:14Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "ShooterIT",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "Oki sorry i thought i had written it before so i inferred you asked the cluster timeout, in phpredis configuration we set timeout à 3s and read_timeout at 3s",
                  "createdAt": "2025-02-07T10:04:47Z",
                  "updatedAt": "2025-02-07T10:05:02Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "ginolegigot",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "if you want to take a look i also created an issue in phpredis here https://github.com/phpredis/phpredis/issues/2620",
                  "createdAt": "2025-02-07T14:02:58Z",
                  "updatedAt": "2025-02-07T14:02:59Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "ginolegigot",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13791",
            "number": 13791,
            "title": "What's the ETA for 8.0?",
            "body": "Cannot wait to tryout https://github.com/redis/redis/pull/13558",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2025-02-06T19:05:37Z",
            "updatedAt": "2025-02-07T05:58:52Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "biran0083",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "We just released M03 (third milestone) with #13558, so you can experiment with it.\r\n\r\nYou can read about our release lifecycle [here](https://github.com/redis/redis/discussions/13735#discussioncomment-11789880).\r\n\r\nSorry, but we don't publish ETAs for releases.\r\n\r\n\r\n\r\n",
                  "createdAt": "2025-02-07T05:58:52Z",
                  "updatedAt": "2025-02-07T05:58:53Z",
                  "authorAssociation": "MEMBER",
                  "author": {
                    "login": "LiorKogan",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13784",
            "number": 13784,
            "title": "Can CLIENT TRACKING ON differentiate between TTL expiring and DEL commands?",
            "body": "```\r\nCLIENT TRACKING on\r\nPSUBSCRIBE __redis__:*\r\n```\r\n\r\nWhen a redis entry is e.g. `DEL MY_KEY` or the TTL expires, the same push message is sent to redis clients.\r\n\r\n```\r\n1) \"invalidate\"\r\n2) 1) \"MY_KEY\"\r\n```\r\n\r\nIs there a way to differentiate when a command `DEL`'d the entry vs when `TTL` of the entry expired?\r\n\r\nThanks!",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": "RESOLVED",
            "createdAt": "2025-02-04T17:16:51Z",
            "updatedAt": "2025-02-06T01:17:40Z",
            "closedAt": "2025-02-06T01:17:40Z",
            "closed": true,
            "author": {
              "login": "spicalous",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "close, duplicated with https://github.com/redis/redis/issues/13785",
                  "createdAt": "2025-02-06T01:17:40Z",
                  "updatedAt": "2025-02-06T01:17:41Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "ShooterIT",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13754",
            "number": 13754,
            "title": "RediSearch indexed items get removed after restart.",
            "body": "I use redis search and after creating an index and making an item it works.\r\nAfter i restart the index dont have any indexed items on the FT.INFO and the search no longer works.\r\nAny idea why?\r\n\r\nI use Redis on Docker Desktop on windows 11 using WSL.\r\nI was thinking using Docker would fix that issue but its not.\r\nI use the latest version of redis stack server.",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2025-01-19T15:04:12Z",
            "updatedAt": "2025-02-05T04:19:50Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "Godwhitelight",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "My guess is that you didn't map your docker's rdb dir to the host.\r\nYou can also try non-redisearch data types to see if they disappear after restarting.\r\n",
                  "createdAt": "2025-02-05T04:19:49Z",
                  "updatedAt": "2025-02-05T04:19:50Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13735",
            "number": 13735,
            "title": "Confused by Versioning of Docker Images",
            "body": "I quiet confused by the versioning of redis and redis-stack versions\r\n\r\nThere are this `-v*` and `-M*` versions but not sure which are stable and which aren't. As in my normal environment versions only without any postfix are considered stable, but not sure which in case of redis should be used. But I'm only familiar with postfixes like alpha, beta, rc (release candidate) not -v or -M so this kind of confuses.\r\n\r\nExample of tags in redis docker images:\r\n\r\n - 8.0-M01 / 8.0-M2\r\n - 6.2 / 7.2\r\n\r\nExample of tags in redis-stack docker images:\r\n \r\n - 6.2.6-v18 / 7.2.0-v14\r\n \r\nSources:\r\n\r\n - https://hub.docker.com/r/redis/redis-stack/tags\r\n - https://hub.docker.com/_/redis/tags\r\n \r\nSo maybe somebody can enlight me here as I want to keep documentation and examples of https://github.com/PHP-CMSIG/search uptodate with latest version of Redis but not accidently want to document that somebody uses a none stable version.",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2025-01-09T16:44:57Z",
            "updatedAt": "2025-01-09T19:49:03Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "alexander-schranz",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": true,
            "answerChosenAt": "2025-01-09T19:49:03Z",
            "answerChosenBy": {
              "login": "alexander-schranz",
              "__typename": "User"
            },
            "comments": {
              "nodes": [
                {
                  "body": "For Redis, we now use the following versioning method:\r\n\r\n![image](https://github.com/user-attachments/assets/479569b7-9596-4c1c-bd8b-567dff1cd9f2)\r\n\r\nFor Redis Stack, we use a different versioning method: x.y.z-r. \r\n\r\nI won't explain the Redis Stack versioning here, but note that with the GA release of Redis 8.0 we will deprecate Redis Stack, because it will become redundant. Starting with Redis 8.0, Redis contains JSON, time series, the 5 probabilistic data structures, and Redis Query Engine (including vector search) previously available only on Redis Stack or as separately installable modules.\r\n\r\n\r\n",
                  "createdAt": "2025-01-09T18:24:08Z",
                  "updatedAt": "2025-01-09T18:28:39Z",
                  "authorAssociation": "MEMBER",
                  "author": {
                    "login": "LiorKogan",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "@LiorKogan Really thank you here.\r\n\r\nThat graphic helps a lot.\r\n\r\n> GA release of Redis 8.0 we will deprecate Redis Stack\r\n\r\nThat is great news, looking forward to it.\r\n\r\n> Starting with Redis 8.0, Redis contains JSON, time series, the 5 probabilistic data structures, and Redis Query Engine (including vector search) previously available only on Redis Stack or as separately installable modules.\r\n\r\nWith Redis Query Engine you mean RediSearch? RediSearch with the JSON module is what we are currently using in [SEAL](https://github.com/PHP-CMSIG/search).",
                        "createdAt": "2025-01-09T18:45:26Z",
                        "updatedAt": "2025-01-09T18:45:27Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "alexander-schranz",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                },
                {
                  "body": "Yes, Redis Query Engine is RediSearch.",
                  "createdAt": "2025-01-09T19:16:49Z",
                  "updatedAt": "2025-01-09T19:16:50Z",
                  "authorAssociation": "MEMBER",
                  "author": {
                    "login": "LiorKogan",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13731",
            "number": 13731,
            "title": "Bugfix versions (7.4.2/7.2.7) are not available via REDIS repo (https://packages.redis.io)",
            "body": "Hi,\r\n\r\n as the latest REDIS CE Versions (7.2.7 / 7.4.2) are available since 06. January we would like to update our REDIS instances, but the Versions are not available via the Debian REPO at https://packages.redis.io. \r\n \r\nIs there any information when the packages will be available?",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2025-01-08T07:23:31Z",
            "updatedAt": "2025-01-08T07:23:32Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "frankhetterich",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": []
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13667",
            "number": 13667,
            "title": "redis rdb saving fails with signal 31",
            "body": "My redis is not able to do the background saving, my log is full with logs like these:\r\n\r\n```\r\n2817:M 23 Nov 2024 11:28:59.938 - DB 0: 27168 keys (0 volatile) in 32768 slots HT.\r\n2817:M 23 Nov 2024 11:28:59.938 . 1 clients connected (0 replicas), 13098864 bytes in use\r\n2817:M 23 Nov 2024 11:29:00.039 * 1 changes in 3600 seconds. Saving...\r\n2817:M 23 Nov 2024 11:29:00.043 * Background saving started by pid 29696\r\n29696:C 23 Nov 2024 11:29:00.051 - Fork CoW for RDB: current 0 MB, peak 0 MB, average 0 MB\r\n2817:M 23 Nov 2024 11:29:00.549 # Background saving terminated by signal 31\r\n```\r\n\r\nUnfortunately I am not able to get more verbose logs about this (the above is with loglevel=debug), so I really do not know how to fix/improve this.\r\n\r\nI already configured my machine to allow memory overcommit:\r\n```\r\ncat /proc/sys/vm/overcommit_memory \r\n1\r\n```\r\n```\r\nredis-server -v\r\nRedis server v=7.0.15 sha=00000000:0 malloc=jemalloc-5.3.0 bits=32 build=8fef3e995a542118\r\n```\r\n\r\nIt seems redis is sometimes able to do the saving:\r\n\r\n```\r\nls /var/lib/redis -al\r\ntotal 4104\r\ndrwxr-x---  2 redis redis    4096 Nov 23 11:45 .\r\ndrwxr-xr-x 28 root  root     4096 Nov 13 10:14 ..\r\n-rw-rw----  1 redis redis 4193638 Nov 16 09:42 dump.rdb\r\n```\r\n\r\nand during a restart it seems to work as well:\r\n\r\n```\r\n# systemctl restart redis\r\n# ls /var/lib/redis -al\r\ntotal 8200\r\ndrwxr-x---  2 redis redis    4096 Nov 23 11:46 .\r\ndrwxr-xr-x 28 root  root     4096 Nov 13 10:14 ..\r\n-rw-rw----  1 redis redis 4193638 Nov 16 09:42 dump.rdb\r\n-rw-rw----  1 redis redis 4194304 Nov 23 11:46 temp-2817.rdb\r\n```\r\n\r\nBut why is it not working in the background?\r\nI am not sure if this is a bug or not, the same thing is happening on two machines (with the same arm processor) - do you think I should open a bug report for this?\r\n\r\n(Note, this is a duplicate of https://serverfault.com/questions/1168060/redis-rdb-saving-fails-with-signal-31 )\r\n",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": "RESOLVED",
            "createdAt": "2024-11-27T08:16:37Z",
            "updatedAt": "2025-01-07T09:17:17Z",
            "closedAt": "2024-11-28T07:36:50Z",
            "closed": true,
            "author": {
              "login": "rkohrt",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": true,
            "answerChosenAt": "2024-11-28T07:37:12Z",
            "answerChosenBy": {
              "login": "rkohrt",
              "__typename": "User"
            },
            "comments": {
              "nodes": [
                {
                  "body": "filed an issue for this: https://github.com/redis/redis/issues/13671 (looks like an integer overflow after more looking into this)",
                  "createdAt": "2024-11-28T07:36:50Z",
                  "updatedAt": "2024-11-28T07:36:51Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "rkohrt",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "@rkohrt do you know the exact overflow position?",
                        "createdAt": "2024-11-28T07:41:46Z",
                        "updatedAt": "2024-11-28T07:41:46Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "not familiar with the redis code base unfortunately, just guessing due to the sizes of the `rdb` files",
                        "createdAt": "2024-11-28T07:57:44Z",
                        "updatedAt": "2024-11-28T07:57:44Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "rkohrt",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@rkohrt can you provide the output of `INFO ALL`?\r\ndo you also use Raspberry Pi?",
                        "createdAt": "2025-01-07T09:17:17Z",
                        "updatedAt": "2025-01-07T09:17:17Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13712",
            "number": 13712,
            "title": "Optimization for function redisPopcount",
            "body": "Hi everyone, \r\nI'm new in redis github, previously I worked with a lot of company versions. So in general during one interview I saw some optimisaiton s for the function redisPopcount.\r\nSo my investigation for this function is in a separate environment. I took the function and ran the calculation for some chunk of random data: 640 MB and 4 bytes the standard function operates around 0.562000 seconds, but when I made some code rearrangement and optimizations I was able to decrease execution time to 0.247000 seconds. I test it on my machine with Intel Core i7-6700HQ CPU\r\nSo the question is should I send pull request for this optimization or this function is almost didn't used.\r\n\r\nAlso I made SSE version for 9 year old PC and it executes like 0.213000 second, This kind of separate question but why SIMD code is not present in repo?\r\n\r\n",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-12-30T18:23:10Z",
            "updatedAt": "2025-01-07T07:45:36Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "retsetB",
              "__typename": "User"
            },
            "category": {
              "name": "Show and tell",
              "description": "Show off something you've made",
              "slug": "show-and-tell"
            },
            "isAnswered": null,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "@retsetB feel free to make a PR.\r\nhttps://github.com/redis/redis/pull/13359 is one use of simd optimization for redisPopcount().",
                  "createdAt": "2024-12-31T01:24:25Z",
                  "updatedAt": "2024-12-31T01:24:26Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "I prepare PR: https://github.com/redis/redis/pull/13719 it is without SIMD just code rearrangement and decreasing complexity, with some time increasing",
                        "createdAt": "2025-01-01T20:34:08Z",
                        "updatedAt": "2025-01-01T20:34:09Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "retsetB",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "[Please see ](https://github.com/redis/redis/pull/2179) you are reverting a pervious optimization.",
                        "createdAt": "2025-01-01T22:11:54Z",
                        "updatedAt": "2025-01-01T22:11:55Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "SergeyVystoropskyi",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@retsetB Can you analyze them? why https://github.com/redis/redis/pull/2179 more slowly than yours?",
                        "createdAt": "2025-01-02T07:30:59Z",
                        "updatedAt": "2025-01-02T07:31:00Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Thanks, I looked at this code, and yes looks like I reverted it and rearranged code a little bit.\r\n\r\nI will look at why my code starts working faster and will try to check on several different PCs.\r\nMy initial thought is that it was made about 10 years ago and it was an improvement in compiler and CPU architecture, so some alignment could be used now that before was not available.\r\nI will post an update when I will have some new information",
                        "createdAt": "2025-01-02T08:40:30Z",
                        "updatedAt": "2025-01-02T08:40:30Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "retsetB",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "This is definitely compiler issue.\r\n\r\nwhen I try to build using mingw version installed on Windows(gcc version 14.2.0 (x86_64-win32-seh-rev0, Built by MinGW-Builds project)) I have the following result:\r\nexecuting time for redisPopcount_4 takes: 0.247000 seconds, result is = 2673827632\r\nexecuting time for redisPopcount takes: 0.562000 seconds, result is = 2673827632\r\n\r\nbut when I try to install compiler(gcc version 11.4.0 (Ubuntu 11.4.0-1ubuntu1~22.04)) and build from docker I have following result:\r\nexecuting time for redisPopcount_4 takes: 0.546000 seconds, result is = 2673827632\r\nexecuting time for redisPopcount takes: 0.527000 seconds, result is = 2673827632\r\n\r\nredisPopcount  - original code\r\nredisPopcount_4  - code from PR",
                        "createdAt": "2025-01-02T12:34:50Z",
                        "updatedAt": "2025-01-02T12:34:51Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "retsetB",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "what does objdump say? this is a special instruction in multiple ISAs, for example:\r\nx86 - https://www.felixcloutier.com/x86/popcnt\r\nrisc-v - https://five-embeddev.com/riscv-bitmanip/1.0.0/bitmanip.html#insns-cpop\r\narm - https://developer.arm.com/documentation/ddi0602/2024-12/Base-Instructions/CNT--Count-bits-\r\n\r\ndid compiler used one of those? was it just because 16 is a factor of cacheline size?\r\n\r\nAppologies in advance forr asking too many questions.",
                        "createdAt": "2025-01-05T03:41:40Z",
                        "updatedAt": "2025-01-05T03:42:24Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "SergeyVystoropskyi",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "I analyzed obj file and in case of build from MinGW, it use commands like: vpaddd instead of add(it automatically add some vectorisation on CPU level), descriptions you could found: https://www.felixcloutier.com/x86/paddb:paddw:paddd:paddq or https://docs.oracle.com/cd/E37838_01/html/E61064/gsesq.html. So MinGW tried to make some AVX/SSE optimization and couldn't do it with the current implementation, when I decreased number of variables from 7 to 4 it it was able to do it and this is a reason why I have this performance boost. when I compile from docker it was not able to do it. \r\nIn both cases I compile code with flag -mavx2 in case of MinGW it is activate AVX2 optimization, and GCC does not use it\r\nI use this flag because in code I have additional functions with SSE optimizations.\r\nI also have a code for pure SSE optimisations, but a lot of changes in build should be made, it requires including one additional flag, and in your case we need to check for the latest optimisation and it will make logic more complex. I think firstly decide with this code and SSE/Neon would be separate topic",
                        "createdAt": "2025-01-06T08:59:06Z",
                        "updatedAt": "2025-01-06T08:59:07Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "retsetB",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "> but when I try to install compiler(gcc version 11.4.0 (Ubuntu 11.4.0-1ubuntu1~22.04)) and build from docker I have following result:\r\n> executing time for redisPopcount_4 takes: 0.546000 seconds, result is = 2673827632\r\n> executing time for redisPopcount takes: 0.527000 seconds, result is = 2673827632\r\n\r\nHi @retsetB why doesn't it bring benefit in ubuntu with gcc 11.4.0",
                        "createdAt": "2025-01-07T03:27:50Z",
                        "updatedAt": "2025-01-07T03:27:51Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "ShooterIT",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Compiler that is used gcc version 11.4.0, do not use vector operation, so compiled code(object file) after analysis has standard asm commands like add, but newer version from MinGW use vector optimization: vaddd from AVX2. I test this separate function on Windows using MinGW and it has this optimization, I do not have installed Ubuntu, so I didn't try other compiler versions. Around weekend I will install some Linux on my laptop and will check with other compiler versions",
                        "createdAt": "2025-01-07T05:41:10Z",
                        "updatedAt": "2025-01-07T05:41:11Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "retsetB",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "from your test, your modification version is even better than https://github.com/redis/redis/pull/13359, yours reduces a half of cost time?\r\n\r\nand do you try  `make CFLAGS=\"-march=native\"` on your ubutun?",
                        "createdAt": "2025-01-07T07:45:36Z",
                        "updatedAt": "2025-01-07T07:45:37Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "ShooterIT",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13727",
            "number": 13727,
            "title": "Redis Sentinel Behavior When Sentinels Are Down but Application Still Works",
            "body": "Hello,  \r\n\r\nI have a question regarding Redis Sentinel's behavior in my application setup. Here's the relevant part of my configuration:  \r\n\r\nsentinels: [\r\n    { host: \"nodeip1\", port: 26379 },\r\n    { host: \"nodeip2\", port: 26379 },\r\n    { host: \"nodeip3\", port: 26379 }\r\n],\r\n\r\nWhen I stop all the `redis-sentinel` services (using `systemd`), no process is listening on port 26379. Despite this, my application continues to read and write to Redis seamlessly.  \r\n\r\nThis seems counterintuitive since my application is configured to point only to Redis Sentinel for service discovery. Logically, I would expect the application to fail since Sentinel services are down.  \r\n\r\nI came across this in the [Redis documentation](https://redis.io/docs/latest/operate/oss_and_stack/management/sentinel/):  \r\n> **Configuration provider.** Sentinel acts as a source of authority for clients' service discovery: clients connect to Sentinels in order to ask for the address of the current Redis master responsible for a given service. If a failover occurs, Sentinels will report the new address.  \r\n\r\nThis explains the role of Sentinels during failover and discovery but doesn't clarify why my application continues to function when all Sentinels are down.  \r\n\r\nCould someone help me understand how this works logically? Specifically:  \r\n1. How does the Redis client maintain connection when all Sentinels are theoretically unreachable?\r\n2. Does the application cache the master node's address after the initial discovery?  \r\n3. Is there a timeout or retry mechanism built into the Redis client?\r\n\r\nAny insights would be greatly appreciated!  \r\n\r\nThank you! ",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2025-01-06T09:36:08Z",
            "updatedAt": "2025-01-06T09:36:09Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "vinayak-somvanshi-dsw",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": []
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13706",
            "number": 13706,
            "title": "Seeking Guidance for Upgrading from Redis Cluster 3.2 to 4.x or Later Versions",
            "body": "In our production environment, we have been running Redis Cluster version 3.2 and are now considering an upgrade to Redis Cluster 4.x or a later stable version. However, given the significant changes between these versions, especially concerning the cluster bus protocol compatibility as mentioned in the official [4.0 RELEASE NOTES](https://raw.githubusercontent.com/antirez/redis/4.0/00-RELEASENOTES), I am seeking advice on the best practices and steps involved in executing this upgrade.\r\n\r\nSpecifically, the release notes indicate that \"The Redis Cluster bus protocol of 4.0 is no longer compatible with Redis 3.2. This change was necessary to provide Docker/NAT compatibility for Redis Cluster. Therefore, upgrading a Redis Cluster to 4.0 requires a mass restart of all instances.\" In our current setup, such a mass restart would mean all instances would be simultaneously replaced and restarted, potentially causing significant disruption to our live services.\r\n\r\nTherefore, I request guidance on the following points:\r\n\r\nIs there a recommended step-by-step guide or checklist available to ensure a smooth transition from Redis Cluster 3.2 to 4.x or a later version?\r\nGiven that the incompatible cluster bus protocol necessitates a mass restart of all instances, are there alternative strategies—such as rolling upgrades—that can minimize downtime and reduce the impact on our live services? If not, what preparations or mitigation measures can we implement to ensure minimal disruption?\r\nWe highly value the expertise and experience within this community and greatly appreciate any guidance, recommendations, or personal experiences shared by those who have successfully completed this upgrade.\r\n\r\nThank you very much for your time and support.\r\n\r\nBest regards,",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-12-27T09:28:47Z",
            "updatedAt": "2025-01-03T02:34:00Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "Terrynech",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "as release notes said, rolling upgrades are not possible. If you want to avoid  a mass restart, maybe you can setup a new cluster, and migrate the data of old cluster to new cluster, and then switch the entrance to the new cluster. for migration tool, you can refer to https://github.com/tair-opensource/RedisShake",
                  "createdAt": "2025-01-03T02:19:14Z",
                  "updatedAt": "2025-01-03T02:19:15Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "ShooterIT",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "Thanks a lot for your reply! It's been super helpful for me to figure out how to upgrade the Redis Cluster. I really appreciate it.",
                        "createdAt": "2025-01-03T02:34:00Z",
                        "updatedAt": "2025-01-03T02:34:00Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "Terrynech",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/10476",
            "number": 10476,
            "title": "Tried to get docs but found: \"Access denied, Error 16\" [Russia]",
            "body": "Hi from Moscow. \r\nToday our command found, that we can't open [redis documentation](https://redis.io/docs/) from Russia.\r\nOf course we can use vpn or tor, but seems that this situation is can be abnormal and nobody from site maintainers know about it.\r\n\r\nCan you give any information about that case?",
            "upvoteCount": 10,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2022-03-24T19:14:51Z",
            "updatedAt": "2024-12-30T09:45:07Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "SomeAkk",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": true,
            "answerChosenAt": "2022-03-25T15:34:03Z",
            "answerChosenBy": {
              "login": "itamarhaber",
              "__typename": "User"
            },
            "comments": {
              "nodes": [
                {
                  "body": "Hello @SomeAkk,\r\n\r\nThanks for letting us know.\r\n\r\nA misconfigured firewall had blocked access to the new website from different parts of the world. The configuration had been updated and it looks like the issue is resolved\r\n\r\n Please let us know if there are any further issues.\r\n\r\nCheers,\r\nItamar",
                  "createdAt": "2022-03-25T15:33:51Z",
                  "updatedAt": "2022-06-17T14:36:33Z",
                  "authorAssociation": "MEMBER",
                  "author": {
                    "login": "itamarhaber",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "Hello, I have stumbled upon the same issue, except this time for the [Redis.com](https://redis.com) website (I was trying to look at a blog post article). Is it possible a misconfiguration happened again or wasn't fixed from the previous time this happened?",
                        "createdAt": "2022-04-26T19:27:17Z",
                        "updatedAt": "2022-06-17T14:36:41Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "andre4ik3",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@itamarhaber Looks like this problem has happened again. Access to redis.io and redis.com is blocked from Russia.\r\n\r\nIncident ID: 1288001150135076246-165679110851927242\r\n\r\n@evrsml also encountered the same problem.",
                        "createdAt": "2024-04-03T06:24:36Z",
                        "updatedAt": "2024-04-03T06:24:36Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "i8enn",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "It's so interesting we can got redis sources from github, but can't view rendered documentation on official site )\r\n",
                        "createdAt": "2024-04-03T06:47:33Z",
                        "updatedAt": "2024-04-03T06:47:33Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "estin",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Hello! I run into the same problem with [redis.io](https://redis.io/docs/). Access is blocked from Russia.\r\n\r\nIncident ID: 633000030005506036-18000693310324932",
                        "createdAt": "2024-04-09T12:39:14Z",
                        "updatedAt": "2024-04-09T12:39:15Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "xuniq",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Hello. Access to documentation denied again.\r\n\r\nIncident ID: 7228000020260613160-60611876034969728",
                        "createdAt": "2024-05-03T07:56:05Z",
                        "updatedAt": "2024-05-03T07:56:16Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "reznikartem",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Hello. When trying to open redis.io I get the following error\r\n\r\nIncident ID: 276000070039895725-141476905648128462",
                        "createdAt": "2024-05-14T09:49:03Z",
                        "updatedAt": "2024-05-14T09:49:04Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "kapralovs",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                },
                {
                  "body": "@itamarhaber here redis team solved problem with redis docs portal ([redis.io](https://redis.io/)). \r\nDoes we need open new discussion to make another redis web resource ([redis.com](https://redis.com/)) available from Russia?\r\n\r\nСС @borisalekseev, @andre4ik3  and thx you for signaling.",
                  "createdAt": "2022-07-24T18:08:59Z",
                  "updatedAt": "2022-07-24T18:09:00Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "SomeAkk",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "Hello! It seems that this problem occurred again. Me and my friends are not able to access redis  site from Russia. Works only with VPN. Please, fix it! ",
                        "createdAt": "2024-04-02T17:48:45Z",
                        "updatedAt": "2024-04-02T17:49:20Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "evrsml",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                },
                {
                  "body": "Hi @SomeAkk, @borisalekseev and @andre4ik3\r\n\r\n> Does we need open new discussion...\r\n\r\nAlthough this is a different topic, I don't think we need a new discussion.\r\n\r\n>  ...is it because of sanctions?\r\n\r\nCorrect, due to recent sanctions the Redis.com and enterprise business resources may be unavailable.\r\nIf you require assistance with a Redis Enterprise account, please contact [support@redis.com](mailto:support@redis.com).\r\n\r\nCheers,\r\nItamar",
                  "createdAt": "2022-07-27T19:06:20Z",
                  "updatedAt": "2022-07-28T13:27:56Z",
                  "authorAssociation": "MEMBER",
                  "author": {
                    "login": "itamarhaber",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "I understand if it is not possible to buy some paid functions, but why block the documentation redis.",
                        "createdAt": "2024-06-10T06:09:34Z",
                        "updatedAt": "2024-06-10T06:09:35Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "Maligosus",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                },
                {
                  "body": "Hello friends, has a solution been found for this problem, I have the same problem and I am in Iran, error 16 Redis site\r\nIt doesn't even work with vpn, what should I do?",
                  "createdAt": "2023-10-31T21:01:51Z",
                  "updatedAt": "2023-10-31T21:01:52Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "mohammadNematollahi",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "Blocking open source software documentation is idiotic. Especially considering the availability of a VPN.",
                  "createdAt": "2024-04-23T06:30:10Z",
                  "updatedAt": "2024-04-23T06:30:10Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "redb0",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "redio.io is blocked in Russia by imperva right now, fix it please 🙏🏻 ",
                  "createdAt": "2024-04-26T13:07:20Z",
                  "updatedAt": "2024-04-26T13:07:21Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "sgjurano",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "пацаны, расходимся, им же насрать - лучше соберитесь и напишите ruredis,\r\nа то мне уже надоели ваши уведомления в почту, ахахахах",
                  "createdAt": "2024-04-26T13:19:52Z",
                  "updatedAt": "2024-04-26T13:20:06Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "SomeAkk",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "Нафига? Лечше альтернативу нормальную найти. Зачем вам эти гнилые блокировщики?\r\nСейчас хочу посмотреть в сторону keyDB",
                        "createdAt": "2024-12-30T09:45:07Z",
                        "updatedAt": "2024-12-30T09:45:07Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "design-principles",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13680",
            "number": 13680,
            "title": "xreadgroup can't read from my stream",
            "body": "Hi, I'm playing with redis streams using python library https://github.com/redis/redis-py . I faced the problem with consumer groups.\r\n\r\nI created a consumer routine `consume1` which works fine using `XREAD` command. The `consume2` uses consumer group (which is created without any error), but I can't read from the stream. I get `NOGROUP No such key 'my-read-key2' or consumer group 'my-group' in XREADGROUP with GROUP option`. I'm sure it's some server side error, not a python library. Can someone help with this?\r\n\r\nHow reproduce this:\r\n\r\nenv\r\n\r\n```\r\n$ lsb_release -a\r\nNo LSB modules are available.\r\nDistributor ID:\tUbuntu\r\nDescription:\tUbuntu 20.04.6 LTS\r\nRelease:\t20.04\r\nCodename:\tfocal\r\n\r\n$ python --version\r\nPython 3.12.7\r\n\r\n$ poetry show redis\r\n name         : redis                                                \r\n version      : 5.2.0\r\n\r\n$ docker-compose images redis\r\nContainer           Repository          Tag                 Image Id            Size\r\nredis-1     redis               7.4.1-alpine        87b460005bd3        46.7MB\r\n```\r\n\r\ncode:\r\n\r\n```python\r\nimport asyncio\r\nfrom asyncio import TaskGroup\r\nfrom datetime import UTC, datetime\r\n\r\nfrom redis.asyncio import Redis\r\n\r\n\r\nasync def main() -> None:\r\n    async with TaskGroup() as tg, Redis.from_url(\"redis://localhost:6379/\") as r:\r\n        # simple publish & read\r\n        async def consume1():\r\n            return await r.xread(streams={\"my-read-key1\": \"$\"}, count=1, block=0)\r\n\r\n        t1 = tg.create_task(consume1())\r\n        await asyncio.sleep(1.0)\r\n        pub1_resp = await r.xadd(\r\n            name=\"my-read-key1\",\r\n            fields={b\"body\": b\"my-msg-\" + datetime.now(UTC).isoformat().encode()},\r\n        )\r\n        print(pub1_resp, await t1)\r\n\r\n        async def consume2():\r\n            # create consumer group\r\n            create_resp = await r.xgroup_create(name=\"my-group-key\", groupname=\"my-group\", mkstream=True)\r\n            print(create_resp)\r\n            read_group_resp = await r.xreadgroup(\r\n                groupname=\"my-group\",\r\n                consumername=\"my-consumer\",\r\n                streams={\"my-read-key2\": \">\"},\r\n                count=1,\r\n                block=0,\r\n            )\r\n            return read_group_resp\r\n\r\n        # publish and read with group\r\n        t2 = tg.create_task(consume2())\r\n        await asyncio.sleep(1.0)\r\n        pub2_resp = await r.xadd(\r\n            name=\"my-read-key2\",\r\n            fields={b\"body\", b\"my-msg-\" + datetime.now(UTC).isoformat().encode()},\r\n        )\r\n        print(pub2_resp, await t2)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    asyncio.run(main())\r\n```\r\n\r\nThe output:\r\n\r\n```\r\nb'1733347779170-0' [[b'my-read-key1', [(b'1733347779170-0', {b'body': b'my-msg-2024-12-04T21:29:39.166565+00:00'})]]]\r\nTrue\r\n  + Exception Group Traceback (most recent call last):\r\n  |   File \".local/red.py\", line 46, in <module>\r\n  |     asyncio.run(main())\r\n  |   File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run\r\n  |     return runner.run(main)\r\n  |            ^^^^^^^^^^^^^^^^\r\n  |   File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run\r\n  |     return self._loop.run_until_complete(task)\r\n  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  |   File \"/usr/lib/python3.12/asyncio/base_events.py\", line 687, in run_until_complete\r\n  |     return future.result()\r\n  |            ^^^^^^^^^^^^^^^\r\n  |   File \".local/red.py\", line 9, in main\r\n  |     async with TaskGroup() as tg, Redis.from_url(\"redis://localhost:6379/\") as r:\r\n  |                ^^^^^^^^^^^\r\n  |   File \"/usr/lib/python3.12/asyncio/taskgroups.py\", line 145, in __aexit__\r\n  |     raise me from None\r\n  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\r\n  +-+---------------- 1 ----------------\r\n    | Traceback (most recent call last):\r\n    |   File \".local/red.py\", line 26, in consume2\r\n    |     read_group_resp = await r.xreadgroup(\r\n    |                       ^^^^^^^^^^^^^^^^^^^\r\n    |   File \".venv/lib/python3.12/site-packages/redis/asyncio/client.py\", line 616, in execute_command\r\n    |     return await conn.retry.call_with_retry(\r\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \".venv/lib/python3.12/site-packages/redis/asyncio/retry.py\", line 59, in call_with_retry\r\n    |     return await do()\r\n    |            ^^^^^^^^^^\r\n    |   File \".venv/lib/python3.12/site-packages/redis/asyncio/client.py\", line 590, in _send_command_parse_response\r\n    |     return await self.parse_response(conn, command_name, **options)\r\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \".venv/lib/python3.12/site-packages/redis/asyncio/client.py\", line 637, in parse_response\r\n    |     response = await connection.read_response()\r\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \".venv/lib/python3.12/site-packages/redis/asyncio/connection.py\", line 571, in read_response\r\n    |     raise response from None\r\n    | redis.exceptions.ResponseError: NOGROUP No such key 'my-read-key2' or consumer group 'my-group' in XREADGROUP with GROUP option\r\n    +------------------------------------\r\n```",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": "RESOLVED",
            "createdAt": "2024-12-04T21:59:25Z",
            "updatedAt": "2024-12-21T10:32:41Z",
            "closedAt": "2024-12-21T10:32:41Z",
            "closed": true,
            "author": {
              "login": "zerlok",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "@zerlok did you check if stream `my-read-key2` exists?",
                  "createdAt": "2024-12-05T01:39:04Z",
                  "updatedAt": "2024-12-05T01:39:05Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "@sundb , it doesn't exist for sure, because I'm starting the script on empty redis. For `my-read-key1` it worked well.",
                        "createdAt": "2024-12-15T19:06:19Z",
                        "updatedAt": "2024-12-15T19:06:19Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "zerlok",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "i don't see stream `my-read-key2` in `create_resp = await r.xgroup_create(name=\"my-group-key\", groupname=\"my-group\", mkstream=True)`.\r\ndid you give the wrong parameter?",
                        "createdAt": "2024-12-16T01:18:42Z",
                        "updatedAt": "2024-12-16T01:18:42Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "It seems like I had wrong parameter. I changed the `name` value in xgroupcreate to `name=\"my-read-key2\"` . But I don't understand why should I create a group to the same key that is used in xreadgroup call ? Should I create a separate consumer group to each key that I want to read from? Why then xreadgroup accepets multiple streams `XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds]\r\n  [NOACK] STREAMS key [key ...] id [id ...]` ?\r\n\r\nP.S. just in case, here is a full working code\r\n\r\n```python\r\nimport asyncio\r\nfrom asyncio import TaskGroup\r\nfrom contextlib import suppress\r\nfrom datetime import UTC, datetime\r\n\r\nfrom redis.asyncio import Redis, ResponseError\r\n\r\n\r\nasync def main() -> None:\r\n    async with TaskGroup() as tg, Redis.from_url(\"redis://localhost:6379/\") as r:\r\n        # simple publish & read\r\n        async def consume1():\r\n            print(await r.keys())\r\n            return await r.xread(streams={\"my-read-key1\": \"$\"}, count=1, block=0)\r\n\r\n        t1 = tg.create_task(consume1())\r\n        await asyncio.sleep(1.0)\r\n        pub1_resp = await r.xadd(\r\n            name=\"my-read-key1\",\r\n            fields={b\"body\": b\"my-msg-\" + datetime.now(UTC).isoformat().encode()},\r\n        )\r\n        print(pub1_resp, await t1)\r\n\r\n        async def consume2():\r\n            # create consumer group\r\n            print(await r.keys())\r\n\r\n            with suppress(ResponseError):\r\n                create_resp = await r.xgroup_create(name=\"my-read-key2\", groupname=\"my-group\", mkstream=True)\r\n                print(create_resp)\r\n\r\n            read_group_resp = await r.xreadgroup(\r\n                groupname=\"my-group\",\r\n                consumername=\"my-consumer\",\r\n                streams={\"my-read-key2\": \">\"},\r\n                count=1,\r\n                block=1000,\r\n            )\r\n            return read_group_resp\r\n\r\n        # publish and read with group\r\n        t2 = tg.create_task(consume2())\r\n        await asyncio.sleep(1.0)\r\n        pub2_resp = await r.xadd(\r\n            name=\"my-read-key2\",\r\n            fields={b\"body\": b\"my-msg-\" + datetime.now(UTC).isoformat().encode()},\r\n        )\r\n        print(pub2_resp, await t2)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    asyncio.run(main())\r\n\r\n```\r\n\r\noutput\r\n\r\n```\r\n[]\r\nb'1734544656686-0' [[b'my-read-key1', [(b'1734544656686-0', {b'body': b'my-msg-2024-12-18T17:57:36.682849+00:00'})]]]\r\n[b'my-read-key1']\r\nTrue\r\nb'1734544657689-0' [[b'my-read-key2', [(b'1734544657689-0', {b'body': b'my-msg-2024-12-18T17:57:37.688927+00:00'})]]]\r\n```",
                        "createdAt": "2024-12-18T18:04:30Z",
                        "updatedAt": "2024-12-18T18:05:30Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "zerlok",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                },
                {
                  "body": "> Why should I create a group to the same key that is used in xreadgroup call ? \r\n\r\n`XREADGROUP GROUP g1...` with multiple streams requires for each of these streams to have a consumer group named `g1`.\r\n\r\n> Should I create a separate consumer group to each key that I want to read from?\r\n\r\nYes, each consumer group 'belongs' to just one stream (even if multiple streams have consumer groups with the same name).\r\n\r\n> Why then xreadgroup accepets multiple streams XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS key [key ...] id [id ...] ?\r\n\r\nJust like `XREAD` - `XREADGROUP` can be called with multiple streams if we want to read at the same time from a number of keys. Especially when blocking with `BLOCK`, to be able to listen with a single connection to multiple keys is a vital feature. ",
                  "createdAt": "2024-12-18T19:37:55Z",
                  "updatedAt": "2024-12-19T06:32:00Z",
                  "authorAssociation": "MEMBER",
                  "author": {
                    "login": "LiorKogan",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "Thanks for answering my questions. I think I'm starting to get it now.\r\n\r\nTo clarify it, let's say there are consumer groups: `g1` on stream with key `k1` and consumer group `g1` on stream `k2`. These consumer groups have same names but they are not the same groups, because they are assigned to different streams. So, when I start one consumer `c1`: `XREADGROUP GROUP g1 c1 COUNT 1 BLOCK 0 STREAMS k1 k2 > >` and second consumer `c2`: `XREADGROUP GROUP g1 c2 COUNT 1 BLOCK 0 STREAMS k1 k2 > >` - there are 2 separate consumer groups with 2 consumers in each (in total 4 consumers). So, in that case, c1 on k1 and c2 on k1 are in one consumer group and will receive distinct messages from stream k1 and same applies to consumers on stream k2. But, c1 on k1 and c1 on k2 - just 2 different consumers and they just \"don't know\" about each other / independent.",
                        "createdAt": "2024-12-19T08:49:03Z",
                        "updatedAt": "2024-12-19T08:49:04Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "zerlok",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Correct. You can try the following:\r\n\r\n```\r\nxadd s1 * a 1\r\nxadd s1 * a 2\r\nxadd s2 * b 1\r\nxadd s2 * b 2\r\n\r\nxgroup create s1 g1 0\r\nxgroup create s1 g2 0\r\nxgroup create s2 g1 0\r\nxgroup create s2 g2 0\r\n\r\nxreadgroup GROUP g1 c1 COUNT 1 STREAMS s1 s2 > >\r\n// get a 1 from s1; get b 1 from s2\r\nxreadgroup GROUP g1 c2 COUNT 1 STREAMS s1 s2 > >\r\n// get a 2 from s1; get b 2 from s2\r\n\r\n\r\nxreadgroup GROUP g2 c1 COUNT 1 STREAMS s1 s2 > >\r\n// also get a 1 from s1; get b 1 from s2\r\nxreadgroup GROUP g2 c2 COUNT 1 STREAMS s1 s2 > >\r\n// also get a 2 from s1; get b 2 from s2\r\n```",
                        "createdAt": "2024-12-19T13:47:40Z",
                        "updatedAt": "2024-12-19T13:56:08Z",
                        "authorAssociation": "MEMBER",
                        "author": {
                          "login": "LiorKogan",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                },
                {
                  "body": "> each consumer group 'belongs' to just one stream (even if multiple streams have consumer groups with the same name).\r\n\r\nSo, to use `XREADGROUP` I should declare a consumer group on stream `my-read-key2` with `XGROUP CREATE` first.",
                  "createdAt": "2024-12-21T10:32:41Z",
                  "updatedAt": "2024-12-21T10:32:42Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "zerlok",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13684",
            "number": 13684,
            "title": "Concurrent Redis SCAN",
            "body": "# Proposal for Enhancing Redis SCAN for Concurrent Operations\r\n\r\n## Background\r\n\r\nThe current cursor-based mechanism of Redis's `SCAN` command presents challenges for concurrent operations, particularly in scenarios where Redis is hosted remotely or efficient pagination is needed.\r\n\r\n## Problem Statement\r\n\r\nThe existing `SCAN` operation is sequential and dependent on a server-generated cursor. When attempting to concurrently fetch keys matching a pattern, tasks frequently share the same cursor since the updated cursor value is unavailable until the preceding `SCAN` call completes. This limits the ability to fully exploit concurrent operations, resulting in reduced performance and scalability, and inefficient utilization of Async I/O capabilities.\r\n\r\n## Proposed Enhancements\r\n\r\n1. **Manual Cursor Management**\r\n   - Introduce an optional parameter in the `SCAN` command to allow users to provide and increment the cursor manually.\r\n   - This would enable clients to independently manage cursor state across concurrent tasks, bypassing the blocking nature of the `SCAN` response.\r\n\r\n2. **Simplify Cursor Type**\r\n   - Currently, the cursor is returned as a string, adding unnecessary overhead when handling it programmatically.\r\n   - Returning it as an integer or providing a configuration option to toggle between types would enhance usability.\r\n\r\n## Benefits\r\n\r\n- **Improved Performance**: Manual cursor management facilitates true parallelism by enabling independent tasks to proceed without waiting for a shared cursor update.\r\n- **Enhanced Usability**: Simplifying the cursor type and providing flexibility in its management reduces development overhead and improves integration with Async I/O frameworks.\r\n- **Backward Compatibility**: These changes can be implemented as optional enhancements, ensuring existing applications are unaffected.\r\n\r\n## Example Use Case\r\n\r\nConsider a method to erase user-specific cache keys using `SCAN` with a given pattern:\r\n\r\n```ruby\r\ndef erase_user_cache(user_id)\r\n  cursor = '0'\r\n  barrier = Async::Barrier.new\r\n  semaphore = Async::Semaphore.new(@config[:redis][:pool][:size])\r\n  matched_keys = []\r\n\r\n  loop do\r\n    semaphore.acquire do\r\n      barrier.async do\r\n        cursor, keys = @redis_client.scan(cursor, match: \"user:#{user_id}:\", count: 1000)\r\n        matched_keys += keys\r\n      end\r\n      break if cursor == '0'\r\n    end\r\n  end\r\n\r\n  barrier.wait\r\n  @redis_client.del(matched_keys) unless matched_keys.empty?\r\nensure\r\n  barrier.stop\r\nend\r\n```\r\n\r\nThe issue here is that concurrent tasks often share the same cursor value because the Redis `SCAN` operation blocks until the cursor is updated, resulting in redundant scans and wasted resources.",
            "upvoteCount": 2,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-12-09T18:29:14Z",
            "updatedAt": "2024-12-10T12:36:58Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "Raimo33",
              "__typename": "User"
            },
            "category": {
              "name": "Ideas",
              "description": "Share ideas for new features",
              "slug": "ideas"
            },
            "isAnswered": null,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "thank you @Raimo33 cool idea, for concurrent operations, in standalone mode, the main db just has one dict, it is hard to support, i cluster mode, echo slot has independent dict, maybe it is possible to support it.",
                  "createdAt": "2024-12-10T12:36:58Z",
                  "updatedAt": "2024-12-10T12:36:59Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "ShooterIT",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13677",
            "number": 13677,
            "title": "How is the memory usage for the key-value calculated?",
            "body": "\r\nI encountered an issue when setting keys with similar values in my Redis environment (version 7.0). Specifically, I ran the following commands:\r\n\r\n```shell\r\n127.0.0.1:6379> set aaaaaa abcdef\r\nOK\r\n127.0.0.1:6379> set aaaaaaa abcdef\r\nOK\r\n127.0.0.1:6379> memory usage aaaaaa\r\n(integer) 64\r\n127.0.0.1:6379> memory usage aaaaaaa\r\n(integer) 72\r\n```\r\n\r\nThis led me to the following questions:\r\n\r\n1. Why does the memory usage increase to 72 bytes when I only add one additional character to the key?\r\n2. How is the memory usage for the key `aaaaaa` calculated?\r\n\r\nI attempted to calculate it myself using the following logic:\r\n\r\n- **Key (`aaaaaa`)**: \r\n  - Redis Object Metadata: 16 bytes\r\n  - Length of Key: 1 byte\r\n  - Allocation Size: 1 byte\r\n  - Flags: 1 byte\r\n  - Key Value: 6 bytes (`abcdef`)\r\n  - Null Terminator: 1 byte\r\n  - Total: 26 bytes\r\n\r\n- **Value (`abcdef`)**:\r\n  - Redis Object Metadata: 16 bytes\r\n  - Length of Value: 1 byte\r\n  - Allocation Size: 1 byte\r\n  - Flags: 1 byte\r\n  - Value: 6 bytes (`abcdef`)\r\n  - Null Terminator: 1 byte\r\n  - Total: 26 bytes\r\n\r\n- **Dictionary Entry**:\r\n  - Pointer to Key: 8 bytes\r\n  - Pointer to Value: 8 bytes\r\n  - Next: 8 bytes\r\n  - Total: 24 bytes\r\n\r\nHowever, even with these calculations, the total memory usage exceeds 64 bytes. Could you help me understand where I might be going wrong?\r\n\r\n---\r\n\r\nFeel free to ask if you need further clarification or have any additional questions!",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-12-03T13:38:58Z",
            "updatedAt": "2024-12-04T14:03:53Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "hnzhrh",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": true,
            "answerChosenAt": "2024-12-04T14:03:53Z",
            "answerChosenBy": {
              "login": "hnzhrh",
              "__typename": "User"
            },
            "comments": {
              "nodes": [
                {
                  "body": "In jemalloc, aaaaaa falls into bin 8, whereas aaaaaaa falls into bin 16.\r\naaaaaa: hdr(1 byte) + str length(6byte) + \\0(1 byte) = 8     -> bin 8\r\naaaaaaa: hdr(1 byte) + str length(7byte) + \\0(1 byte) = 9   -> bin 16\r\n\r\nref\r\nhttps://jemalloc.net/jemalloc.3.htm\r\n```\r\nThe realloc(), rallocx(), and xallocx() functions may resize allocations without moving them under limited circumstances. Unlike the *allocx() API, the standard API does not officially round up the usable size of an allocation to the nearest size class, so technically it is necessary to call realloc() to grow e.g. a 9-byte allocation to 16 bytes, or shrink a 16-byte allocation to 9 bytes. Growth and shrinkage trivially succeeds in place as long as the pre-size and post-size both round up to the same size class. No other API guarantees are made regarding in-place resizing, but the current implementation also tries to resize large allocations in place, as long as the pre-size and post-size are both large. For shrinkage to succeed, the extent allocator must support splitting (see [arena.<i>.extent_hooks](https://jemalloc.net/jemalloc.3.html#arena.i.extent_hooks)). Growth only succeeds if the trailing memory is currently available, and the extent allocator supports merging.\r\n```\r\n\r\n",
                  "createdAt": "2024-12-04T01:39:49Z",
                  "updatedAt": "2024-12-04T01:39:50Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "@sundb This is really helpful for my question 1.( Why does the memory usage increase to 72 bytes when I only add one additional character to the key?). Thanks for the answer.\r\n\r\nI remain confused about the memory calculation:\r\n\r\nThe key will use sds5 and the value will use sds8. ([https://cloud.tencent.com/developer/article/1837860](https://cloud.tencent.com/developer/article/1837860))\r\nThe new calculation will be the next:\r\n1. For the key, Redis object header (16B) + sds5(1B + 6B + 1B = 8B) = 24B\r\n2. For the value, Redis object header(16B) + sds8( len(1B) + alloc(1B) + flags(1B) + \"abcdef\" string(6B) + \\0 (1B) = 10B, **Will be to 16B. Am I right?**) \r\nThen will be 24B + 16B + 16B = 56B. And the dictEntry will be 24B at least. (**jemalloc will make it to 36B? Am i right?**)\r\nExceed the `MEMORY USAGE` result（64B）.\r\n\r\n\r\n",
                        "createdAt": "2024-12-04T06:40:57Z",
                        "updatedAt": "2024-12-04T06:40:58Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "hnzhrh",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "> 1. For the key, Redis object header (16B) + sds5(1B + 6B + 1B = 8B) = 24B\r\n\r\nno, key was saved as sds, not robj, so it's 8B.\r\n\r\n> 2. For the value, Redis object header(16B) + sds8( len(1B) + alloc(1B) + flags(1B) + \"abcdef\" string(6B) + \\0 (1B) = 10B, **Will be to 16B. Am I right?**)\r\n\r\nthere are two cases here:\r\n1. if the sds is `OBJ_ENCODING_RAW` encoding, consume 24B.\r\n2. if the sds is `OBJ_ENCODING_EMBSTR` encoding, consume 32B, because `robj` and `sds` are in contiguous memory, occupying a total of 24 bytes, so they are allocated to bin 32\r\n\r\n>    Then will be 24B + 16B + 16B = 56B. And the dictEntry will be 24B at least. (**jemalloc will make it to 36B? Am i right?**)\r\n\r\nso the memory is (24B or 32B) + 8B  + 24B(dictEntry)\r\n",
                        "createdAt": "2024-12-04T07:11:33Z",
                        "updatedAt": "2024-12-04T12:12:46Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "\r\n> no, value was saved as sds, not robj, so it's 8B.\r\n\r\nThanks for the answer. @sundb But I checked the source code. Looks like the value stored as redisObject. Am I missing something here? Thanks. The source code version is 7.0.\r\n\r\n![image](https://github.com/user-attachments/assets/eeb077fd-bddf-4a3f-8577-58642839ae87)\r\n\r\n\r\n```c\r\n/* Overwrite an existing key with a new value. Incrementing the reference\r\n * count of the new value is up to the caller.\r\n * This function does not modify the expire time of the existing key.\r\n *\r\n * The 'overwrite' flag is an indication whether this is done as part of a\r\n * complete replacement of their key, which can be thought as a deletion and\r\n * replacement (in which case we need to emit deletion signals), or just an\r\n * update of a value of an existing key (when false).\r\n *\r\n * The dictEntry input is optional, can be used if we already have one.\r\n *\r\n * The program is aborted if the key was not already present. */\r\nstatic void dbSetValue(redisDb *db, robj *key, robj *val, int overwrite, dictEntry *de) {\r\n    int slot = getKeySlot(key->ptr);\r\n    if (!de) de = kvstoreDictFind(db->keys, slot, key->ptr);\r\n    serverAssertWithInfo(NULL,key,de != NULL);\r\n    robj *old = dictGetVal(de);\r\n\r\n    val->lru = old->lru;\r\n\r\n    if (overwrite) {\r\n        /* RM_StringDMA may call dbUnshareStringValue which may free val, so we\r\n         * need to incr to retain old */\r\n        incrRefCount(old);\r\n        /* Although the key is not really deleted from the database, we regard\r\n         * overwrite as two steps of unlink+add, so we still need to call the unlink\r\n         * callback of the module. */\r\n        moduleNotifyKeyUnlink(key,old,db->id,DB_FLAG_KEY_OVERWRITE);\r\n        /* We want to try to unblock any module clients or clients using a blocking XREADGROUP */\r\n        signalDeletedKeyAsReady(db,key,old->type);\r\n        decrRefCount(old);\r\n        /* Because of RM_StringDMA, old may be changed, so we need get old again */\r\n        old = dictGetVal(de);\r\n    }\r\n    kvstoreDictSetVal(db->keys, slot, de, val);\r\n\r\n    /* if hash with HFEs, take care to remove from global HFE DS */\r\n    if (old->type == OBJ_HASH)\r\n        hashTypeRemoveFromExpires(&db->hexpires, old);\r\n\r\n    if (server.lazyfree_lazy_server_del) {\r\n        freeObjAsync(key,old,db->id);\r\n    } else {\r\n        decrRefCount(old);\r\n    }\r\n}\r\n\r\n// dict.c \r\nvoid dictSetVal(dict *d, dictEntry *de, void *val) {\r\n    assert(entryHasValue(de));\r\n    de->v.val = d->type->valDup ? d->type->valDup(d, val) : val;\r\n}\r\n\r\n```",
                        "createdAt": "2024-12-04T11:52:44Z",
                        "updatedAt": "2024-12-04T11:52:45Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "hnzhrh",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@hnzhrh sorry, my fault.\r\ni updated the [comment](https://github.com/redis/redis/discussions/13677#discussioncomment-11457729).\r\n\r\n",
                        "createdAt": "2024-12-04T12:13:03Z",
                        "updatedAt": "2024-12-04T12:13:05Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Thanks for the answer. This makes sense for me. Thanks again. I pasted the related code to record.\r\n```c\r\n/* Add the key to the DB. It's up to the caller to increment the reference\r\n * counter of the value if needed.\r\n *\r\n * If the update_if_existing argument is false, the program is aborted\r\n * if the key already exists, otherwise, it can fall back to dbOverwrite. */\r\nstatic dictEntry *dbAddInternal(redisDb *db, robj *key, robj *val, int update_if_existing) {\r\n    dictEntry *existing;\r\n    int slot = getKeySlot(key->ptr);\r\n    dictEntry *de = kvstoreDictAddRaw(db->keys, slot, key->ptr, &existing);\r\n    if (update_if_existing && existing) {\r\n        dbSetValue(db, key, val, 1, existing);\r\n        return existing;\r\n    }\r\n    serverAssertWithInfo(NULL, key, de != NULL);\r\n    kvstoreDictSetKey(db->keys, slot, de, sdsdup(key->ptr)); // Here set the key to sds.\r\n    initObjectLRUOrLFU(val);\r\n    kvstoreDictSetVal(db->keys, slot, de, val);\r\n    signalKeyAsReady(db, key, val->type);\r\n    notifyKeyspaceEvent(NOTIFY_NEW,\"new\",key,db->id);\r\n    return de;\r\n}\r\n```",
                        "createdAt": "2024-12-04T13:34:54Z",
                        "updatedAt": "2024-12-04T13:34:55Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "hnzhrh",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13648",
            "number": 13648,
            "title": "SET key overwrites LIST key, Is this a feature of a bug? To me it seems a bug",
            "body": "I was running redis through docker `docker run -d --name myredis -p 6379:6379 redis`, version is `7.4.1`. When I run in `redis-cli`:\r\n`SET foo bar`\r\n`GET foo`\r\n`LPUSH foo ele1 ele2`\r\nI get - `(error) WRONGTYPE Operation against a key holding the wrong kind of value`\r\n\r\nbut when I do opposite i.e.\r\n`LPUSH foo ele1 ele2`\r\n`LRANGE foo 0 2`\r\n`SET foo bar`\r\n`GET foo`\r\nI get - `bar`\r\n\r\nSET is given preference and my list key is overwritten by SET. Is it supposed to happen like this?\r\n![redis-issue-1](https://github.com/user-attachments/assets/b99de124-017d-4156-a198-657fa1229afe)\r\n",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": "RESOLVED",
            "createdAt": "2024-11-10T05:57:36Z",
            "updatedAt": "2024-11-15T06:12:16Z",
            "closedAt": "2024-11-15T06:12:16Z",
            "closed": true,
            "author": {
              "login": "thecuriouscoding",
              "__typename": "User"
            },
            "category": {
              "name": "General",
              "description": "Chat about anything and everything here",
              "slug": "general"
            },
            "isAnswered": null,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "The docs for \"set\" ( https://redis.io/docs/latest/commands/set/ ) literally\r\nsay \"If key already holds a value, it is overwritten, regardless of its\r\ntype.\"\r\n\r\nFundamentally, set is an operation that *replaces* an existing value (if\r\none), without needing to know anything about it; lpush *mutates* an\r\nexisting value (if one) - it needs the existing value to be a list to even\r\nthink about changing it in that way.\r\n\r\nOn Sun, 10 Nov 2024, 05:57 Curious Coding, ***@***.***> wrote:\r\n\r\n> I was running redis through docker docker run -d --name myredis -p\r\n> 6379:6379 redis, version is 7.4.1. When I run in redis-cli:\r\n> SET foo bar\r\n> GET foo\r\n> LPUSH foo ele1 ele2\r\n> I get - (error) WRONGTYPE Operation against a key holding the wrong kind\r\n> of value\r\n>\r\n> but when I do opposite i.e.\r\n> LPUSH foo ele1 ele2\r\n> LRANGE foo 0 2\r\n> SET foo bar\r\n> GET foo\r\n> I get - bar\r\n>\r\n> SET is given preference and my list key is overwritten by SET. Is it\r\n> supposed to happen like this?\r\n> redis-issue-1.png (view on web)\r\n> <https://github.com/user-attachments/assets/b99de124-017d-4156-a198-657fa1229afe>\r\n>\r\n> —\r\n> Reply to this email directly, view it on GitHub\r\n> <https://github.com/redis/redis/discussions/13648>, or unsubscribe\r\n> <https://github.com/notifications/unsubscribe-auth/AAAEHMB2ZSYUAJK26BGNQWTZ73YWPAVCNFSM6AAAAABRP3V5OKVHI2DSMVQWIX3LMV43ERDJONRXK43TNFXW4OZXGQ2TGMRRGY>\r\n> .\r\n> You are receiving this because you are subscribed to this thread.Message\r\n> ID: ***@***.***>\r\n>\r\n",
                  "createdAt": "2024-11-10T08:08:40Z",
                  "updatedAt": "2024-11-10T08:08:41Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "mgravell",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "Thanks",
                        "createdAt": "2024-11-15T06:11:47Z",
                        "updatedAt": "2024-11-15T06:11:47Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "thecuriouscoding",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13187",
            "number": 13187,
            "title": "Upgrade redis opensource 5 to 7",
            "body": "I wanna upgrade redis from 5 to 7. \r\nI think i will install newest redis version on other port and use the same datafile. could it be ok?\r\nHow can i do it ? is there any best practices to do this ?\r\nthanks.",
            "upvoteCount": 2,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-04-02T04:54:17Z",
            "updatedAt": "2024-11-09T01:46:20Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "huynv9699",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "first make sure you have a backup of your data file first, and then use it directly in the new version, which will automatically upgrade the old rdb to new version.",
                  "createdAt": "2024-04-02T04:59:40Z",
                  "updatedAt": "2024-04-02T04:59:41Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "@sundb How to make the upgrade in zero downtime in production env?\r\n",
                  "createdAt": "2024-04-04T05:37:35Z",
                  "updatedAt": "2024-04-04T05:37:35Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "Atom1010",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "are you using standard or cluster? I'm not expert on this, so I can only offer some advice.\r\nif is standard:\r\n```\r\n1. create a high version replication of the current master.\r\n2 waiting for the replication to sync to complete.\r\n3 using `failover` command to switch master to the new replication.\r\n4. directing your services to the new master.\r\n```\r\n\r\nbut in any case, it will be unavailable for a short time in the middle of 3) 4).\r\nbtw, it would be smoother in cluster mode or sentinel mode.",
                        "createdAt": "2024-04-04T08:27:17Z",
                        "updatedAt": "2024-04-04T08:27:32Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "hi @sundb ! i am interested in this topic, and would like to ask a follow up question... How about if I am using a non-clustered redis but just using Redis sentinel for HA... Can I apply as well these steps in general for my use case? Also, do I need to go through the version switches until I reach v7, or I can just directly upgrade from v5 to v7?\r\n\r\nThanks in advance for the answer!",
                        "createdAt": "2024-07-22T01:57:02Z",
                        "updatedAt": "2024-07-22T02:02:16Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "corneliohydie",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@corneliohydie yes, the upgrade process is smoother in sentinel mode, and you can use sentinel's failover to complete the switch master node.\r\n",
                        "createdAt": "2024-07-22T03:07:17Z",
                        "updatedAt": "2024-07-22T03:07:18Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "thanks @sundb !",
                        "createdAt": "2024-07-22T03:21:18Z",
                        "updatedAt": "2024-07-22T03:21:19Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "corneliohydie",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@huynv9699, please note that directly upgrading from Redis 5 to Redis 7.2 comes with certain risks, and you’ll need to have a rollback plan in place. I've created redis 5.0.3 and 7.2 test instance for my upgrade plan.I set up a Redis 5.0.3 (master-replica) and a Redis 7.2 (configured as replicas of the 5.0.3 master). Here’s what I observed:\r\n\r\nI was able to failover from the Redis 5.0.3 master to the Redis 7.2 replica using redis sentinel 6.2. However, once the 7.2 replica became the master, the Redis 5.0.3 instance lost connection to its new master (master_link_status:down).\r\nThe logs showed that \"partial resynchronization\" did not go through, prompting a \"Full resync,\" which led to the exception: \"Can't handle RDB format version 11.\"\r\nKey RDB version details:\r\n\r\nRedis 5.x RDB version: 9\r\nRedis 6.0 and 6.2: no changes to the RDB version\r\nRedis 7.0 RDB version: 10\r\nRedis 7.2 RDB version: 11\r\nGiven these differences, a safer upgrade path would be 5.x -> 6.2 -> 7.0 -> 7.2. However, I haven’t tested this path fully for backward compatibility yet.\r\n\r\n@sundb, any suggestions on a backward compatible upgrade path?\r\n\r\n",
                        "createdAt": "2024-11-08T09:43:26Z",
                        "updatedAt": "2024-11-08T09:43:27Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "gopivalleru",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@gopivalleru we do allow cross version upgraion.\r\nthe reason of `\"Can't handle RDB format version 11.\"` is that you upgrade the master first, then the repl can't reconize new rdb format.\r\ninstead you should upgrade the repl first, it can recognize the old version rdb from old version of master.",
                        "createdAt": "2024-11-08T09:50:56Z",
                        "updatedAt": "2024-11-08T09:50:56Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@sundb  Here’s what I did:\r\n\r\n1. Created the master and replica both on version 5.0.3.\r\n2. Added two new replicas to the master, both on version 7.2, intending to promote this pair as the new master and replica.\r\n3. Initiated a failover to one of the 7.2 replicas using Redis Sentinel 6.2.6.\r\n\r\nAfter the failover, here’s what I observed:\r\n\r\n1. The new master and replica on 7.2 are communicating as expected, but the 5.0.3 instances encountered an error with \"Can't handle RDB format version 11.\"\r\n\r\nThis highlights the issue: while upgrading from 5.0.3 to 7.2 is feasible, if any problems arise with 7.2, a downgrade is not possible, meaning I can’t switch the master back to one of the 5.0.3 nodes.",
                        "createdAt": "2024-11-08T16:34:36Z",
                        "updatedAt": "2024-11-08T16:44:29Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "gopivalleru",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@gopivalleru yes, so you must back up the 5.0.3 data before the upgrade, and manually restore the node when the upgrade fails.\r\nAt this point, the 5.0.3 node will no longer have traffic, so you can finally upgrade it to 7.x.\r\nUpgrading itself is a big risk, so it's safer to do a rehearsal in the development environment.",
                        "createdAt": "2024-11-09T01:46:20Z",
                        "updatedAt": "2024-11-09T01:47:08Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13643",
            "number": 13643,
            "title": "Inquiring about the Redis Licensing",
            "body": "Hi Community Members,\r\n\r\nWe are using Redis 7.4.2 in our production server. The configuration is attached below\r\n\r\n<img width=\"798\" alt=\"image\" src=\"https://github.com/user-attachments/assets/1ea16472-cb14-42eb-87cf-677989635f46\">\r\n\r\nNow with the license change https://redis.io/blog/redis-adopts-dual-source-available-licensing/\r\n\r\n![image](https://github.com/user-attachments/assets/68906821-f079-4c8a-b478-1ac56838e60a)\r\n\r\nWe are using Redis as a backend for our Metadata service within our organisation, it's not we are providing Redis as a service like cloud providers.\r\n\r\nNeed your guidance and clarification, before we explore other options like Valkey for instance\r\n\r\nThanks,\r\nSusmit\r\n\r\n",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-11-06T17:37:06Z",
            "updatedAt": "2024-11-07T09:25:09Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "Susmit07",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": true,
            "answerChosenAt": "2024-11-07T09:24:33Z",
            "answerChosenBy": {
              "login": "Susmit07",
              "__typename": "User"
            },
            "comments": {
              "nodes": [
                {
                  "body": "[gingebeard36](https://github.com/gingebeard36) [LiorKogan](https://github.com/LiorKogan) can you guys please help..",
                  "createdAt": "2024-11-07T04:24:19Z",
                  "updatedAt": "2024-11-07T04:24:19Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "Susmit07",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "<img width=\"707\" alt=\"image (2)\" src=\"https://github.com/user-attachments/assets/d9addba1-2879-4992-a068-779cf562d4c9\">\r\n\r\n<img width=\"1246\" alt=\"image (1)\" src=\"https://github.com/user-attachments/assets/a4e2bf6b-3271-45cf-9ae8-a0121b0805e7\">\r\n",
                  "createdAt": "2024-11-07T04:25:20Z",
                  "updatedAt": "2024-11-07T04:25:21Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "Susmit07",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "If source-available Redis 7.4.x or Redis Community Edition 8.x or later (that's the name since 8.0) is used strictly within your organization (\"You may not make the functionality of the Software or a Modified version available to third parties as a service or distribute the Software or a Modified version in a manner that makes the functionality of the Software available to third parties.\") - you most likely comply with RSALv2.\r\n\r\nYou mentioned using Redis 7.4.2, but [our latest source-available release (as of today) is 7.4.1](https://github.com/redis/redis/releases/tag/7.4.1).\r\n\r\n",
                  "createdAt": "2024-11-07T06:45:16Z",
                  "updatedAt": "2024-11-07T06:47:50Z",
                  "authorAssociation": "MEMBER",
                  "author": {
                    "login": "LiorKogan",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "My bad it's 7.4.1, yes we comply with RSALv2, in other words \r\n\r\nUsing Redis 7.4.x or later versions within my organization should be compliant with the RSALv2 license as long as we are not exposing Redis as a service to third parties or redistributing it in a way that would provide functionality to third parties. This means:\r\n\r\nWe can use Redis within our internal applications and infrastructure.\r\nWe can modify Redis as needed for our organization’s use, as long as the modifications and functionality aren’t distributed externally.\r\n\r\nI hope my above statements are true, will close if the discussion thread, if it sounds good",
                  "createdAt": "2024-11-07T09:25:09Z",
                  "updatedAt": "2024-11-07T09:40:38Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "Susmit07",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13625",
            "number": 13625,
            "title": "can rfs(Receive Flow Steering)  speed up redis performance?",
            "body": "when i try to use rfs(Receive Flow Steering)  speed up redis performance,  I got a opposite result，why?\r\n\r\n8 Core\r\n\r\nChannel parameters for eth0:\r\nPre-set maximums:\r\nRX:             0\r\nTX:             0\r\nOther:          0\r\nCombined:       2\r\nCurrent hardware settings:\r\nRX:             0\r\nTX:             0\r\nOther:          0\r\nCombined:       2\r\n\r\nshell:\r\n./redis-benchmark -n 1000000 -h ***** -p 6379 -a ****** -r 1000000 -t set,get,lpush,lpop -q\r\n\r\nbefore i use rfs:\r\n\r\nSET: 101142.91 requests per second, p50=0.431 msec\r\nGET: 104547.84 requests per second, p50=0.407 msec\r\nLPUSH: 104340.57 requests per second, p50=0.415 msec\r\nLPOP: 104177.52 requests per second, p50=0.415 msec\r\n\r\n\r\nwhen I use rfs:\r\nSET: 88144.55 requests per second, p50=0.495 msec\r\nGET: 92755.77 requests per second, p50=0.471 msec\r\nLPUSH: 89798.85 requests per second, p50=0.479 msec\r\nLPOP: 89055.12 requests per second, p50=0.487 msec\r\n\r\n\r\nrfs shell:\r\nenable:\r\necho \"32768\" > /proc/sys/net/core/rps_sock_flow_entries\r\necho \"16384\" > /sys/class/net/eth0/queues/rx-0/rps_flow_cnt\r\necho \"16384\" > /sys/class/net/eth0/queues/rx-1/rps_flow_cnt\r\n\r\ndisable:\r\necho \"0\" > /proc/sys/net/core/rps_sock_flow_entries\r\necho \"0\" > /sys/class/net/eth0/queues/rx-0/rps_flow_cnt\r\necho \"0\" > /sys/class/net/eth0/queues/rx-1/rps_flow_cnt\r\n\r\n\r\n",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-10-29T08:10:39Z",
            "updatedAt": "2024-10-29T08:10:40Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "iNanos",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": []
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13598",
            "number": 13598,
            "title": "Inquiry Regarding Redis Licensing Compliance",
            "body": "Dear Redis Licensing Team,\r\nI hope this message finds you well. I am writing to request clarification regarding the changes to Redis' licensing terms introduced with version 7.4 and later versions, as detailed on your website.\r\n\r\nPlanned Use Case:\r\n\r\nWe are in the process of developing a product that will integrate Redis within its architecture to provide internal caching services. Additionally, a sub-product within this solution will enable customers to deploy Redis on their Kubernetes platform via an open-source operator for internal use within their organization.\r\n\r\nIn this scenario, Redis would be deployed both internally within our product architecture and by our customers within their own infrastructure (Kubernetes-based), specifically for internal use only.\r\n\r\nQuestions:\r\n\r\nWould our sale of a product that includes Redis in this way, and enables its deployment via an open-source operator on a customer’s Kubernetes infrastructure, comply with Redis' 7.4+ licensing terms?\r\n\r\nWould our customer’s use of Redis, when deployed internally for their organizational purposes (without providing Redis as a service to third parties), also comply with these updated licensing terms?\r\n\r\nWe wish to ensure that our planned product and its usage by our customers adhere fully to Redis' licensing requirements. Your guidance on this matter would be greatly appreciated.\r\n\r\nThank you for your time and assistance.",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-10-14T03:20:24Z",
            "updatedAt": "2024-10-24T02:18:27Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "c10one",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "@c10one can you please provide more details about the nature of your product? Specifically, does it offers Redis functionality to your users, or does it use Redis internally?\r\n",
                  "createdAt": "2024-10-14T08:00:23Z",
                  "updatedAt": "2024-10-14T08:00:24Z",
                  "authorAssociation": "MEMBER",
                  "author": {
                    "login": "LiorKogan",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "The product provide Redis functionality(but use internally)",
                        "createdAt": "2024-10-14T08:02:16Z",
                        "updatedAt": "2024-10-14T08:02:16Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "c10one",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Let me ask it more clearly. Is your product competitive with Redis in some way, or does it address a completely different market and just use Redis internally?",
                        "createdAt": "2024-10-14T08:14:45Z",
                        "updatedAt": "2024-10-14T08:14:45Z",
                        "authorAssociation": "MEMBER",
                        "author": {
                          "login": "LiorKogan",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "We are developing an Internal Developer Platform (IDP) similar to platform engineering. We provide IDP platforms for our clients, and internal app developers at the client’s organization can request Redis instances through the IDP platform in a self-service manner for internal business use.\r\n\r\nBefore having the IDP, the process was that app developers would submit a ticket to the internal operations team, who would then create the Redis instance on a virtual machine and share the IP and password. With the IDP, operations staff no longer need to perform manual tasks. They just need to grant developers the necessary permissions and quotas, and the app developers can create Redis instances themselves via the IDP’s web UI. These Redis instances run on-demand in containers within the IDP's Kubernetes environment.\r\n\r\nOur contract with clients is for the IDP platform itself, and we do not charge extra based on the number of Redis instances created.",
                        "createdAt": "2024-10-14T08:29:19Z",
                        "updatedAt": "2024-10-14T08:37:28Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "c10one",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Based on this information, it seems that your usage of Redis is compatible with RSALv2. I just asked our legal team to review this conversation and approve it. I'll update.",
                        "createdAt": "2024-10-14T08:46:02Z",
                        "updatedAt": "2024-10-14T08:46:02Z",
                        "authorAssociation": "MEMBER",
                        "author": {
                          "login": "LiorKogan",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@LiorKogan  Thanks",
                        "createdAt": "2024-10-14T08:47:08Z",
                        "updatedAt": "2024-10-14T08:47:09Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "c10one",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@c10one , please email redis_licensing@redis.com and I will connect you with the right people. Thanks.",
                        "createdAt": "2024-10-14T17:18:44Z",
                        "updatedAt": "2024-10-14T17:18:46Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "gingebeard36",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@gingebeard36  OK",
                        "createdAt": "2024-10-15T02:15:24Z",
                        "updatedAt": "2024-10-15T02:15:26Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "c10one",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@LiorKogan @gingebeard36 The email has been sent. The title is “Inquiry Regarding Redis Licensing Compliance.” It would be greatly appreciated if you could reply to the email.",
                        "createdAt": "2024-10-17T06:33:29Z",
                        "updatedAt": "2024-10-17T07:33:49Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "c10one",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@c10one , I did not receive the email for some reason. I am on that alias. Can you please resend?",
                        "createdAt": "2024-10-23T21:07:27Z",
                        "updatedAt": "2024-10-23T21:07:28Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "gingebeard36",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@gingebeard36 Sure, I have resent the email. The subject is “Inquiry Regarding Redis Licensing Compliance.” It has been sent to redis_licensing@redis.com.",
                        "createdAt": "2024-10-24T02:18:27Z",
                        "updatedAt": "2024-10-24T02:18:28Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "c10one",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13574",
            "number": 13574,
            "title": "elastic reopen source, will redis reopen?",
            "body": "Elasticsearch is Open Source, Again\r\nhttps://www.elastic.co/cn/blog/elasticsearch-is-open-source-again",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-09-26T10:19:34Z",
            "updatedAt": "2024-09-26T10:19:34Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "chenshi5012",
              "__typename": "User"
            },
            "category": {
              "name": "General",
              "description": "Chat about anything and everything here",
              "slug": "general"
            },
            "isAnswered": null,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": []
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13569",
            "number": 13569,
            "title": "Does the pipeline have isolation?",
            "body": "Does the pipeline have isolation? I couldn't find the answer in the official documentation.\r\nI heard an explanation that if the command packaged by pipeline is sent to the server at once, pipeline can provide isolation guarantees, and other commands will not intervene between the pipelined commands.",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-09-24T14:18:58Z",
            "updatedAt": "2024-09-25T19:01:34Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "chcheng2930",
              "__typename": "User"
            },
            "category": {
              "name": "General",
              "description": "Chat about anything and everything here",
              "slug": "general"
            },
            "isAnswered": null,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "no, pipe doesn't guarantee the isolation.\nit is just used to reduce the number of round trips.",
                  "createdAt": "2024-09-24T15:27:30Z",
                  "updatedAt": "2024-09-24T15:27:31Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "Thanks for your answer. Have you seen the official documentation explaining this or how to verify it.",
                        "createdAt": "2024-09-25T10:48:02Z",
                        "updatedAt": "2024-09-25T10:48:03Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "chcheng2930",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                },
                {
                  "body": "Normally, it is a free-for-all between different connections - any connection with a complete request buffered is eligible to execute, and the server is free to balance load as it chooses. Individual request operations are effectively atomic. If you want multiple commands on a single connection to run as a contiguous pipeline without risk of other connections getting slices between operations, that is fine: just use a `MULTI`/`EXEC` block around the work to run sequentially. Or use Lua.",
                  "createdAt": "2024-09-25T19:01:34Z",
                  "updatedAt": "2024-09-25T19:01:34Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "mgravell",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13536",
            "number": 13536,
            "title": "Security support of redis 7.2",
            "body": "Hi,\r\nFrom https://www.versio.io/product-release-end-of-life-eol-Redis-Redis.html, I understand the maintenance support will end after mid of 2025 for redis7.2 release. However, it's not clear on the plan for end of security support. Could you please let me know the plan for end of security support of redis7.2 release? \r\nThanks.",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-09-11T10:38:34Z",
            "updatedAt": "2024-09-12T08:33:05Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "sivakp1",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": true,
            "answerChosenAt": "2024-09-12T08:33:05Z",
            "answerChosenBy": {
              "login": "sivakp1",
              "__typename": "User"
            },
            "comments": {
              "nodes": [
                {
                  "body": "It is an independent site and the information there seems to be incorrect.\r\n\r\nPlease see https://github.com/redis/redis/security/policy\r\n",
                  "createdAt": "2024-09-11T12:34:06Z",
                  "updatedAt": "2024-09-11T12:34:07Z",
                  "authorAssociation": "MEMBER",
                  "author": {
                    "login": "LiorKogan",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "Hi, \r\nThanks for your response and sharing the correct link. \r\nThe information on supported versions of Redis releases is captured on this link whereas I would like to have the info on EOL/EOS of these versions. Do we have this info captured in some docs/links?\r\n",
                  "createdAt": "2024-09-12T03:30:44Z",
                  "updatedAt": "2024-09-12T03:30:45Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "sivakp1",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "@sivakp1 the information you are looking for is there. For Redis Community Edition we don't have a fixed security support period. The security support period is based on the introduction of new versions as detailed.",
                  "createdAt": "2024-09-12T06:10:35Z",
                  "updatedAt": "2024-09-12T06:11:21Z",
                  "authorAssociation": "MEMBER",
                  "author": {
                    "login": "LiorKogan",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13523",
            "number": 13523,
            "title": "Fixed size values to enhance memory allocation efficiency",
            "body": "Considering key-value pairs, values can have an arbitrary length. What happens if I store fixed-size values? Will that affect Redis' memory allocation efficiency positively?",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-09-07T11:37:09Z",
            "updatedAt": "2024-09-08T09:20:29Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "kincsescsaba",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": true,
            "answerChosenAt": "2024-09-08T09:16:41Z",
            "answerChosenBy": {
              "login": "kincsescsaba",
              "__typename": "User"
            },
            "comments": {
              "nodes": [
                {
                  "body": "which data type are you referring to?\r\nif it's string type, because string is stored as variable size, whethere to store fixed-size has no affect to memory.",
                  "createdAt": "2024-09-07T12:02:51Z",
                  "updatedAt": "2024-09-07T12:02:52Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "Any data type can be suitable, to which the value can be converted. The question is: does Redis support fixed-size data types?\r\nOr can I add a custom type that is basically a fixed-size string?",
                        "createdAt": "2024-09-07T13:16:38Z",
                        "updatedAt": "2024-09-07T13:19:18Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "kincsescsaba",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "What is the intent of this datatype? when reach a fixed size of forbidden insertion?",
                        "createdAt": "2024-09-07T14:08:19Z",
                        "updatedAt": "2024-09-07T14:08:20Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Radically reducing memory usage for applications dealing with lots of data.",
                        "createdAt": "2024-09-07T14:30:32Z",
                        "updatedAt": "2024-09-07T14:30:33Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "kincsescsaba",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "does not bring benefits, in Redis length and memory usage is proportional, the shorter the length, the less memory it consumes.",
                        "createdAt": "2024-09-08T02:14:35Z",
                        "updatedAt": "2024-09-08T02:14:35Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "I would assume - without knowing Redis internals - that a varying value length can result in fragmented memory. Hence I thought about the fixed-size solution. Is there any way that can reduce fragmentation further from the extent Redis already handles this if I know the exact size of data units?",
                        "createdAt": "2024-09-08T07:55:29Z",
                        "updatedAt": "2024-09-08T07:55:30Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "kincsescsaba",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Reids use compact memory data struct `listpack` to optimize the storage of datatype, such as hash, sorted set, list, and set, when the size is small and the data size is small.\r\nthat means, when the size is small, the internal fragmentation rate is almost negligible.",
                        "createdAt": "2024-09-08T09:09:45Z",
                        "updatedAt": "2024-09-08T09:09:46Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "That's great! What is the threshold to consider a data unit small?",
                        "createdAt": "2024-09-08T09:16:36Z",
                        "updatedAt": "2024-09-08T09:16:37Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "kincsescsaba",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "hash\r\n```\r\nhash-max-listpack-entries 512\r\nhash-max-listpack-value 64\r\n```\r\n\r\nset\r\n```\r\nset-max-listpack-entries 128\r\nset-max-listpack-value 64\r\n```\r\n\r\nzset\r\n```\r\nzset-max-listpack-entries 128\r\nzset-max-listpack-value 64\r\n```\r\n\r\nlist use `listpack` internal by default.",
                        "createdAt": "2024-09-08T09:20:29Z",
                        "updatedAt": "2024-09-08T09:20:30Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13508",
            "number": 13508,
            "title": "ECCN for Redis (Below v7.4) ?",
            "body": "What is the ECCN to be considered for Open Source Redis ?\r\n\r\nHow to determine country of origion for redis ? ",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-08-30T10:20:46Z",
            "updatedAt": "2024-08-30T10:20:46Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "hemantmakhijani",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": []
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13484",
            "number": 13484,
            "title": "All possible env variables",
            "body": "Hi,\r\n\r\nis there a list of all possible ENV variables, i am having hard times navigating google lately for some reason.\r\n\r\n\r\ni'd like to avoid having custom redis.conf and am just wondering if I can set some things via env variables.\r\n\r\nI'm trying to run redis with docker and have config within docker-compose.yml file \r\n\r\nThanks\r\n",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-08-24T10:23:29Z",
            "updatedAt": "2024-08-26T05:27:58Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "primski",
              "__typename": "User"
            },
            "category": {
              "name": "General",
              "description": "Chat about anything and everything here",
              "slug": "general"
            },
            "isAnswered": null,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "@primski no, Reids doesn't use the ENV variables, the configuration is all.",
                  "createdAt": "2024-08-26T05:27:58Z",
                  "updatedAt": "2024-08-26T05:28:00Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13485",
            "number": 13485,
            "title": "Redis replica is out of sync with master but it does not attempts to resync to correct data",
            "body": "Hi, we have a setup of redis Sentinel with 3 sentinels and redis cluster with 1 master + 2 replicas. On Aug 16, we notice a failover log ,log 1 and 2 (these are the 2 replicas). Sentinels also recognize this event as well. After such event, we saw a change in our RDB copy on write background job frequency to 3600 seconds (previously was much lower as the log indicates).  We are not sure why this is the case\r\n\r\nAug 23: There was another failover in which a new leader was elected, however the data on the new master is now of the data from Aug 16 ( we don't know the exact version of data, but all the data we saw after the failover on the 23rd of Aug was now from the 16th of Aug). This means something happedned to all the data from the 16th of Aug till the 23rd of Aug, we are not sure what happens here, we just notice that the data is no longer there as expected\r\n\r\nDoes anyone know/ have experience this issue? How can we prevent this in the future? Could we get some explanation as to what could have potentially happened?\r\n\r\nAug 16th redis replica 1\r\n```\r\n1:S 16 Aug 2024 07:24:18.117 * Background saving terminated with success\r\n1:S 16 Aug 2024 07:45:00.285 * 1000000 changes in 900 seconds. Saving...\r\n1:S 16 Aug 2024 07:45:01.247 * Background saving started by pid 48919\r\n48919:C 16 Aug 2024 07:49:08.840 * DB saved on disk\r\n48919:C 16 Aug 2024 07:49:10.136 * RDB: 5622 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 07:49:11.671 * Background saving terminated with success\r\n1:S 16 Aug 2024 08:26:31.003 * 1000000 changes in 900 seconds. Saving...\r\n1:S 16 Aug 2024 08:26:31.937 * Background saving started by pid 57586\r\n57586:C 16 Aug 2024 08:30:11.263 * DB saved on disk\r\n57586:C 16 Aug 2024 08:30:12.410 * RDB: 51 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 08:30:13.531 * Background saving terminated with success\r\n1:S 16 Aug 2024 09:11:01.772 * 1000000 changes in 900 seconds. Saving...\r\n1:S 16 Aug 2024 09:11:02.812 * Background saving started by pid 66868\r\n66868:C 16 Aug 2024 09:14:36.494 * DB saved on disk\r\n66868:C 16 Aug 2024 09:14:37.445 * RDB: 55 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 09:14:38.523 * Background saving terminated with success\r\n1:S 16 Aug 2024 09:54:36.167 * 1000000 changes in 900 seconds. Saving...\r\n1:S 16 Aug 2024 09:54:37.063 * Background saving started by pid 75990\r\n75990:C 16 Aug 2024 09:58:10.875 * DB saved on disk\r\n75990:C 16 Aug 2024 09:58:11.954 * RDB: 68 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 09:58:13.142 * Background saving terminated with success\r\n1:S 16 Aug 2024 10:23:24.142 * 1000000 changes in 900 seconds. Saving...\r\n1:S 16 Aug 2024 10:23:25.024 * Background saving started by pid 81971\r\n81971:C 16 Aug 2024 10:26:58.712 * DB saved on disk\r\n81971:C 16 Aug 2024 10:26:59.880 * RDB: 385 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 10:27:01.095 * Background saving terminated with success\r\n1:S 16 Aug 2024 11:05:55.065 * 1000000 changes in 900 seconds. Saving...\r\n1:S 16 Aug 2024 11:05:55.917 * Background saving started by pid 90868\r\n90868:C 16 Aug 2024 11:09:30.369 * DB saved on disk\r\n90868:C 16 Aug 2024 11:09:31.458 * RDB: 105 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 11:09:32.591 * Background saving terminated with success\r\n1:M 16 Aug 2024 11:10:13.522 # Connection with master lost.\r\n1:M 16 Aug 2024 11:10:13.522 * Caching the disconnected master state.\r\n1:S 16 Aug 2024 11:10:13.523 * Connecting to MASTER 10.42.11.223:6379\r\n1:S 16 Aug 2024 11:10:13.523 * MASTER <-> REPLICA sync started\r\n1:S 16 Aug 2024 11:10:13.523 * REPLICAOF 10.42.11.223:6379 enabled (user request from 'id=15065195 addr=10.42.11.124:36484 laddr=10.42.12.172:6379 fd=29 name=sentinel-52c86644-cmd age=171088 idle=0 flags=x db=0 sub=0 psub=0 multi=4 qbuf=341 qbuf-free=40613 argv-mem=4 obl=45 oll=0 omem=0 tot-mem=61468 events=r cmd=exec user=default redir=-1')\r\n1:S 16 Aug 2024 11:10:13.524 # Could not create tmp config file (Read-only file system)\r\n1:S 16 Aug 2024 11:10:13.524 # CONFIG REWRITE failed: Invalid argument\r\n1:S 16 Aug 2024 11:10:13.527 * Non blocking connect for SYNC fired the event.\r\n1:S 16 Aug 2024 11:10:13.527 * Master replied to PING, replication can continue...\r\n1:S 16 Aug 2024 11:10:13.527 * Trying a partial resynchronization (request a3c177bec4a9c0da2cfc6c9a04b759e5c404d43a:50511431294838).\r\n1:S 16 Aug 2024 11:10:13.528 * Successful partial resynchronization with master.\r\n1:S 16 Aug 2024 11:10:13.528 # Master replication ID changed to 40a56ef60d8706827a53aff1c5c2823e57a1cd4e\r\n1:S 16 Aug 2024 11:10:13.528 * MASTER <-> REPLICA sync: Master accepted a Partial Resynchronization.\r\n1:S 16 Aug 2024 12:09:33.026 * 1 changes in 3600 seconds. Saving...\r\n1:S 16 Aug 2024 12:09:33.962 * Background saving started by pid 104112\r\n104112:C 16 Aug 2024 12:13:03.640 * DB saved on disk\r\n104112:C 16 Aug 2024 12:13:04.804 * RDB: 29 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 12:13:06.063 * Background saving terminated with success\r\n1:S 16 Aug 2024 13:13:07.082 * 1 changes in 3600 seconds. Saving...\r\n1:S 16 Aug 2024 13:13:08.012 * Background saving started by pid 117389\r\n117389:C 16 Aug 2024 13:16:36.343 * DB saved on disk\r\n117389:C 16 Aug 2024 13:16:37.406 * RDB: 67 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 13:16:38.569 * Background saving terminated with success\r\n1:S 16 Aug 2024 14:16:39.026 * 1 changes in 3600 seconds. Saving...\r\n1:S 16 Aug 2024 14:16:39.895 * Background saving started by pid 130640\r\n130640:C 16 Aug 2024 14:20:10.720 * DB saved on disk\r\n130640:C 16 Aug 2024 14:20:11.818 * RDB: 54 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 14:20:12.987 * Background saving terminated with success\r\n1:S 16 Aug 2024 15:20:13.091 * 1 changes in 3600 seconds. Saving...\r\n1:S 16 Aug 2024 15:20:13.955 * Background saving started by pid 13100\r\n13100:C 16 Aug 2024 15:23:43.607 * DB saved on disk\r\n13100:C 16 Aug 2024 15:23:44.694 * RDB: 24 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 15:23:45.801 * Background saving terminated with success\r\n1:S 16 Aug 2024 16:23:46.073 * 1 changes in 3600 seconds. Saving...\r\n1:S 16 Aug 2024 16:23:46.940 * Background saving started by pid 26377\r\n26377:C 16 Aug 2024 16:27:18.089 * DB saved on disk\r\n26377:C 16 Aug 2024 16:27:19.191 * RDB: 18 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 16:27:20.308 * Background saving terminated with success\r\n1:S 16 Aug 2024 17:27:21.038 * 1 changes in 3600 seconds. Saving...\r\n1:S 16 Aug 2024 17:27:21.983 * Background saving started by pid 39638\r\n39638:C 16 Aug 2024 17:30:50.835 * DB saved on disk\r\n39638:C 16 Aug 2024 17:30:51.951 * RDB: 17 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 17:30:53.177 * Background saving terminated with success\r\n1:S 16 Aug 2024 18:30:54.095 * 1 changes in 3600 seconds. Saving...\r\n1:S 16 Aug 2024 18:30:54.981 * Background saving started by pid 52892\r\n52892:C 16 Aug 2024 18:34:27.874 * DB saved on disk\r\n52892:C 16 Aug 2024 18:34:29.036 * RDB: 17 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 18:34:30.298 * Background saving terminated with success\r\n1:S 16 Aug 2024 19:34:31.037 * 1 changes in 3600 seconds. Saving...\r\n1:S 16 Aug 2024 19:34:31.920 * Background saving started by pid 66159\r\n66159:C 16 Aug 2024 19:38:05.053 * DB saved on disk\r\n```\r\n\r\nAug 16th log of replica 2 \r\n```\r\n1:S 16 Aug 2024 07:39:44.610 * Background saving terminated with success\r\n1:S 16 Aug 2024 08:12:00.661 * 1000000 changes in 900 seconds. Saving...\r\n1:S 16 Aug 2024 08:12:01.902 * Background saving started by pid 13023\r\n13023:C 16 Aug 2024 08:15:53.053 * DB saved on disk\r\n13023:C 16 Aug 2024 08:15:54.489 * RDB: 1552 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 08:15:56.117 * Background saving terminated with success\r\n1:S 16 Aug 2024 08:57:30.505 * 1000000 changes in 900 seconds. Saving...\r\n1:S 16 Aug 2024 08:57:32.000 * Background saving started by pid 22516\r\n22516:C 16 Aug 2024 09:01:21.169 * DB saved on disk\r\n22516:C 16 Aug 2024 09:01:22.759 * RDB: 801 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 09:01:24.342 * Background saving terminated with success\r\n1:S 16 Aug 2024 09:47:34.181 * 1000000 changes in 900 seconds. Saving...\r\n1:S 16 Aug 2024 09:47:35.463 * Background saving started by pid 32949\r\n32949:C 16 Aug 2024 09:51:24.688 * DB saved on disk\r\n32949:C 16 Aug 2024 09:51:26.060 * RDB: 169 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 09:51:27.326 * Background saving terminated with success\r\n1:S 16 Aug 2024 10:15:42.773 * 1000000 changes in 900 seconds. Saving...\r\n1:S 16 Aug 2024 10:15:44.135 * Background saving started by pid 38833\r\n38833:C 16 Aug 2024 10:19:17.755 * DB saved on disk\r\n38833:C 16 Aug 2024 10:19:19.043 * RDB: 171 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 10:19:20.285 * Background saving terminated with success\r\n1:S 16 Aug 2024 10:54:00.525 * 1000000 changes in 900 seconds. Saving...\r\n1:S 16 Aug 2024 10:54:01.939 * Background saving started by pid 46841\r\n46841:C 16 Aug 2024 10:57:49.135 * DB saved on disk\r\n46841:C 16 Aug 2024 10:57:50.642 * RDB: 42 MB of memory used by copy-on-write\r\n1:S 16 Aug 2024 10:57:52.112 * Background saving terminated with success\r\n1:M 16 Aug 2024 11:10:13.459 # Connection with master lost.\r\n1:M 16 Aug 2024 11:10:13.459 * Caching the disconnected master state.\r\n1:M 16 Aug 2024 11:10:13.459 * Discarding previously cached master state.\r\n1:M 16 Aug 2024 11:10:13.459 # Setting secondary replication ID to a3c177bec4a9c0da2cfc6c9a04b759e5c404d43a, valid up to offset: 50511431294838. New replication ID is 40a56ef60d8706827a53aff1c5c2823e57a1cd4e\r\n1:M 16 Aug 2024 11:10:13.459 * MASTER MODE enabled (user request from 'id=14395475 addr=10.42.11.124:54252 laddr=10.42.11.223:6379 fd=67 name=sentinel-52c86644-cmd age=171086 idle=0 flags=x db=0 sub=0 psub=0 multi=4 qbuf=202 qbuf-free=40752 argv-mem=4 obl=45 oll=0 omem=0 tot-mem=61468 events=r cmd=exec user=default redir=-1')\r\n1:M 16 Aug 2024 11:10:13.460 # Could not create tmp config file (Read-only file system)\r\n1:M 16 Aug 2024 11:10:13.460 # CONFIG REWRITE failed: Invalid argument\r\n1:M 16 Aug 2024 11:10:13.527 * Replica 10.42.12.172:6379 asks for synchronization\r\n1:M 16 Aug 2024 11:10:13.527 * Partial resynchronization request from 10.42.12.172:6379 accepted. Sending 164 bytes of backlog starting from offset 50511431294838.\r\n1:M 16 Aug 2024 11:57:53.021 * 1 changes in 3600 seconds. Saving...\r\n1:M 16 Aug 2024 11:57:54.317 * Background saving started by pid 55584\r\n55584:C 16 Aug 2024 12:01:28.084 * DB saved on disk\r\n55584:C 16 Aug 2024 12:01:29.607 * RDB: 29 MB of memory used by copy-on-write\r\n1:M 16 Aug 2024 12:01:31.002 * Background saving terminated with success\r\n1:M 16 Aug 2024 13:01:32.099 * 1 changes in 3600 seconds. Saving...\r\n1:M 16 Aug 2024 13:01:33.260 * Background saving started by pid 62762\r\n62762:C 16 Aug 2024 13:05:05.967 * DB saved on disk\r\n62762:C 16 Aug 2024 13:05:07.284 * RDB: 44 MB of memory used by copy-on-write\r\n1:M 16 Aug 2024 13:05:08.472 * Background saving terminated with success\r\n1:M 16 Aug 2024 14:05:09.041 * 1 changes in 3600 seconds. Saving...\r\n1:M 16 Aug 2024 14:05:10.250 * Background saving started by pid 69941\r\n69941:C 16 Aug 2024 14:08:35.671 * DB saved on disk\r\n69941:C 16 Aug 2024 14:08:36.915 * RDB: 26 MB of memory used by copy-on-write\r\n1:M 16 Aug 2024 14:08:38.229 * Background saving terminated with success\r\n1:M 16 Aug 2024 15:08:39.044 * 1 changes in 3600 seconds. Saving...\r\n1:M 16 Aug 2024 15:08:40.098 * Background saving started by pid 77095\r\n77095:C 16 Aug 2024 15:11:57.638 * DB saved on disk\r\n77095:C 16 Aug 2024 15:11:58.905 * RDB: 21 MB of memory used by copy-on-write\r\n1:M 16 Aug 2024 15:12:00.274 * Background saving terminated with success\r\n1:M 16 Aug 2024 16:12:01.043 * 1 changes in 3600 seconds. Saving...\r\n1:M 16 Aug 2024 16:12:02.135 * Background saving started by pid 84231\r\n84231:C 16 Aug 2024 16:15:20.383 * DB saved on disk\r\n84231:C 16 Aug 2024 16:15:21.727 * RDB: 20 MB of memory used by copy-on-write\r\n1:M 16 Aug 2024 16:15:22.954 * Background saving terminated with success\r\n1:M 16 Aug 2024 17:15:23.016 * 1 changes in 3600 seconds. Saving...\r\n1:M 16 Aug 2024 17:15:24.174 * Background saving started by pid 91354\r\n91354:C 16 Aug 2024 17:18:45.153 * DB saved on disk\r\n91354:C 16 Aug 2024 17:18:46.417 * RDB: 41 MB of memory used by copy-on-write\r\n1:M 16 Aug 2024 17:18:47.708 * Background saving terminated with success\r\n1:M 16 Aug 2024 18:18:48.006 * 1 changes in 3600 seconds. Saving...\r\n1:M 16 Aug 2024 18:18:49.120 * Background saving started by pid 98485\r\n98485:C 16 Aug 2024 18:22:04.456 * DB saved on disk\r\n```\r\n\r\nredis master log Aug 23\r\n```\r\n61964:C 23 Aug 2024 16:39:58.453 * DB saved on disk\r\n61964:C 23 Aug 2024 16:39:59.624 * RDB: 30 MB of memory used by copy-on-write\r\n1:M 23 Aug 2024 16:40:00.801 * Background saving terminated with success\r\n1:M 23 Aug 2024 16:45:01.006 * 10 changes in 300 seconds. Saving...\r\n1:M 23 Aug 2024 16:45:11.922 * Background saving started by pid 62878\r\n1:S 23 Aug 2024 16:47:43.207 * Before turning into a replica, using my own master parameters to synthesize a cached master: I may be able to synchronize with the new master with just a partial transfer.\r\n1:S 23 Aug 2024 16:47:43.207 * Connecting to MASTER 10.42.11.223:6379\r\n1:S 23 Aug 2024 16:47:43.208 * MASTER <-> REPLICA sync started\r\n1:S 23 Aug 2024 16:47:43.208 * REPLICAOF 10.42.11.223:6379 enabled (user request from 'id=17183851 addr=10.42.11.208:55650 laddr=10.42.3.228:6379 fd=32 name= age=2 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=46 qbuf-free=40908 argv-mem=23 obl=0 oll=0 omem=0 tot-mem=61487 events=r cmd=slaveof user=default redir=-1')\r\n1:S 23 Aug 2024 16:47:43.410 * Non blocking connect for SYNC fired the event.\r\n1:S 23 Aug 2024 16:47:43.410 * Master replied to PING, replication can continue...\r\n1:S 23 Aug 2024 16:47:46.188 * Trying a partial resynchronization (request aee9f0b46e010e7bee507f9d50a95872182408ff:50513460264540).\r\n1:S 23 Aug 2024 16:48:01.236 * Full resync from master: 40a56ef60d8706827a53aff1c5c2823e57a1cd4e:50519748340233\r\n1:S 23 Aug 2024 16:48:01.236 * Discarding previously cached master state.\r\n62878:C 23 Aug 2024 16:50:55.821 * DB saved on disk\r\n62878:C 23 Aug 2024 16:50:56.947 * RDB: 41 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 16:50:58.159 * Background saving terminated with success\r\n1:S 23 Aug 2024 16:51:08.052 * MASTER <-> REPLICA sync: receiving 10090330061 bytes from master to disk\r\n1:S 23 Aug 2024 16:51:41.589 * MASTER <-> REPLICA sync: Flushing old data\r\n1:S 23 Aug 2024 16:52:15.831 * MASTER <-> REPLICA sync: Loading DB in memory\r\n1:S 23 Aug 2024 16:52:15.831 * Loading RDB produced by version 6.2.6\r\n1:S 23 Aug 2024 16:52:15.831 * RDB age 268 seconds\r\n1:S 23 Aug 2024 16:52:15.831 * RDB memory usage when created 34948.47 Mb\r\n1:S 23 Aug 2024 16:53:53.814 # Done loading RDB, keys loaded: 11520987, keys expired: 0.\r\n1:S 23 Aug 2024 16:53:53.814 * MASTER <-> REPLICA sync: Finished with success\r\n1:S 23 Aug 2024 16:55:59.080 * 10 changes in 300 seconds. Saving...\r\n1:S 23 Aug 2024 16:55:59.967 * Background saving started by pid 64412\r\n64412:C 23 Aug 2024 16:59:01.738 * DB saved on disk\r\n64412:C 23 Aug 2024 16:59:02.851 * RDB: 21 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 16:59:03.934 * Background saving terminated with success\r\n1:S 23 Aug 2024 17:04:04.032 * 10 changes in 300 seconds. Saving...\r\n1:S 23 Aug 2024 17:04:04.884 * Background saving started by pid 878\r\n878:C 23 Aug 2024 17:07:06.796 * DB saved on disk\r\n878:C 23 Aug 2024 17:07:07.940 * RDB: 21 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 17:07:09.070 * Background saving terminated with success\r\n1:S 23 Aug 2024 17:12:10.073 * 10 changes in 300 seconds. Saving...\r\n1:S 23 Aug 2024 17:12:10.932 * Background saving started by pid 2594\r\n2594:C 23 Aug 2024 17:15:14.369 * DB saved on disk\r\n2594:C 23 Aug 2024 17:15:15.495 * RDB: 79 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 17:15:16.835 * Background saving terminated with success\r\n1:S 23 Aug 2024 17:20:17.088 * 10 changes in 300 seconds. Saving...\r\n1:S 23 Aug 2024 17:20:17.912 * Background saving started by pid 4329\r\n4329:C 23 Aug 2024 17:23:19.516 * DB saved on disk\r\n4329:C 23 Aug 2024 17:23:20.612 * RDB: 12 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 17:23:21.748 * Background saving terminated with success\r\n1:S 23 Aug 2024 17:28:22.036 * 10 changes in 300 seconds. Saving...\r\n1:S 23 Aug 2024 17:28:22.894 * Background saving started by pid 6010\r\n6010:C 23 Aug 2024 17:31:27.736 * DB saved on disk\r\n6010:C 23 Aug 2024 17:31:28.830 * RDB: 63 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 17:31:29.764 * Background saving terminated with success\r\n1:S 23 Aug 2024 17:36:30.085 * 10 changes in 300 seconds. Saving...\r\n1:S 23 Aug 2024 17:36:30.932 * Background saving started by pid 7733\r\n7733:C 23 Aug 2024 17:39:37.340 * DB saved on disk\r\n7733:C 23 Aug 2024 17:39:38.500 * RDB: 9 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 17:39:39.669 * Background saving terminated with success\r\n1:S 23 Aug 2024 17:44:40.091 * 10 changes in 300 seconds. Saving...\r\n1:S 23 Aug 2024 17:44:40.925 * Background saving started by pid 9447\r\n9447:C 23 Aug 2024 17:47:46.156 * DB saved on disk\r\n9447:C 23 Aug 2024 17:47:47.280 * RDB: 70 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 17:47:48.537 * Background saving terminated with success\r\n1:S 23 Aug 2024 17:52:49.097 * 10 changes in 300 seconds. Saving...\r\n1:S 23 Aug 2024 17:52:49.965 * Background saving started by pid 11175\r\n11175:C 23 Aug 2024 17:55:52.094 * DB saved on disk\r\n11175:C 23 Aug 2024 17:55:53.214 * RDB: 16 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 17:55:54.386 * Background saving terminated with success\r\n1:S 23 Aug 2024 18:00:55.036 * 10 changes in 300 seconds. Saving...\r\n1:S 23 Aug 2024 18:00:55.893 * Background saving started by pid 12914\r\n12914:C 23 Aug 2024 18:04:03.381 * DB saved on disk\r\n12914:C 23 Aug 2024 18:04:04.474 * RDB: 17 MB of memory used by copy-on-write\r\n```\r\n\r\nredis replica 1 log during the time of failover on Aug 23rd\r\n```\r\n520:C 23 Aug 2024 13:03:55.320 * RDB: 12 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 13:03:56.740 * Background saving terminated with success\r\n1:S 23 Aug 2024 14:03:57.096 * 1 changes in 3600 seconds. Saving...\r\n1:S 23 Aug 2024 14:03:58.580 * Background saving started by pid 13837\r\n13837:C 23 Aug 2024 14:07:26.144 * DB saved on disk\r\n13837:C 23 Aug 2024 14:07:27.395 * RDB: 13 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 14:07:28.787 * Background saving terminated with success\r\n1:S 23 Aug 2024 15:07:29.014 * 1 changes in 3600 seconds. Saving...\r\n1:S 23 Aug 2024 15:07:29.985 * Background saving started by pid 27160\r\n27160:C 23 Aug 2024 15:10:57.243 * DB saved on disk\r\n27160:C 23 Aug 2024 15:10:58.546 * RDB: 12 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 15:10:59.654 * Background saving terminated with success\r\n1:S 23 Aug 2024 16:11:00.073 * 1 changes in 3600 seconds. Saving...\r\n1:S 23 Aug 2024 16:11:01.069 * Background saving started by pid 40474\r\n40474:C 23 Aug 2024 16:14:26.251 * DB saved on disk\r\n40474:C 23 Aug 2024 16:14:27.469 * RDB: 12 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 16:14:28.825 * Background saving terminated with success\r\n1:S 23 Aug 2024 16:47:43.470 * REPLICAOF would result into synchronization with the master we are already connected with. No operation performed.\r\n1:S 23 Aug 2024 17:14:29.091 * 1 changes in 3600 seconds. Saving...\r\n1:S 23 Aug 2024 17:14:30.090 * Background saving started by pid 53808\r\n53808:C 23 Aug 2024 17:17:58.267 * DB saved on disk\r\n53808:C 23 Aug 2024 17:17:59.484 * RDB: 149 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 17:18:00.875 * Background saving terminated with success\r\n1:S 23 Aug 2024 18:18:01.046 * 1 changes in 3600 seconds. Saving...\r\n1:S 23 Aug 2024 18:18:02.073 * Background saving started by pid 67129\r\n67129:C 23 Aug 2024 18:21:35.719 * DB saved on disk\r\n67129:C 23 Aug 2024 18:21:36.936 * RDB: 28 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 18:21:38.144 * Background saving terminated with success\r\n1:S 23 Aug 2024 19:21:39.077 * 1 changes in 3600 seconds. Saving...\r\n1:S 23 Aug 2024 19:21:40.082 * Background saving started by pid 80499\r\n80499:C 23 Aug 2024 19:25:11.287 * DB saved on disk\r\n80499:C 23 Aug 2024 19:25:12.501 * RDB: 21 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 19:25:13.760 * Background saving terminated with success\r\n1:S 23 Aug 2024 20:25:14.082 * 1 changes in 3600 seconds. Saving...\r\n1:S 23 Aug 2024 20:25:15.170 * Background saving started by pid 93820\r\n93820:C 23 Aug 2024 20:28:50.358 * DB saved on disk\r\n93820:C 23 Aug 2024 20:28:51.449 * RDB: 29 MB of memory used by copy-on-write\r\n1:S 23 Aug 2024 20:28:52.639 * Background saving terminated with success\r\n1:S 23 Aug 2024 21:28:53.053 * 1 changes in 3600 seconds. Saving...\r\n1:S 23 Aug 2024 21:28:54.007 * Background saving started by pid 107165\r\n107165:C 23 Aug 2024 21:32:31.711 * DB saved on disk\r\n```\r\n\r\nredis replica 2 log during the time of failover on Aug 23rd\r\n```\r\n1:M 23 Aug 2024 13:52:07.844 * Background saving terminated with success\r\n1:M 23 Aug 2024 14:52:08.020 * 1 changes in 3600 seconds. Saving...\r\n1:M 23 Aug 2024 14:52:09.309 * Background saving started by pid 41185\r\n41185:C 23 Aug 2024 14:55:33.009 * DB saved on disk\r\n41185:C 23 Aug 2024 14:55:34.368 * RDB: 11 MB of memory used by copy-on-write\r\n1:M 23 Aug 2024 14:55:35.608 * Background saving terminated with success\r\n1:M 23 Aug 2024 15:55:36.019 * 1 changes in 3600 seconds. Saving...\r\n1:M 23 Aug 2024 15:55:37.339 * Background saving started by pid 48433\r\n48433:C 23 Aug 2024 15:59:04.236 * DB saved on disk\r\n48433:C 23 Aug 2024 15:59:05.616 * RDB: 11 MB of memory used by copy-on-write\r\n1:M 23 Aug 2024 15:59:07.067 * Background saving terminated with success\r\n1:M 23 Aug 2024 16:47:46.188 * Replica 10.42.3.228:6379 asks for synchronization\r\n1:M 23 Aug 2024 16:47:46.188 * Partial resynchronization not accepted: Replication ID mismatch (Replica asked for 'aee9f0b46e010e7bee507f9d50a95872182408ff', my replication IDs are '40a56ef60d8706827a53aff1c5c2823e57a1cd4e' and 'a3c177bec4a9c0da2cfc6c9a04b759e5c404d43a')\r\n1:M 23 Aug 2024 16:47:46.188 * Starting BGSAVE for SYNC with target: disk\r\n1:M 23 Aug 2024 16:47:47.449 * Background saving started by pid 54363\r\n54363:C 23 Aug 2024 16:51:05.407 * DB saved on disk\r\n54363:C 23 Aug 2024 16:51:06.820 * RDB: 11 MB of memory used by copy-on-write\r\n1:M 23 Aug 2024 16:51:08.052 * Background saving terminated with success\r\n1:M 23 Aug 2024 16:51:41.589 * Synchronization with replica 10.42.3.228:6379 succeeded\r\n1:M 23 Aug 2024 17:51:09.073 * 1 changes in 3600 seconds. Saving...\r\n1:M 23 Aug 2024 17:51:10.278 * Background saving started by pid 61594\r\n61594:C 23 Aug 2024 17:54:32.622 * DB saved on disk\r\n61594:C 23 Aug 2024 17:54:33.997 * RDB: 24 MB of memory used by copy-on-write\r\n1:M 23 Aug 2024 17:54:35.149 * Background saving terminated with success\r\n1:M 23 Aug 2024 18:54:36.096 * 1 changes in 3600 seconds. Saving...\r\n1:M 23 Aug 2024 18:54:37.160 * Background saving started by pid 68796\r\n68796:C 23 Aug 2024 18:57:58.673 * DB saved on disk\r\n68796:C 23 Aug 2024 18:57:59.995 * RDB: 17 MB of memory used by copy-on-write\r\n1:M 23 Aug 2024 18:58:01.302 * Background saving terminated with success\r\n1:M 23 Aug 2024 19:58:02.098 * 1 changes in 3600 seconds. Saving...\r\n1:M 23 Aug 2024 19:58:03.162 * Background saving started by pid 76029\r\n76029:C 23 Aug 2024 20:01:21.262 * DB saved on disk\r\n76029:C 23 Aug 2024 20:01:22.267 * RDB: 49 MB of memory used by copy-on-write\r\n1:M 23 Aug 2024 20:01:23.320 * Background saving terminated with success\r\n1:M 23 Aug 2024 21:01:24.073 * 1 changes in 3600 seconds. Saving...\r\n1:M 23 Aug 2024 21:01:25.185 * Background saving started by pid 83239\r\n83239:C 23 Aug 2024 21:04:47.258 * DB saved on disk\r\n```\r\n\r\nsentinel log on Aug 16th + Aug 23rd\r\n```\r\n1:X 16 Aug 2024 11:04:49.068 # +set master mymaster 10.42.3.228 6379 failover-timeout 10000\r\n1:X 16 Aug 2024 11:10:13.092 # +sdown master mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:13.192 # +odown master mymaster 10.42.3.228 6379 #quorum 2/2\r\n1:X 16 Aug 2024 11:10:13.192 # +new-epoch 40\r\n1:X 16 Aug 2024 11:10:13.192 # +try-failover master mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:13.195 # +vote-for-leader 52c866444177ce54ef81b88023651af159ef958d 40\r\n1:X 16 Aug 2024 11:10:13.199 # 4edf0b542a3fc32c3f1fefb6700ee01754030dea voted for 52c866444177ce54ef81b88023651af159ef958d 40\r\n1:X 16 Aug 2024 11:10:13.200 # 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 voted for 52c866444177ce54ef81b88023651af159ef958d 40\r\n1:X 16 Aug 2024 11:10:13.267 # +elected-leader master mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:13.267 # +failover-state-select-slave master mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:13.358 # +selected-slave slave 10.42.11.223:6379 10.42.11.223 6379 @ mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:13.358 * +failover-state-send-slaveof-noone slave 10.42.11.223:6379 10.42.11.223 6379 @ mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:13.459 * +failover-state-wait-promotion slave 10.42.11.223:6379 10.42.11.223 6379 @ mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:13.465 # +promoted-slave slave 10.42.11.223:6379 10.42.11.223 6379 @ mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:13.465 # +failover-state-reconf-slaves master mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:13.522 * +slave-reconf-sent slave 10.42.12.172:6379 10.42.12.172 6379 @ mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:14.330 # -odown master mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:14.463 * +slave-reconf-inprog slave 10.42.12.172:6379 10.42.12.172 6379 @ mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:14.463 * +slave-reconf-done slave 10.42.12.172:6379 10.42.12.172 6379 @ mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:14.553 # +failover-end master mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:14.553 # +switch-master mymaster 10.42.3.228 6379 10.42.11.223 6379\r\n1:X 16 Aug 2024 11:10:14.554 * +slave slave 10.42.12.172:6379 10.42.12.172 6379 @ mymaster 10.42.11.223 6379\r\n1:X 16 Aug 2024 11:10:14.554 * +slave slave 10.42.3.228:6379 10.42.3.228 6379 @ mymaster 10.42.11.223 6379\r\n1:X 16 Aug 2024 11:10:19.572 # +sdown slave 10.42.3.228:6379 10.42.3.228 6379 @ mymaster 10.42.11.223 6379\r\n1:X 16 Aug 2024 11:10:27.503 # -sdown slave 10.42.3.228:6379 10.42.3.228 6379 @ mymaster 10.42.11.223 6379\r\n1:X 16 Aug 2024 11:10:27.769 # -monitor master mymaster 10.42.11.223 6379\r\n1:X 16 Aug 2024 11:10:27.775 # +monitor master mymaster 10.42.3.228 6379 quorum 2\r\n1:X 16 Aug 2024 11:10:27.775 # +set master mymaster 10.42.3.228 6379 auth-pass MmNUWUExQ014c3lZYWI1YnBZSHZVZw==\r\n1:X 16 Aug 2024 11:10:27.972 # +reset-master master mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:27.977 # +reset-master master mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:28.071 # +set master mymaster 10.42.3.228 6379 down-after-milliseconds 5000\r\n1:X 16 Aug 2024 11:10:28.073 # +set master mymaster 10.42.3.228 6379 failover-timeout 10000\r\n1:X 16 Aug 2024 11:10:29.957 * +sentinel sentinel 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 10.42.12.171 26379 @ mymaster 10.42.3.228 6379\r\n1:X 16 Aug 2024 11:10:30.043 * +sentinel sentinel 4edf0b542a3fc32c3f1fefb6700ee01754030dea 10.42.6.21 26379 @ mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:45:06.304 # +new-epoch 41\r\n1:X 23 Aug 2024 16:45:06.306 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 41\r\n1:X 23 Aug 2024 16:45:07.042 # +sdown master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:45:07.101 # +odown master mymaster 10.42.3.228 6379 #quorum 3/2\r\n1:X 23 Aug 2024 16:45:07.101 # Next failover delay: I will not start a failover before Fri Aug 23 16:45:26 2024\r\n1:X 23 Aug 2024 16:45:13.279 # -sdown master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:45:13.280 # -odown master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:45:33.429 # +new-epoch 42\r\n1:X 23 Aug 2024 16:45:33.431 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 42\r\n1:X 23 Aug 2024 16:45:33.463 # +sdown master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:45:33.564 # +odown master mymaster 10.42.3.228 6379 #quorum 3/2\r\n1:X 23 Aug 2024 16:45:33.564 # Next failover delay: I will not start a failover before Fri Aug 23 16:45:53 2024\r\n1:X 23 Aug 2024 16:45:40.382 # -sdown master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:45:40.382 # -odown master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:45:46.380 # +sdown master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:45:46.471 # +odown master mymaster 10.42.3.228 6379 #quorum 2/2\r\n1:X 23 Aug 2024 16:45:54.000 # +new-epoch 43\r\n1:X 23 Aug 2024 16:45:54.000 # +try-failover master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:45:54.004 # +vote-for-leader 52c866444177ce54ef81b88023651af159ef958d 43\r\n1:X 23 Aug 2024 16:45:54.009 # 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 voted for 52c866444177ce54ef81b88023651af159ef958d 43\r\n1:X 23 Aug 2024 16:45:54.013 # 4edf0b542a3fc32c3f1fefb6700ee01754030dea voted for 52c866444177ce54ef81b88023651af159ef958d 43\r\n1:X 23 Aug 2024 16:45:54.104 # +elected-leader master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:45:54.104 # +failover-state-select-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:45:54.157 # -failover-abort-no-good-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:45:54.229 # Next failover delay: I will not start a failover before Fri Aug 23 16:46:14 2024\r\n1:X 23 Aug 2024 16:46:14.102 # +new-epoch 44\r\n1:X 23 Aug 2024 16:46:14.104 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 44\r\n1:X 23 Aug 2024 16:46:14.116 # Next failover delay: I will not start a failover before Fri Aug 23 16:46:34 2024\r\n1:X 23 Aug 2024 16:46:34.458 # +new-epoch 45\r\n1:X 23 Aug 2024 16:46:34.458 # +try-failover master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:46:34.461 # +vote-for-leader 52c866444177ce54ef81b88023651af159ef958d 45\r\n1:X 23 Aug 2024 16:46:34.464 # 4edf0b542a3fc32c3f1fefb6700ee01754030dea voted for 52c866444177ce54ef81b88023651af159ef958d 45\r\n1:X 23 Aug 2024 16:46:34.466 # 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 voted for 52c866444177ce54ef81b88023651af159ef958d 45\r\n1:X 23 Aug 2024 16:46:34.551 # +elected-leader master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:46:34.551 # +failover-state-select-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:46:34.611 # -failover-abort-no-good-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:46:34.678 # Next failover delay: I will not start a failover before Fri Aug 23 16:46:55 2024\r\n1:X 23 Aug 2024 16:46:55.277 # +new-epoch 46\r\n1:X 23 Aug 2024 16:46:55.279 # +vote-for-leader 4edf0b542a3fc32c3f1fefb6700ee01754030dea 46\r\n1:X 23 Aug 2024 16:46:55.296 # Next failover delay: I will not start a failover before Fri Aug 23 16:47:15 2024\r\n1:X 23 Aug 2024 16:47:15.639 # +new-epoch 47\r\n1:X 23 Aug 2024 16:47:15.641 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 47\r\n1:X 23 Aug 2024 16:47:15.641 # Next failover delay: I will not start a failover before Fri Aug 23 16:47:36 2024\r\n1:X 23 Aug 2024 16:47:35.778 # +new-epoch 48\r\n1:X 23 Aug 2024 16:47:35.780 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 48\r\n1:X 23 Aug 2024 16:47:35.833 # Next failover delay: I will not start a failover before Fri Aug 23 16:47:56 2024\r\n1:X 23 Aug 2024 16:47:56.044 # +new-epoch 49\r\n1:X 23 Aug 2024 16:47:56.044 # +try-failover master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:47:56.048 # +vote-for-leader 52c866444177ce54ef81b88023651af159ef958d 49\r\n1:X 23 Aug 2024 16:47:56.051 # 4edf0b542a3fc32c3f1fefb6700ee01754030dea voted for 52c866444177ce54ef81b88023651af159ef958d 49\r\n1:X 23 Aug 2024 16:47:56.053 # 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 voted for 52c866444177ce54ef81b88023651af159ef958d 49\r\n1:X 23 Aug 2024 16:47:56.114 # +elected-leader master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:47:56.114 # +failover-state-select-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:47:56.191 # -failover-abort-no-good-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:47:56.247 # Next failover delay: I will not start a failover before Fri Aug 23 16:48:16 2024\r\n1:X 23 Aug 2024 16:48:15.865 # -sdown master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:48:15.865 # -odown master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:48:21.893 # +sdown master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:48:22.017 # +new-epoch 50\r\n1:X 23 Aug 2024 16:48:22.018 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 50\r\n1:X 23 Aug 2024 16:48:23.016 # +odown master mymaster 10.42.3.228 6379 #quorum 2/2\r\n1:X 23 Aug 2024 16:48:23.016 # Next failover delay: I will not start a failover before Fri Aug 23 16:48:42 2024\r\n1:X 23 Aug 2024 16:48:24.606 # -sdown master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:48:24.606 # -odown master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:48:30.718 # +sdown master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:48:30.785 # +odown master mymaster 10.42.3.228 6379 #quorum 3/2\r\n1:X 23 Aug 2024 16:48:42.216 # +new-epoch 51\r\n1:X 23 Aug 2024 16:48:42.218 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 51\r\n1:X 23 Aug 2024 16:48:42.253 # Next failover delay: I will not start a failover before Fri Aug 23 16:49:02 2024\r\n1:X 23 Aug 2024 16:49:02.885 # +new-epoch 52\r\n1:X 23 Aug 2024 16:49:02.887 # +vote-for-leader 4edf0b542a3fc32c3f1fefb6700ee01754030dea 52\r\n1:X 23 Aug 2024 16:49:02.914 # Next failover delay: I will not start a failover before Fri Aug 23 16:49:23 2024\r\n1:X 23 Aug 2024 16:49:23.003 # +new-epoch 53\r\n1:X 23 Aug 2024 16:49:23.005 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 53\r\n1:X 23 Aug 2024 16:49:23.010 # Next failover delay: I will not start a failover before Fri Aug 23 16:49:43 2024\r\n1:X 23 Aug 2024 16:49:43.782 # +new-epoch 54\r\n1:X 23 Aug 2024 16:49:43.782 # +try-failover master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:49:43.785 # +vote-for-leader 52c866444177ce54ef81b88023651af159ef958d 54\r\n1:X 23 Aug 2024 16:49:43.788 # 4edf0b542a3fc32c3f1fefb6700ee01754030dea voted for 52c866444177ce54ef81b88023651af159ef958d 54\r\n1:X 23 Aug 2024 16:49:43.790 # 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 voted for 52c866444177ce54ef81b88023651af159ef958d 54\r\n1:X 23 Aug 2024 16:49:43.848 # +elected-leader master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:49:43.848 # +failover-state-select-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:49:43.906 # -failover-abort-no-good-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:49:43.990 # Next failover delay: I will not start a failover before Fri Aug 23 16:50:03 2024\r\n1:X 23 Aug 2024 16:50:03.886 # +new-epoch 55\r\n1:X 23 Aug 2024 16:50:03.886 # +try-failover master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:50:03.889 # +vote-for-leader 52c866444177ce54ef81b88023651af159ef958d 55\r\n1:X 23 Aug 2024 16:50:03.892 # 4edf0b542a3fc32c3f1fefb6700ee01754030dea voted for 52c866444177ce54ef81b88023651af159ef958d 55\r\n1:X 23 Aug 2024 16:50:03.895 # 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 voted for 52c866444177ce54ef81b88023651af159ef958d 55\r\n1:X 23 Aug 2024 16:50:03.951 # +elected-leader master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:50:03.951 # +failover-state-select-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:50:04.023 # -failover-abort-no-good-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:50:04.095 # Next failover delay: I will not start a failover before Fri Aug 23 16:50:24 2024\r\n1:X 23 Aug 2024 16:50:24.300 # +new-epoch 56\r\n1:X 23 Aug 2024 16:50:24.302 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 56\r\n1:X 23 Aug 2024 16:50:24.321 # Next failover delay: I will not start a failover before Fri Aug 23 16:50:45 2024\r\n1:X 23 Aug 2024 16:50:44.443 # +new-epoch 57\r\n1:X 23 Aug 2024 16:50:44.444 # +vote-for-leader 4edf0b542a3fc32c3f1fefb6700ee01754030dea 57\r\n1:X 23 Aug 2024 16:50:44.462 # Next failover delay: I will not start a failover before Fri Aug 23 16:51:05 2024\r\n1:X 23 Aug 2024 16:51:05.103 # +new-epoch 58\r\n1:X 23 Aug 2024 16:51:05.104 # +vote-for-leader 4edf0b542a3fc32c3f1fefb6700ee01754030dea 58\r\n1:X 23 Aug 2024 16:51:05.181 # Next failover delay: I will not start a failover before Fri Aug 23 16:51:25 2024\r\n1:X 23 Aug 2024 16:51:25.197 # +new-epoch 59\r\n1:X 23 Aug 2024 16:51:25.197 # +try-failover master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:51:25.200 # +vote-for-leader 52c866444177ce54ef81b88023651af159ef958d 59\r\n1:X 23 Aug 2024 16:51:25.204 # 4edf0b542a3fc32c3f1fefb6700ee01754030dea voted for 52c866444177ce54ef81b88023651af159ef958d 59\r\n1:X 23 Aug 2024 16:51:25.205 # 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 voted for 52c866444177ce54ef81b88023651af159ef958d 59\r\n1:X 23 Aug 2024 16:51:25.253 # +elected-leader master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:51:25.253 # +failover-state-select-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:51:25.325 # -failover-abort-no-good-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:51:25.401 # Next failover delay: I will not start a failover before Fri Aug 23 16:51:45 2024\r\n1:X 23 Aug 2024 16:51:45.420 # +new-epoch 60\r\n1:X 23 Aug 2024 16:51:45.422 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 60\r\n1:X 23 Aug 2024 16:51:45.458 # Next failover delay: I will not start a failover before Fri Aug 23 16:52:06 2024\r\n1:X 23 Aug 2024 16:52:06.041 # +new-epoch 61\r\n1:X 23 Aug 2024 16:52:06.042 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 61\r\n1:X 23 Aug 2024 16:52:06.047 # Next failover delay: I will not start a failover before Fri Aug 23 16:52:26 2024\r\n1:X 23 Aug 2024 16:52:26.111 # +new-epoch 62\r\n1:X 23 Aug 2024 16:52:26.111 # +try-failover master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:52:26.113 # +vote-for-leader 52c866444177ce54ef81b88023651af159ef958d 62\r\n1:X 23 Aug 2024 16:52:26.116 # 4edf0b542a3fc32c3f1fefb6700ee01754030dea voted for 52c866444177ce54ef81b88023651af159ef958d 62\r\n1:X 23 Aug 2024 16:52:26.119 # 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 voted for 52c866444177ce54ef81b88023651af159ef958d 62\r\n1:X 23 Aug 2024 16:52:26.190 # +elected-leader master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:52:26.190 # +failover-state-select-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:52:26.245 # -failover-abort-no-good-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:52:26.312 # Next failover delay: I will not start a failover before Fri Aug 23 16:52:46 2024\r\n1:X 23 Aug 2024 16:52:46.385 # +new-epoch 63\r\n1:X 23 Aug 2024 16:52:46.387 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 63\r\n1:X 23 Aug 2024 16:52:46.387 # Next failover delay: I will not start a failover before Fri Aug 23 16:53:07 2024\r\n1:X 23 Aug 2024 16:53:06.680 # +new-epoch 64\r\n1:X 23 Aug 2024 16:53:06.682 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 64\r\n1:X 23 Aug 2024 16:53:06.761 # Next failover delay: I will not start a failover before Fri Aug 23 16:53:27 2024\r\n1:X 23 Aug 2024 16:53:26.978 # +new-epoch 65\r\n1:X 23 Aug 2024 16:53:26.980 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 65\r\n1:X 23 Aug 2024 16:53:26.997 # Next failover delay: I will not start a failover before Fri Aug 23 16:53:47 2024\r\n1:X 23 Aug 2024 16:53:47.489 # +new-epoch 66\r\n1:X 23 Aug 2024 16:53:47.491 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 66\r\n1:X 23 Aug 2024 16:53:47.491 # Next failover delay: I will not start a failover before Fri Aug 23 16:54:07 2024\r\n1:X 23 Aug 2024 16:54:07.550 # +new-epoch 67\r\n1:X 23 Aug 2024 16:54:07.552 # +vote-for-leader 4edf0b542a3fc32c3f1fefb6700ee01754030dea 67\r\n1:X 23 Aug 2024 16:54:07.574 # Next failover delay: I will not start a failover before Fri Aug 23 16:54:28 2024\r\n1:X 23 Aug 2024 16:54:27.831 # +new-epoch 68\r\n1:X 23 Aug 2024 16:54:27.833 # +vote-for-leader 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 68\r\n1:X 23 Aug 2024 16:54:27.870 # Next failover delay: I will not start a failover before Fri Aug 23 16:54:47 2024\r\n1:X 23 Aug 2024 16:54:47.954 # +new-epoch 69\r\n1:X 23 Aug 2024 16:54:47.954 # +try-failover master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:54:47.957 # +vote-for-leader 52c866444177ce54ef81b88023651af159ef958d 69\r\n1:X 23 Aug 2024 16:54:47.960 # 4edf0b542a3fc32c3f1fefb6700ee01754030dea voted for 52c866444177ce54ef81b88023651af159ef958d 69\r\n1:X 23 Aug 2024 16:54:47.964 # 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 voted for 52c866444177ce54ef81b88023651af159ef958d 69\r\n1:X 23 Aug 2024 16:54:48.013 # +elected-leader master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:54:48.013 # +failover-state-select-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:54:48.075 # -failover-abort-no-good-slave master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:54:48.133 # Next failover delay: I will not start a failover before Fri Aug 23 16:55:08 2024\r\n1:X 23 Aug 2024 16:54:55.775 # -monitor master mymaster 10.42.3.228 6379\r\n1:X 23 Aug 2024 16:54:55.780 # +monitor master mymaster 10.42.11.223 6379 quorum 2\r\n1:X 23 Aug 2024 16:54:55.780 # +set master mymaster 10.42.11.223 6379 auth-pass MmNUWUExQ014c3lZYWI1YnBZSHZVZw==\r\n1:X 23 Aug 2024 16:54:55.808 * +slave slave 10.42.12.172:6379 10.42.12.172 6379 @ mymaster 10.42.11.223 6379\r\n1:X 23 Aug 2024 16:54:55.810 * +slave slave 10.42.3.228:6379 10.42.3.228 6379 @ mymaster 10.42.11.223 6379\r\n1:X 23 Aug 2024 16:54:55.975 # +reset-master master mymaster 10.42.11.223 6379\r\n1:X 23 Aug 2024 16:54:56.071 # +reset-master master mymaster 10.42.11.223 6379\r\n1:X 23 Aug 2024 16:54:56.074 # +set master mymaster 10.42.11.223 6379 down-after-milliseconds 5000\r\n1:X 23 Aug 2024 16:54:56.076 # +set master mymaster 10.42.11.223 6379 failover-timeout 10000\r\n1:X 23 Aug 2024 16:54:57.894 * +sentinel sentinel 9ff0b29dfce16d93f2dd4088f3d79f1c39d1b635 10.42.12.171 26379 @ mymaster 10.42.11.223 6379\r\n1:X 23 Aug 2024 16:54:58.016 * +sentinel sentinel 4edf0b542a3fc32c3f1fefb6700ee01754030dea 10.42.6.21 26379 @ mymaster 10.42.11.223 6379\r\n1:X 23 Aug 2024 16:55:05.875 * +slave slave 10.42.12.172:6379 10.42.12.172 6379 @ mymaster 10.42.11.223 6379\r\n1:X 23 Aug 2024 16:55:05.878 * +slave slave 10.42.3.228:6379 10.42.3.228 6379 @ mymaster 10.42.11.223 6379\r\n1:X 23 Aug 2024 17:01:04.670 # +set master mymaster 10.42.11.223 6379 down-after-milliseconds 5000\r\n1:X 23 Aug 2024 17:01:04.672 # +set master mymaster 10.42.11.223 6379 failover-timeout 10000\r\n1:X 23 Aug 2024 17:07:20.871 # +set master mymaster 10.42.11.223 6379 down-after-milliseconds 5000\r\n1:X 23 Aug 2024 17:07:20.874 # +set master mymaster 10.42.11.223 6379 failover-timeout 10000\r\n1:X 23 Aug 2024 17:13:36.769 # +set master mymaster 10.42.11.223 6379 down-after-milliseconds 5000\r\n1:X 23 Aug 2024 17:13:36.772 # +set master mymaster 10.42.11.223 6379 failover-timeout 10000\r\n1:X 23 Aug 2024 17:19:54.876 # +set master mymaster 10.42.11.223 6379 down-after-milliseconds 5000\r\n1:X 23 Aug 2024 17:19:54.968 # +set master mymaster 10.42.11.223 6379 failover-timeout 10000\r\n1:X 23 Aug 2024 17:26:20.271 # +set master mymaster 10.42.11.223 6379 down-after-milliseconds 5000\r\n1:X 23 Aug 2024 17:26:20.274 # +set master mymaster 10.42.11.223 6379 failover-timeout 10000\r\n1:X 23 Aug 2024 17:32:39.078 # +set master mymaster 10.42.11.223 6379 down-after-milliseconds 5000\r\n1:X 23 Aug 2024 17:32:39.168 # +set master mymaster 10.42.11.223 6379 failover-timeout 10000\r\n1:X 23 Aug 2024 17:38:48.870 # +set master mymaster 10.42.11.223 6379 down-after-milliseconds 5000\r\n1:X 23 Aug 2024 17:38:48.872 # +set master mymaster 10.42.11.223 6379 failover-timeout 10000\r\n1:X 23 Aug 2024 17:45:03.577 # +set master mymaster 10.42.11.223 6379 down-after-milliseconds 5000\r\n1:X 23 Aug 2024 17:45:03.668 # +set master mymaster 10.42.11.223 6379 failover-timeout 10000\r\n1:X 23 Aug 2024 17:51:08.871 # +set master mymaster 10.42.11.223 6379 down-after-milliseconds 5000\r\n1:X 23 Aug 2024 17:51:08.873 # +set master mymaster 10.42.11.223 6379 failover-timeout 10000\r\n1:X 23 Aug 2024 17:57:19.272 # +set master mymaster 10.42.11.223 6379 down-after-milliseconds 5000\r\n1:X 23 Aug 2024 17:57:19.276 # +set master mymaster 10.42.11.223 6379 failover-timeout 10000\r\n1:X 23 Aug 2024 18:03:36.669 # +set master mymaster 10.42.11.223 6379 down-after-milliseconds 5000\r\n1:X 23 Aug 2024 18:03:36.673 # +set master mymaster 10.42.11.223 6379 failover-timeout 10000\r\n1:X 23 Aug 2024 18:09:50.869 # +set master mymaster 10.42.11.223 6379 down-after-milliseconds 5000\r\n```\r\n",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-08-26T03:37:55Z",
            "updatedAt": "2024-08-26T03:40:26Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "NathanDo1",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": []
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13481",
            "number": 13481,
            "title": "Search a value inside multiple bounds",
            "body": "This is what I am trying to do; I have a bunch of freight table that is used on shipping quote proccess. To get correct value of a shipping, I've to verify if zip code is inside a VALUE RANGE and if product weight is inside on another VALUE RANGE, something like bellow:\r\n\r\n```\r\nHSET Frete:1 id 1 zip_code_begin 3433090 zip_code_end 3433100 weight_min 0.0000 weight_max 1000.0000 price 8.0000\r\nHSET Frete:2 id 2 zip_code_begin 3433090 zip_code_end 3433100 weight_min 1001.0000 weight_max 2000.0000 price 9.0000\r\nHSET Frete:3 id 3 zip_code_begin 3433090 zip_code_end 3433100 weight_min 2001.0000 weight_max 3000.0000 price 10.0000\r\nHSET Frete:4 id 4 zip_code_begin 3433090 zip_code_end 3433100 weight_min 3001.0000 weight_max 4000.0000 price 11.0000\r\nHSET Frete:5 id 5 zip_code_begin 3433090 zip_code_end 3433100 weight_min 4001.0000 weight_max 9999.9999 price 15.0000\r\n\r\nHSET Frete:6 id 6 zip_code_begin 3433101 zip_code_end 3433110 weight_min 0.0000 weight_max 1000.0000 price 8.0000\r\nHSET Frete:7 id 7 zip_code_begin 3433101 zip_code_end 3433110 weight_min 1001.0000 weight_max 2000.0000 price 9.0000\r\nHSET Frete:8 id 8 zip_code_begin 3433101 zip_code_end 3433110 weight_min 2001.0000 weight_max 3000.0000 price 10.0000\r\nHSET Frete:9 id 9 zip_code_begin 3433101 zip_code_end 3433110 weight_min 3001.0000 weight_max 4000.0000 price 11.0000\r\nHSET Frete:10 id 10 zip_code_begin 3433101 zip_code_end 3433110 weight_min 4001.0000 weight_max 9999.9999 price 15.0000\r\n```\r\n\r\nEveryime I'll search with 2 information: zip_code and product_weight, so, examples bellow with expected results:\r\n\r\nzip_code: 3433095 + product_weight: 800.00 => Frete:1\r\nzip_code: 3433095 + product_weight: 3200.00 => Frete:4\r\nzip_code: 3433107 + product_weight: 4500.00 => Frete:10\r\nzip_code: 3433107 + product_weight: 1000.00 => Frete:6\r\n\r\nzip_code: 8580500 + product_weight: 500.00 => NULL\r\nzip_code: 8580500 + product_weight: 1200.00 => NULL\r\nzip_code: 3433095 + product_weight: 10000.00 => NULL\r\n\r\nHow to do that?\r\nI've made a lot of search and found some references for ZADD + ZRANGEBYSCORE, and others using custom lua scripts, but anything that I can use on my scenarios.",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-08-22T20:28:09Z",
            "updatedAt": "2024-08-23T13:51:32Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "lfaltran",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "IMHO, this kind of complex query is actually more suitable for [RediSearch](https://github.com/RediSearch/RediSearch).",
                  "createdAt": "2024-08-23T09:36:07Z",
                  "updatedAt": "2024-08-23T09:36:08Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "Hello @sundb thank you for your contribution.\r\n\r\nAn important information that I've missed to provide is that I'm using Memorystore for Redis from Google (GCP), and Redis Modules is not allowed :(",
                        "createdAt": "2024-08-23T13:51:32Z",
                        "updatedAt": "2024-08-23T13:51:33Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "lfaltran",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13464",
            "number": 13464,
            "title": "Security support change after 7.4 release and 8.0",
            "body": "Hi,\r\n\r\nFor me the support release is a bit contradicting or maybe not explained well https://redis.io/about/releases/\r\nBased on the examples, I expected to see 7.0 as one of the supported branches when 7.4 came out.\r\nCan someone explain why 7.2 and 6.2 are supported here?\r\n\"The previous minor version of the latest stable release.\r\nThe previous stable major release.\"\r\n\r\nIf this means\r\nThe previous minor version of the latest stable release = previous minor version\r\nThe previous stable major release = last minor version of previous major release\r\n\r\nwhat happens with the example of when 3.0 comes out both 2.0 and 2.2 are supported, wouldn't it be just 2.2?\r\n\r\nso based on the example support rule is\r\nThe previous minor version of the latest stable release = last minor version of previous major release\r\nThe previous stable major release = last major version\r\nbut this would mean that after 7.4 release, both 6.2 and 7.0 are supported. and actually what is dropped from the support list is 7.2\r\nhttps://github.com/redis/redis/pull/13448#discussion_r1694219149\r\n\"When version 2.2 is the latest stable release, both 2.0 and 1.2 are maintained.\"\r\n\r\nso now that I'm properly confused, what will be dropped with the release of 8.0?\r\n7.4 will stay if that is the last minor release of the previous major version.\r\nso 7.2 or 6.2?\r\n\r\nthanks in advance",
            "upvoteCount": 2,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": "RESOLVED",
            "createdAt": "2024-08-05T09:44:17Z",
            "updatedAt": "2024-08-20T14:52:06Z",
            "closedAt": "2024-08-06T06:18:29Z",
            "closed": true,
            "author": {
              "login": "bszollos",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": true,
            "answerChosenAt": "2024-08-06T06:17:56Z",
            "answerChosenBy": {
              "login": "bszollos",
              "__typename": "User"
            },
            "comments": {
              "nodes": [
                {
                  "body": "ping @oranagra ",
                  "createdAt": "2024-08-05T09:48:51Z",
                  "updatedAt": "2024-08-05T09:48:52Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "Your first interpretation is the correct one, but I guess we need to document the policy for a new major release:\r\n\r\nWith the first release of a new stable major, we will also support the two latest minors of the previous stable major.\r\nSo with 8.0 - it would be 7.4 and 7.2.\r\n\r\nThen, with 8.2 - it would be 8.0 and 7.4, and with 8.4 it would be 8.2 and 7.4\r\n\r\n@oranagra please confirm.",
                  "createdAt": "2024-08-05T10:03:35Z",
                  "updatedAt": "2024-08-05T10:04:55Z",
                  "authorAssociation": "MEMBER",
                  "author": {
                    "login": "LiorKogan",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "@LiorKogan @oranagra \r\n\r\nThank you for the clarification. Is it correct to say that the latest three minor releases are supported at any time?",
                        "createdAt": "2024-08-20T13:59:30Z",
                        "updatedAt": "2024-08-20T13:59:30Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "shazib-summar",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "No. \r\nSuppose we have 7.4, 8.0, 8.2 and now we introduce 8.4 - we will drop support for 8.0 but will keep support for 7.4\r\nThen, on 8.6 we will drop support for 8.2 but still keep support for 7.4\r\n\r\n\r\n",
                        "createdAt": "2024-08-20T14:52:06Z",
                        "updatedAt": "2024-08-20T14:52:06Z",
                        "authorAssociation": "MEMBER",
                        "author": {
                          "login": "LiorKogan",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                },
                {
                  "body": "it's a little hard (for me) to follow all these nuances and examples, so i'll try to explain the reasoning that i suppose is behind this.\r\n\r\nlet's say that 6.0 was a successful version, that many people used, it is likely that 6.2 contains a bunch of nice updates, but no major architecture change or command breaking change. so in theory, everyone who used 6.0 can probably safely upgrade to 6.2 (it could be that they can't easily upgrade to 7.0), and that's the one we prefer to maintain.\r\nrather than maintaining 6.0 which is more outdated.\r\n\r\nanother reasoning is that we don't want to keep maintaining code that's too old (harder), so we prefer to maintain 6.2 rather than 6.0, and as soon as 8.0 is out, we rather not keep maintaining both 7.x and 6.x.\r\n\r\n\r\n",
                  "createdAt": "2024-08-05T21:09:46Z",
                  "updatedAt": "2024-08-05T21:09:47Z",
                  "authorAssociation": "MEMBER",
                  "author": {
                    "login": "oranagra",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "Thanks for the explanation",
                  "createdAt": "2024-08-06T06:18:29Z",
                  "updatedAt": "2024-08-06T06:18:30Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "bszollos",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/9456",
            "number": 9456,
            "title": "Are RedisGrears modules free/open source?",
            "body": "Are RedisGrears modules free/open source?\nam unable to find the modules in the opensource,\nAfter trying with RG client getting the following error: (error) ERR unknown command RG.PYEXECUTE do we have any solutions to overcome this issue in opensource.\nThe intention is to put some keyspace events to stream.\nThanks in advance",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2021-09-02T14:38:02Z",
            "updatedAt": "2024-08-09T06:55:56Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "vsharathis",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": true,
            "answerChosenAt": "2021-09-29T15:05:04Z",
            "answerChosenBy": {
              "login": "yossigo",
              "__typename": "User"
            },
            "comments": {
              "nodes": [
                {
                  "body": "RedisGears is source available and licenced under Redis Source Available License ([RSAL](https://github.com/RedisGears/RedisGears/blob/master/LICENSE)). This license is permissive if your product is not a \"database product\". The definition of \"database product\" is within the license. \r\n\r\n[Here's](https://redisgears.io/quickstart.html) a guide on how to get started with this Redis module.\r\n\r\n",
                  "createdAt": "2021-09-02T19:06:12Z",
                  "updatedAt": "2022-08-06T17:25:26Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "K-Jo",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "Thank you @K-Jo ",
                        "createdAt": "2021-09-03T04:34:52Z",
                        "updatedAt": "2022-08-06T17:25:31Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "vsharathis",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Can we take a docker image used as a sidecar for Redis server ? But the docker image maintained by Redis lab, can we take only the image and integrate with Redis io image as a sidecar ?? Won't be any commercial license issue ?? Kindly share comments\n\nThanks ",
                        "createdAt": "2021-09-03T17:32:39Z",
                        "updatedAt": "2022-08-06T17:25:31Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "vsharathis",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Do you mean take this docker? https://hub.docker.com/r/redislabs/redisgears/",
                        "createdAt": "2021-09-04T08:57:38Z",
                        "updatedAt": "2024-03-27T09:04:51Z",
                        "authorAssociation": "CONTRIBUTOR",
                        "author": {
                          "login": "gkorland",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Yes ",
                        "createdAt": "2021-09-04T09:24:14Z",
                        "updatedAt": "2024-03-27T09:04:51Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "vsharathis",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                },
                {
                  "body": "Can I use Redis Gears with Redis Stack and if so How do I install redis gears inside the redis stack docker container . Or is there a way to run a separate redis gears container and attach it with Redis Stack server",
                  "createdAt": "2024-08-09T06:55:55Z",
                  "updatedAt": "2024-08-09T06:55:56Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "Harsh-Maheshwari",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13373",
            "number": 13373,
            "title": "Does a Redis cluster support more than 1000 nodes?",
            "body": "Hi, I'm investigating the scale limitations of Redis Cluster.\r\n\r\nAccording to [the official document](https://redis.io/docs/latest/operate/oss_and_stack/reference/cluster-spec/), it indicates around 1000 nodes as a limitation.\r\n\r\n```\r\nRedis Cluster is a distributed implementation of Redis with the following goals in order of importance in the design:\r\n\r\nHigh performance and linear scalability up to 1000 nodes. There are no proxies, asynchronous replication is used, and no merge operations are performed on values.\r\n```\r\n\r\nI want to use a Redis Cluster with more than 1000 nodes. Maybe the number of nodes can reach 2000 or 3000 nodes if possible.\r\n\r\nIn my rough research, there are several known issues to scale out Redis Clusters.\r\n- [Pubsub write amplification](https://github.com/redis/redis/issues/8029)\r\n- [Redis Cluster: reduce gossip messages total traffic\r\n](https://github.com/redis/redis/issues/3929)\r\n\r\nMay I have some question?\r\nQ1. Does `node` mean `Redis instance`? (High performance and linear scalability up to 1000 Redis instances.)\r\nQ2. Does 1000 nodes is the limitation of the official cluster? \r\nQ3. Can I use a Redis Cluster more than 1000 nodes? Maybe I need sophisticated infrastructure, options, and Linux tuning. What is the expected bottleneck? \r\n",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-06-27T09:24:01Z",
            "updatedAt": "2024-08-02T13:27:32Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "achimbab",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "> Q1. Does `node` mean `Redis instance`? (High performance and linear scalability up to 1000 Redis instances.) \r\n\r\nyes, one node is one Reids instance.\r\n\r\n> Q2. Does 1000 nodes is the limitation of the official cluster? \r\n\r\nno, it's not suggestion not limitation.\r\nit is for official cluster.\r\n\r\n> Q3. Can I use a Redis Cluster more than 1000 nodes? Maybe I need sophisticated infrastructure, options, and Linux tuning. What is the expected bottleneck?\r\n\r\nyou can certainly create a Redis Cluster over 1000 nodes.\r\nthe bottleneck is that when we add a new node to a cluster with N nodes, N new connections will be created.",
                  "createdAt": "2024-08-02T13:27:32Z",
                  "updatedAt": "2024-08-02T13:27:44Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/12436",
            "number": 12436,
            "title": "unit-testing services that use bullMQ(nestjs) with redis",
            "body": "Hi guys!, hope you are all doing well.\r\n\r\ni have found myself in a pinch.\r\nwe have a monolith which uses bull mq with Redis for scheduling jobs such as emials notifs etc,\r\n\r\nso far i have been running redis in a docker container for unit tests but i realise this not a unit-test everything in a unittest should happen in memory and i am unable to find a solution that helps me.\r\nafter removing he docker and runnning the testsi basically get this\r\n\r\n```\r\n  at Queue.emit (node_modules/bullmq/src/classes/queue-base.ts:110:17)\r\n      at Queue.emit (node_modules/bullmq/src/classes/queue.ts:138:18)\r\n      at RedisConnection.<anonymous> (node_modules/bullmq/src/classes/queue-base.ts:61:56)\r\n      at EventEmitter.RedisConnection.handleClientError (node_modules/bullmq/src/classes/redis-connection.ts:101:12)\r\n\r\n  console.error\r\n    Error: connect ECONNREFUSED 127.0.0.1:6379\r\n        at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1494:16) {\r\n      errno: -61,\r\n      code: 'ECONNREFUSED',\r\n      syscall: 'connect',\r\n      address: '127.0.0.1',\r\n      port: 6379\r\n    }\r\n  \r\n```\r\n    \r\n    my guess is that i need to run an in memory redis similar to how an sqllitedb works in unit tests\r\n    \r\n    i think i cannot use bullmq mocks as the services are nested and some use the queue and some dont.\r\n    \r\n    for example i am writing unittests for foe service1 but service 1 imports services2 and service2 (in the module file has the queue initialised)\r\n    \r\n\r\n\r\n\r\n```\r\n    @Module({\r\n  imports: [\r\n    BullModule.forRoot({\r\n      /**\r\n       * Redis client connection string\r\n       */\r\n      connection: {\r\n        host: REDIS_HOST,\r\n        port: parseInt(REDIS_PORT),\r\n        password: REDIS_PASSWORD,\r\n      },\r\n    }),\r\n  ],\r\n  controllers: [],\r\n  providers: [QueueJobsRepository, QueueService],\r\n  exports: [BullModule, QueueService],\r\n})\r\nexport class QueueModule {}\r\n\r\n```\r\n\r\n```\r\n@Module({\r\n  imports: [\r\n    ConfigModule,\r\n    QueueModule,\r\n    // more queues can be added here\r\n    BullModule.registerQueue({\r\n      name: EMAIL_QUEUE,\r\n    }),\r\n  ],\r\n  exports: [EmailService],\r\n  providers: [EmailService, EmailConsumer, EmailQueueService],\r\n  controllers: [],\r\n})\r\nexport class EmailModule {}\r\n```\r\n\r\n\r\ni have used redis-mock,\r\nioredis\r\nredis-server\r\nnothing seems to work.\r\n\r\nany help is greatly apriciated \r\n\r\n\r\n\r\n\r\n\r\n\r\n    \r\n    \r\n    ",
            "upvoteCount": 3,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2023-07-25T05:00:39Z",
            "updatedAt": "2024-08-01T10:47:19Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "RafiuJaman",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "You should be able to use: [Redis Testcontainers](https://testcontainers.com/modules/redis/). You have to create a setup file after installing the package for Jest to use it.",
                  "createdAt": "2024-08-01T10:30:26Z",
                  "updatedAt": "2024-08-01T10:33:23Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "kaloczki-norbert",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13455",
            "number": 13455,
            "title": "Redis license and version mapping",
            "body": "Since https://github.com/redis/redis/pull/13157 was introduced, which and greater versions would be effective for the new license, 7.4 released, would the 7.4 version use the new license, or would keep using the BSD license?\r\nFrom https://github.com/redis/redis/blob/7.4.0/LICENSE.txt , it seems to use new license, is there a clear explanation about the new license and version",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-07-30T05:27:58Z",
            "updatedAt": "2024-07-30T06:13:14Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "WeihanLi",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": true,
            "answerChosenAt": "2024-07-30T06:13:14Z",
            "answerChosenBy": {
              "login": "WeihanLi",
              "__typename": "User"
            },
            "comments": {
              "nodes": [
                {
                  "body": "Redis 7.4 Community Edition (the new name of Redis OSS since 7.4) is released under dual RSALv2+SSPLv1.\r\n\r\nRead more [here](https://redis.io/blog/redis-adopts-dual-source-available-licensing/) and [here](https://redis.io/legal/licenses/).\r\n\r\nI also suggest taking a look [here](https://www.infoworld.com/article/2336563/the-bizarre-defense-of-trillion-dollar-cabals.html).",
                  "createdAt": "2024-07-30T05:58:25Z",
                  "updatedAt": "2024-07-30T05:58:41Z",
                  "authorAssociation": "MEMBER",
                  "author": {
                    "login": "LiorKogan",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13441",
            "number": 13441,
            "title": "Managing Streams memory on high volatility scenarios",
            "body": "Hi guys, first time writing here, hope to get some light shed in my problem.\r\n\r\nWe're using Redis Streams to serve as a short-living storage in a micro-service, but the memory consumption is going through the roof very fast. The scenario is:\r\n\r\n1. Receive an http call, XAdd the data to the stream, finish.\r\n2. On multiple micro-service instances (each one is a different consumer), do a XReadGroup on that stream. After working with the messages data, we XAck it.\r\n3. In a best case scenario, the data should live in the stream for a couple of seconds.\r\n\r\nBut even so the memory usage increases very fast, XTrim won't help much since we can't afford to lose unread messages. **I tried to do an XDel on the messages after the XAck, but the command always returns 0.**\r\n\r\n### Questions:\r\n\r\n- Does XAck not only removes the message from the PEL, but deletes the record from the stream?\r\n  - If so, why the memory consumption keeps rising?\r\n  - If not, why XDel returns 0?\r\n- Should I do XDel and then XAck?\r\n\r\nAny suggestions would be very welcoming, thanks!",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-07-24T11:29:42Z",
            "updatedAt": "2024-07-29T11:22:22Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "mfelipe",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": true,
            "answerChosenAt": "2024-07-29T11:22:22Z",
            "answerChosenBy": {
              "login": "mfelipe",
              "__typename": "User"
            },
            "comments": {
              "nodes": [
                {
                  "body": "> ### Questions:\r\n> * Does XAck not only removes the message from the PEL, but deletes the record from the stream?\r\n\r\nnot, `XACK` just remove the message from the consume group's pending entries list and consume's PEL.\r\nthe message is still there.\r\n\r\n>   * If not, why XDel returns 0?\r\n\r\n1. the message you passed to `XDEL` command is wrong. (from your scenario, it's likely)\r\n2. the message was deleted by other client.\r\n\r\n> * Should I do XDel and then XAck?\r\n\r\nthere is no dependency between them.\r\n",
                  "createdAt": "2024-07-24T11:51:33Z",
                  "updatedAt": "2024-07-24T11:51:33Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "maybe you can share the output of  `XINFO STREAM`",
                        "createdAt": "2024-07-24T11:56:40Z",
                        "updatedAt": "2024-07-24T11:56:41Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Thanks for your input. Please consider this after my tests:\r\n\r\n- If I XAck before XDel, XDel returns 0. XAck succeeds.\r\n- If I XDel before XAck, XDel returns 1. XAck behavior don't change.",
                        "createdAt": "2024-07-24T11:59:33Z",
                        "updatedAt": "2024-07-24T11:59:34Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "mfelipe",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@mfelipe can you give me your reproduce stpes?",
                        "createdAt": "2024-07-24T12:28:11Z",
                        "updatedAt": "2024-07-24T12:28:12Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Considering:\r\n\r\n- Existing single stream and group\r\n- Auto generated IDs\r\n- Commands are valid. Same XDel command works if run before XAck\r\n\r\nFailed XDel scenario:\r\n1. XAdd: Success\r\n2. XReadGroup: Success\r\n3. XAck: Returns 1\r\n4. XDel: Returns 0\r\n\r\nAll success scenario:\r\n1. XAdd: Success\r\n2. XReadGroup: Success\r\n3. XDel: Returns 1\r\n4. XAck: Returns 1",
                        "createdAt": "2024-07-24T12:46:17Z",
                        "updatedAt": "2024-07-24T12:46:17Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "mfelipe",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "```\r\nXADD mystream * field1 value1\r\nXGROUP CREATE mystream mygroup $ MKSTREAM\r\nXADD mystream * field1 value1\r\nXREADGROUP GROUP mygroup consumer1 COUNT 1 STREAMS mystream >\r\nXACK mystream mygroup <message-id>\r\nXDEL mystream <message-id>\r\n```\r\n\r\nplease try these commands, and replace the <message-id> with the message id from `XREADGROUP` command.\r\ni tested it and `XDel` returns 1.",
                        "createdAt": "2024-07-24T12:53:36Z",
                        "updatedAt": "2024-07-24T12:53:36Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Running manually it works. I'll do some more testing and come back to you, but thanks so far for the help.\r\n\r\n`\r\n> XADD mystream * field1 value1\r\n\"1721827487208-0\"\r\n\r\n> XGROUP CREATE mystream mygroup $ MKSTREAM\r\n\"OK\"\r\n\r\n> XREADGROUP GROUP mygroup consumer1 COUNT 1 STREAMS mystream >\r\n1) 1) \"mystream\"\r\n   2) 1) 1) \"1721827487208-0\"\r\n         2) 1) \"field1\"\r\n            2) \"value1\"\r\n\r\n> XACK mystream mygroup 1721827487208-0\r\n(integer) 1\r\n\r\n> XDEL mystream 1721827487208-0\r\n(integer) 1\r\n`",
                        "createdAt": "2024-07-24T13:38:51Z",
                        "updatedAt": "2024-07-24T13:38:52Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "mfelipe",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13429",
            "number": 13429,
            "title": "Copy entire contents of one virtual db to another one inside same Redis instance, without using backup utilities",
            "body": "I inherited a system that was designed and implemented by a web developer, when it's not really a web application.  I'm stuck with it.  \r\n\r\nThe simplest way I can think of to solve the current problem I'm facing involves keeping \"snapshots\" of the contents of one virtual DB at three points in time, overwriting in a FIFO daisy-chain.   I've spent many hours looking for a way to accomplish this, but unfortunately search results are dominated by backup and recovery solutions, which is not what I need.  \r\n\r\nThere's not a lot of data; like, it would be measured in megabytes, maybe four or so.  Nothing, by DB or caching standards.  But I need to take these point-in-time snapshots frequently; more than twice a minute.  I mean, that's not frequently by many standards, but it does preclude any solution that involves running some command line tool or going through some backup and restoration process.  It's not going to be all that uncommon to restore from one of the two non-current copies-- they're definitely not for disaster recovery purposes.\r\n\r\nI can sort of understand why functionality like this wouldn't be in much demand for a caching layer, because it's so non-performant, and the data is presumably changing while you're copying.  But our system has regular periods of times, lasting multiple seconds, when no data should be getting changed; my plan is to use those regularly scheduled periods of cache inactivity to make the copy.  \r\n\r\nSo I want to copy the entire contents (all whopping 4mb or so) from one virtual data base to another, in code.  Python if possible. \r\n I thought as a last resort, surely I could just iterate through all the keys and copy them (and their values) by first bringing them into my applications memory, then just writing them out.  But even that seems to be very challenging, as there is apparently no programmatic way to iterate through all the keys in the given virtual DB.  If it makes a difference, the way I'm currently connecting to Redis is through Django's django.core.cache object, but I could connect another way for this purpose if necessary.\r\n\r\nAs my deadline is measured in days, not weeks, any solution that involves changing the basic architecture-- like \"hey, it's only 4mb, you don't need Redis, dummy!\"-- just isn't going to be practical.  If I do a good job with this, I hope to get the opportunity to re-architect if from an application programmer's perspective, rather than a web designer's; but I don't have much time to prove myself.  Any pointers or suggestions gratefully accepted.  Thanks!",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": "OUTDATED",
            "createdAt": "2024-07-19T07:30:19Z",
            "updatedAt": "2024-07-19T07:46:11Z",
            "closedAt": "2024-07-19T07:43:26Z",
            "closed": true,
            "author": {
              "login": "andrewbgross",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "not sure i can follow you, your core demand is to sync data across three instances?",
                  "createdAt": "2024-07-19T07:46:11Z",
                  "updatedAt": "2024-07-19T07:46:11Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13420",
            "number": 13420,
            "title": "Does Redis LFU really used a logarithm counter?",
            "body": "In the document [Key eviction](https://redis.io/docs/latest/develop/reference/eviction/), The new LFU mode section, [Least Frequently Used eviction mode](http://antirez.com/news/109) was mentioned, it was stated that LFU **used a used logarithmic counter**\r\n\r\nThe below is the code of `LFULogIncr()`\r\n\r\n```c\r\nuint8_t LFULogIncr(uint8_t counter) {\r\n      if (counter == 255) return 255;\r\n\r\n      double r = (double)rand() / RAND_MAX;\r\n      double baseval = counter - LFU_INIT_VAL;\r\n      if (baseval < 0) baseval = 0;\r\n      double p = 1.0 / (baseval * server.lfu_log_factor + 1);\r\n      if (r < p) counter++;\r\n      return counter;\r\n}\r\n```\r\n\r\nFrom the code snippet, we can see that when the counter is $C$ (, it achieving an increment probability of  $1 / (F * (C - 5)$, which  $F$ is the log factor, and 5 is the **LFU_INIT_VAL**, to simply the calculation, we will ignore the **INIT_VAL** first\r\n\r\nTo increment by one from counter $C$, the expected number of accesses is nearly $F * C$. And thus to reach counter $N$, the total expected number of accesses is $E(N)$, which is actually a power function\r\n```math\r\n$$E(C) = F + F * 2 + F * 3  + ... + F * (C-1) = F * \\frac{C (C-1)}{2} = \\frac{F}{2} (C^2 - C)$$\r\n```\r\n\r\nTo know the estimated counter for a given number of accesses, we need to compute the reverse function of the $E(C)$, and the function indeed is another power function, not a logarithm function.\r\n$C = E^{-1}(N) = \\frac{1+\\sqrt{1 + 8N/F}}{2} = \\frac{1}{2} + \\sqrt{\\frac{1}{4} + 2N/F}$\r\n\r\nTake an example, when the number of accesses is $100k$, and $F$ is the default value $10$, then $C =  \\frac{1+\\sqrt{1 + 8 * 100\\\\_000 / 10}}{2} \\approx 142$, plus the `LFU_INIT_VAL`, we can get the final estimated counter is $147$\r\n",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-07-16T06:19:48Z",
            "updatedAt": "2024-07-16T06:19:49Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "jackyfkc",
              "__typename": "User"
            },
            "category": {
              "name": "General",
              "description": "Chat about anything and everything here",
              "slug": "general"
            },
            "isAnswered": null,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": []
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13346",
            "number": 13346,
            "title": "SLOWLOG GET - Zabbix Alert",
            "body": "On my Zabbix monitor i got an alert: TOO MANY ENTRIES IN THE SLOWLOG\r\n\r\nSLOWLOG GET:\r\n\r\n![Screenshot 2024-06-14 at 4 12 26 PM (2)](https://github.com/redis/redis/assets/98938307/c82175ec-c775-4611-908e-f20fc36c400f)\r\n",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-06-14T11:15:04Z",
            "updatedAt": "2024-06-27T11:33:16Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "irtaza9",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "1. what version are you using?\r\n2. could you give the output of `INFO ALL`.\r\n3. can you try `./src/redis-cli --intrinsic-latency 100` to see the latency of Redis.",
                  "createdAt": "2024-06-14T11:33:50Z",
                  "updatedAt": "2024-06-14T11:33:51Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "INFO ALL:\r\n\r\n127.0.0.1:6379> INFO ALL\r\n# Server\r\nredis_version:7.2.5\r\nredis_git_sha1:00000000\r\nredis_git_dirty:0\r\nredis_build_id:d284576ab9ca3cc5\r\nredis_mode:standalone\r\nos:Linux 5.15.0-107-generic x86_64\r\narch_bits:64\r\nmonotonic_clock:POSIX clock_gettime\r\nmultiplexing_api:epoll\r\natomicvar_api:c11-builtin\r\ngcc_version:11.4.0\r\nprocess_id:36019\r\nprocess_supervised:systemd\r\nrun_id:836eac09ffb84510963e5f1d9dc8cdde173446b8\r\ntcp_port:6379\r\nserver_time_usec:1718365290569628\r\nuptime_in_seconds:177460\r\nuptime_in_days:2\r\nhz:10\r\nconfigured_hz:10\r\nlru_clock:7089258\r\nexecutable:/usr/bin/redis-server\r\nconfig_file:/etc/redis/redis.conf\r\nio_threads_active:0\r\nlistener0:name=tcp,bind=*,bind=-::*,port=6379\r\n\r\n# Clients\r\nconnected_clients:6\r\ncluster_connections:0\r\nmaxclients:10000\r\nclient_recent_max_input_buffer:20480\r\nclient_recent_max_output_buffer:20504\r\nblocked_clients:0\r\ntracking_clients:0\r\nclients_in_timeout_table:0\r\ntotal_blocking_keys:0\r\ntotal_blocking_keys_on_nokey:0\r\n\r\n# Memory\r\nused_memory:2650712\r\nused_memory_human:2.53M\r\nused_memory_rss:13238272\r\nused_memory_rss_human:12.62M\r\nused_memory_peak:3015384\r\nused_memory_peak_human:2.88M\r\nused_memory_peak_perc:87.91%\r\nused_memory_overhead:2008824\r\nused_memory_startup:867200\r\nused_memory_dataset:641888\r\nused_memory_dataset_perc:35.99%\r\nallocator_allocated:2794048\r\nallocator_active:3125248\r\nallocator_resident:6209536\r\ntotal_system_memory:4101455872\r\ntotal_system_memory_human:3.82G\r\nused_memory_lua:31744\r\nused_memory_vm_eval:31744\r\nused_memory_lua_human:31.00K\r\nused_memory_scripts_eval:0\r\nnumber_of_cached_scripts:0\r\nnumber_of_functions:0\r\nnumber_of_libraries:0\r\nused_memory_vm_functions:32768\r\nused_memory_vm_total:64512\r\nused_memory_vm_total_human:63.00K\r\nused_memory_functions:184\r\nused_memory_scripts:184\r\nused_memory_scripts_human:184B\r\nmaxmemory:0\r\nmaxmemory_human:0B\r\nmaxmemory_policy:noeviction\r\nallocator_frag_ratio:1.12\r\nallocator_frag_bytes:331200\r\nallocator_rss_ratio:1.99\r\nallocator_rss_bytes:3084288\r\nrss_overhead_ratio:2.13\r\nrss_overhead_bytes:7028736\r\nmem_fragmentation_ratio:5.04\r\nmem_fragmentation_bytes:10609976\r\nmem_not_counted_for_evict:13472\r\nmem_replication_backlog:1048592\r\nmem_total_replication_buffers:1066208\r\nmem_clients_slaves:17632\r\nmem_clients_normal:75144\r\nmem_cluster_links:0\r\nmem_aof_buffer:0\r\nmem_allocator:jemalloc-5.3.0\r\nactive_defrag_running:0\r\nlazyfree_pending_objects:0\r\nlazyfreed_objects:0\r\n\r\n# Persistence\r\nloading:0\r\nasync_loading:0\r\ncurrent_cow_peak:0\r\ncurrent_cow_size:0\r\ncurrent_cow_size_age:0\r\ncurrent_fork_perc:0.00\r\ncurrent_save_keys_processed:0\r\ncurrent_save_keys_total:0\r\nrdb_changes_since_last_save:0\r\nrdb_bgsave_in_progress:0\r\nrdb_last_save_time:1718278234\r\nrdb_last_bgsave_status:ok\r\nrdb_last_bgsave_time_sec:0\r\nrdb_current_bgsave_time_sec:-1\r\nrdb_saves:4\r\nrdb_last_cow_size:856064\r\nrdb_last_load_keys_expired:0\r\nrdb_last_load_keys_loaded:0\r\naof_enabled:0\r\naof_rewrite_in_progress:0\r\naof_rewrite_scheduled:0\r\naof_last_rewrite_time_sec:-1\r\naof_current_rewrite_time_sec:-1\r\naof_last_bgrewrite_status:ok\r\naof_rewrites:0\r\naof_rewrites_consecutive_failures:0\r\naof_last_write_status:ok\r\naof_last_cow_size:0\r\nmodule_fork_in_progress:0\r\nmodule_fork_last_cow_size:0\r\n\r\n# Stats\r\ntotal_connections_received:128\r\ntotal_commands_processed:1598103\r\ninstantaneous_ops_per_sec:5\r\ntotal_net_input_bytes:104423796\r\ntotal_net_output_bytes:932178727\r\ntotal_net_repl_input_bytes:36169587\r\ntotal_net_repl_output_bytes:27141928\r\ninstantaneous_input_kbps:0.29\r\ninstantaneous_output_kbps:7.55\r\ninstantaneous_input_repl_kbps:0.00\r\ninstantaneous_output_repl_kbps:0.35\r\nrejected_connections:0\r\nsync_full:4\r\nsync_partial_ok:2\r\nsync_partial_err:4\r\nexpired_keys:0\r\nexpired_stale_perc:0.00\r\nexpired_time_cap_reached_count:0\r\nexpire_cycle_cpu_milliseconds:1257\r\nevicted_keys:0\r\nevicted_clients:0\r\ntotal_eviction_exceeded_time:0\r\ncurrent_eviction_exceeded_time:0\r\nkeyspace_hits:7\r\nkeyspace_misses:0\r\npubsub_channels:1\r\npubsub_patterns:0\r\npubsubshard_channels:0\r\nlatest_fork_usec:713\r\ntotal_forks:8\r\nmigrate_cached_sockets:0\r\nslave_expires_tracked_keys:0\r\nactive_defrag_hits:0\r\nactive_defrag_misses:0\r\nactive_defrag_key_hits:0\r\nactive_defrag_key_misses:0\r\ntotal_active_defrag_time:0\r\ncurrent_active_defrag_time:0\r\ntracking_total_keys:0\r\ntracking_total_items:0\r\ntracking_total_prefixes:0\r\nunexpected_error_replies:0\r\ntotal_error_replies:7\r\ndump_payload_sanitizations:0\r\ntotal_reads_processed:1500906\r\ntotal_writes_processed:3791386\r\nio_threaded_reads_processed:0\r\nio_threaded_writes_processed:0\r\nreply_buffer_shrinks:73323\r\nreply_buffer_expands:74485\r\neventloop_cycles:2987971\r\neventloop_duration_sum:408148700\r\neventloop_duration_cmd_sum:10972012\r\ninstantaneous_eventloop_cycles_per_sec:14\r\ninstantaneous_eventloop_duration_usec:133\r\nacl_access_denied_auth:0\r\nacl_access_denied_cmd:0\r\nacl_access_denied_key:0\r\nacl_access_denied_channel:0\r\n\r\n# Replication\r\nrole:master\r\nconnected_slaves:2\r\nslave0:ip=172.29.28.17,port=6379,state=online,offset=86813318,lag=1\r\nslave1:ip=172.29.22.8,port=6379,state=online,offset=86813318,lag=0\r\nmaster_failover_state:no-failover\r\nmaster_replid:f22e85d355e6ec98002ead1888cc72773c747cc9\r\nmaster_replid2:400321eb4caa92695754deb436937817e70c9bf8\r\nmaster_repl_offset:86813318\r\nsecond_repl_offset:73214025\r\nrepl_backlog_active:1\r\nrepl_backlog_size:1048576\r\nrepl_backlog_first_byte_offset:85756891\r\nrepl_backlog_histlen:1056428\r\n\r\n# CPU\r\nused_cpu_sys:250.793003\r\nused_cpu_user:267.362233\r\nused_cpu_sys_children:0.023470\r\nused_cpu_user_children:0.006277\r\nused_cpu_sys_main_thread:250.343630\r\nused_cpu_user_main_thread:266.928151\r\n\r\n# Modules\r\n\r\n# Commandstats\r\ncmdstat_command|docs:calls=14,usec=18989,usec_per_call=1356.36,rejected_calls=0,failed_calls=0\r\ncmdstat_exec:calls=6,usec=147542,usec_per_call=24590.33,rejected_calls=0,failed_calls=0\r\ncmdstat_multi:calls=6,usec=8,usec_per_call=1.33,rejected_calls=0,failed_calls=0\r\ncmdstat_client|kill:calls=12,usec=1377,usec_per_call=114.75,rejected_calls=0,failed_calls=0\r\ncmdstat_client|setname:calls=95,usec=317,usec_per_call=3.34,rejected_calls=0,failed_calls=0\r\ncmdstat_set:calls=6,usec=98,usec_per_call=16.33,rejected_calls=0,failed_calls=0\r\ncmdstat_select:calls=3,usec=4,usec_per_call=1.33,rejected_calls=0,failed_calls=0\r\ncmdstat_get:calls=4,usec=51,usec_per_call=12.75,rejected_calls=0,failed_calls=0\r\ncmdstat_psync:calls=6,usec=711,usec_per_call=118.50,rejected_calls=0,failed_calls=0\r\ncmdstat_role:calls=7,usec=104,usec_per_call=14.86,rejected_calls=0,failed_calls=0\r\ncmdstat_ping:calls=735072,usec=997471,usec_per_call=1.36,rejected_calls=0,failed_calls=0\r\ncmdstat_keys:calls=8,usec=81,usec_per_call=10.12,rejected_calls=1,failed_calls=0\r\ncmdstat_flushall:calls=1,usec=19844,usec_per_call=19844.00,rejected_calls=0,failed_calls=0\r\ncmdstat_config|get:calls=1502,usec=348359,usec_per_call=231.93,rejected_calls=0,failed_calls=0\r\ncmdstat_config|rewrite:calls=6,usec=145157,usec_per_call=24192.83,rejected_calls=0,failed_calls=0\r\ncmdstat_monitor:calls=4,usec=6,usec_per_call=1.50,rejected_calls=0,failed_calls=0\r\ncmdstat_subscribe:calls=50,usec=146,usec_per_call=2.92,rejected_calls=0,failed_calls=0\r\ncmdstat_replconf:calls=178622,usec=315366,usec_per_call=1.77,rejected_calls=0,failed_calls=0\r\ncmdstat_slowlog|get:calls=1506,usec=13741,usec_per_call=9.12,rejected_calls=0,failed_calls=0\r\ncmdstat_exists:calls=3,usec=23,usec_per_call=7.67,rejected_calls=0,failed_calls=0\r\ncmdstat_publish:calls=607653,usec=3635393,usec_per_call=5.98,rejected_calls=0,failed_calls=0\r\ncmdstat_slaveof:calls=6,usec=755,usec_per_call=125.83,rejected_calls=0,failed_calls=0\r\ncmdstat_info:calls=73511,usec=5473758,usec_per_call=74.46,rejected_calls=0,failed_calls=0\r\n\r\n# Errorstats\r\nerrorstat_ERR:count=7\r\n\r\n# Latencystats\r\nlatency_percentiles_usec_command|docs:p50=1318.911,p99=1736.703,p99.9=1736.703\r\nlatency_percentiles_usec_exec:p50=19005.439,p99=38535.167,p99.9=38535.167\r\nlatency_percentiles_usec_multi:p50=1.003,p99=2.007,p99.9=2.007\r\nlatency_percentiles_usec_client|kill:p50=96.255,p99=182.271,p99.9=182.271\r\nlatency_percentiles_usec_client|setname:p50=2.007,p99=18.047,p99.9=33.023\r\nlatency_percentiles_usec_set:p50=17.023,p99=29.055,p99.9=29.055\r\nlatency_percentiles_usec_select:p50=1.003,p99=2.007,p99.9=2.007\r\nlatency_percentiles_usec_get:p50=4.015,p99=38.143,p99.9=38.143\r\nlatency_percentiles_usec_psync:p50=97.279,p99=166.911,p99.9=166.911\r\nlatency_percentiles_usec_role:p50=13.055,p99=43.007,p99.9=43.007\r\nlatency_percentiles_usec_ping:p50=1.003,p99=3.007,p99.9=21.119\r\nlatency_percentiles_usec_keys:p50=6.015,p99=32.127,p99.9=32.127\r\nlatency_percentiles_usec_flushall:p50=19922.943,p99=19922.943,p99.9=19922.943\r\nlatency_percentiles_usec_config|get:p50=217.087,p99=385.023,p99.9=1220.607\r\nlatency_percentiles_usec_config|rewrite:p50=18612.223,p99=38273.023,p99.9=38273.023\r\nlatency_percentiles_usec_monitor:p50=1.003,p99=2.007,p99.9=2.007\r\nlatency_percentiles_usec_subscribe:p50=3.007,p99=8.031,p99.9=8.031\r\nlatency_percentiles_usec_replconf:p50=2.007,p99=3.007,p99.9=24.063\r\nlatency_percentiles_usec_slowlog|get:p50=6.015,p99=37.119,p99.9=94.207\r\nlatency_percentiles_usec_exists:p50=10.047,p99=10.047,p99.9=10.047\r\nlatency_percentiles_usec_publish:p50=6.015,p99=21.119,p99.9=61.183\r\nlatency_percentiles_usec_slaveof:p50=126.463,p99=259.071,p99.9=259.071\r\nlatency_percentiles_usec_info:p50=76.287,p99=172.031,p99.9=606.207\r\n\r\n# Cluster\r\ncluster_enabled:0\r\n\r\n# Keyspace\r\ndb0:keys=1,expires=0,avg_ttl=0\r\n\r\n# latency\r\n\r\n127.0.0.1:6379> latency doctor\r\nI'm sorry, Dave, I can't do that. Latency monitoring is disabled in this Redis instance. You may use \"CONFIG SET latency-monitor-threshold <milliseconds>.\" in order to enable it. If we weren't in a deep space mission I'd suggest to take a look at https://redis.io/topics/latency-monitor.\r\n\r\n\r\n# redis-cli --intrinsic-latency 100\r\n\r\nMax latency so far: 1 microseconds.\r\nMax latency so far: 19 microseconds.\r\nMax latency so far: 22 microseconds.\r\nMax latency so far: 23 microseconds.\r\nMax latency so far: 3427 microseconds.\r\nMax latency so far: 4203 microseconds.\r\nMax latency so far: 6116 microseconds.\r\nMax latency so far: 7319 microseconds.\r\nMax latency so far: 9058 microseconds.\r\nMax latency so far: 10165 microseconds.\r\nMax latency so far: 15737 microseconds.\r\nMax latency so far: 16038 microseconds.\r\n\r\n1743638936 total runs (avg latency: 0.0574 microseconds / 57.35 nanoseconds per run).\r\nWorst run took 279645x longer than the average latency.\r\n\r\n# Version\r\nRedis server v=7.2.5 sha=00000000:0 malloc=jemalloc-5.3.0 bits=64 build=d284576ab9ca3cc5\r\n",
                  "createdAt": "2024-06-14T11:49:26Z",
                  "updatedAt": "2024-06-14T12:05:40Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "irtaza9",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "127.0.0.1:6379> latency doctor\r\nDave, no latency spike was observed during the lifetime of this Redis instance, not in the slightest bit. I honestly think you ought to sit down calmly, take a stress pill, and think things over.\r\n127.0.0.1:6379> ",
                  "createdAt": "2024-06-14T12:07:57Z",
                  "updatedAt": "2024-06-14T12:07:59Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "irtaza9",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "```\r\nMax latency so far: 1 microseconds.\r\nMax latency so far: 19 microseconds.\r\nMax latency so far: 22 microseconds.\r\nMax latency so far: 23 microseconds.\r\nMax latency so far: 3427 microseconds.\r\nMax latency so far: 4203 microseconds.\r\nMax latency so far: 6116 microseconds.\r\nMax latency so far: 7319 microseconds.\r\nMax latency so far: 9058 microseconds.\r\nMax latency so far: 10165 microseconds.\r\nMax latency so far: 15737 microseconds.\r\nMax latency so far: 16038 microseconds.\r\n```\r\n\r\nyou should check if there are any high load processes in this server or vm that could affect Redis and cause so high latency.",
                  "createdAt": "2024-06-14T12:13:50Z",
                  "updatedAt": "2024-06-14T12:13:51Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "# 127.0.0.1:6379> monitor\r\nOK\r\n1718367731.723281 [0 172.16.179.96:53732] \"PING\"\r\n1718367731.723298 [0 172.16.179.96:53752] \"PING\"\r\n1718367731.899451 [0 [::1]:41134] \"PING\"\r\n1718367732.658597 [0 172.16.179.96:53732] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster1,172.16.179.96,6379,21\"\r\n1718367732.658637 [0 172.16.179.96:53752] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster,172.16.179.96,6379,22\"\r\n1718367732.730746 [0 172.16.179.96:53732] \"PING\"\r\n1718367732.730759 [0 172.16.179.96:53752] \"PING\"\r\n1718367733.737229 [0 172.16.179.96:53732] \"PING\"\r\n1718367733.737245 [0 172.16.179.96:53752] \"PING\"\r\n1718367734.399338 [0 [::1]:41134] \"PING\"\r\n1718367734.721568 [0 172.16.179.96:53732] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster1,172.16.179.96,6379,21\"\r\n1718367734.721592 [0 172.16.179.96:53752] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster,172.16.179.96,6379,22\"\r\n1718367734.822692 [0 172.16.179.96:53732] \"PING\"\r\n1718367734.822710 [0 172.16.179.96:53752] \"PING\"\r\n1718367735.856009 [0 172.16.179.96:53732] \"PING\"\r\n1718367735.856191 [0 172.16.179.96:53752] \"PING\"\r\n1718367736.737106 [0 172.16.179.96:53732] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster1,172.16.179.96,6379,21\"\r\n1718367736.737125 [0 172.16.179.96:53752] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster,172.16.179.96,6379,22\"\r\n1718367736.885400 [0 172.16.179.96:53732] \"PING\"\r\n1718367736.885564 [0 172.16.179.96:53752] \"PING\"\r\n1718367736.898643 [0 [::1]:41134] \"PING\"\r\n1718367737.910197 [0 172.16.179.96:53732] \"PING\"\r\n1718367737.910212 [0 172.16.179.96:53752] \"PING\"\r\n1718367738.777266 [0 172.16.179.96:53732] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster1,172.16.179.96,6379,21\"\r\n1718367738.777285 [0 172.16.179.96:53752] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster,172.16.179.96,6379,22\"\r\n1718367738.916360 [0 172.16.179.96:53732] \"PING\"\r\n1718367738.916383 [0 172.16.179.96:53752] \"PING\"\r\n1718367739.352869 [0 172.16.179.96:53732] \"INFO\"\r\n1718367739.352928 [0 172.16.179.96:53752] \"INFO\"\r\n1718367739.398647 [0 [::1]:41134] \"PING\"\r\n1718367739.977139 [0 172.16.179.96:53732] \"PING\"\r\n1718367739.977153 [0 172.16.179.96:53752] \"PING\"\r\n1718367740.806940 [0 172.16.179.96:53732] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster1,172.16.179.96,6379,21\"\r\n1718367740.806976 [0 172.16.179.96:53752] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster,172.16.179.96,6379,22\"\r\n1718367740.993400 [0 172.16.179.96:53732] \"PING\"\r\n1718367740.993538 [0 172.16.179.96:53752] \"PING\"\r\n1718367741.899376 [0 [::1]:41134] \"PING\"\r\n1718367742.001327 [0 172.16.179.96:53732] \"PING\"\r\n1718367742.001517 [0 172.16.179.96:53752] \"PING\"\r\n1718367742.816274 [0 172.16.179.96:53732] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster1,172.16.179.96,6379,21\"\r\n1718367742.816524 [0 172.16.179.96:53752] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster,172.16.179.96,6379,22\"\r\n1718367743.055181 [0 172.16.179.96:53732] \"PING\"\r\n1718367743.055407 [0 172.16.179.96:53752] \"PING\"\r\n1718367744.124087 [0 172.16.179.96:53732] \"PING\"\r\n1718367744.124245 [0 172.16.179.96:53752] \"PING\"\r\n1718367744.399440 [0 [::1]:41134] \"PING\"\r\n1718367744.827637 [0 172.16.179.96:53732] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster1,172.16.179.96,6379,21\"\r\n1718367744.827822 [0 172.16.179.96:53752] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster,172.16.179.96,6379,22\"\r\n1718367745.059885 [0 [::1]:41134] \"INFO\" \"default\"\r\n1718367745.141720 [0 172.16.179.96:53732] \"PING\"\r\n1718367745.141822 [0 172.16.179.96:53752] \"PING\"\r\n1718367746.169033 [0 [::1]:41134] \"PING\"\r\n1718367746.239515 [0 172.16.179.96:53732] \"PING\"\r\n1718367746.239616 [0 172.16.179.96:53752] \"PING\"\r\n1718367746.898827 [0 [::1]:41134] \"PING\"\r\n1718367746.909170 [0 172.16.179.96:53732] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster1,172.16.179.96,6379,21\"\r\n1718367746.909368 [0 172.16.179.96:53752] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster,172.16.179.96,6379,22\"\r\n1718367747.301474 [0 172.16.179.96:53732] \"PING\"\r\n1718367747.301487 [0 172.16.179.96:53752] \"PING\"\r\n1718367748.361227 [0 172.16.179.96:53732] \"PING\"\r\n1718367748.361242 [0 172.16.179.96:53752] \"PING\"\r\n1718367748.965467 [0 172.16.179.96:53732] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster1,172.16.179.96,6379,21\"\r\n1718367748.965484 [0 172.16.179.96:53752] \"PUBLISH\" \"__sentinel__:hello\" \"172.16.179.96,26379,86758a935c58b2233545f26fddc92d2355652648,22,mymaster,172.16.179.96,6379,22\"\r\n1718367749.392558 [0 172.16.179.96:53732] \"INFO\"\r\n1718367749.392565 [0 172.16.179.96:53732] \"PING\"\r\n1718367749.392619 [0 172.16.179.96:53752] \"INFO\"\r\n1718367749.392621 [0 172.16.179.96:53752] \"PING\"\r\n1718367749.398992 [0 [::1]:41134] \"PING\"\r\n1718367750.450689 [0 172.16.179.96:53732] \"PING\"\r\n1718367750.450700 [0 172.16.179.96:53752] \"PING\"",
                  "createdAt": "2024-06-14T12:23:09Z",
                  "updatedAt": "2024-06-14T12:23:10Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "irtaza9",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "The only thing which is running on my three servers are redis and sentinel. ",
                  "createdAt": "2024-06-14T12:29:48Z",
                  "updatedAt": "2024-06-14T12:29:49Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "irtaza9",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "can you use `taskset` to make redis instance and sentinel instance in different cores?\r\n",
                        "createdAt": "2024-06-14T12:31:58Z",
                        "updatedAt": "2024-06-14T12:47:21Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "I have 3 machines, each with 4 GB RAM, 4 CPUs, and 30 GB storage. I configured a master-slave setup with the help of Sentinel. Can you give me a good configuration to maximize the usage of these machines?",
                        "createdAt": "2024-06-14T15:50:14Z",
                        "updatedAt": "2024-06-14T15:50:15Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "irtaza9",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "did you run master and 2 replicas independently on 3 machines and then run sentinel on all 3 machines?\r\nif so please run the instances by following:\r\n```\r\ntaskset -c 0-1 redis-server  redis.conf\r\ntaskset -c 2-3 redis-server sentinel.conf\r\n``` ",
                        "createdAt": "2024-06-15T01:18:17Z",
                        "updatedAt": "2024-06-15T01:18:18Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Yes master with 2 replicas independently, I have read some article about taskset that when we run redis on multiple threads then we have to make sure that 1 thread of that CPU should remain free for other background works, What are your suggestions in this regards? Also Please go through to this [redis-doc](https://redis.io/docs/latest/operate/oss_and_stack/management/config-file/) providing some other configuration about the multiply instance of redis and other things.",
                        "createdAt": "2024-06-15T04:36:39Z",
                        "updatedAt": "2024-06-15T04:36:40Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "irtaza9",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "> Yes master with 2 replicas independently, I have read some article about taskset that when we run redis on multiple threads then we have to make sure that 1 thread of that CPU should remain free for other background works, What are your suggestions in this regards? Also Please go through to this [redis-doc](https://redis.io/docs/latest/operate/oss_and_stack/management/config-file/) providing some other configuration about the multiply instance of redis and other things.\n\nthat is all right, i just verify that if sentinel slows down redis instance.",
                        "createdAt": "2024-06-15T05:08:04Z",
                        "updatedAt": "2024-06-15T05:08:05Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Should I have to run these two commands on all instance? How can I verify that redis and sentinel is working on multiple threads? Like is there any command that gives all the details? Plus What Should I do after running these command to verify the performance or share logs with you?\r\n\r\n```\r\ntaskset -c 0-1 redis-server  redis.conf\r\ntaskset -c 2-3 redis-server sentinel.conf\r\n```",
                        "createdAt": "2024-06-15T05:22:59Z",
                        "updatedAt": "2024-06-15T05:26:18Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "irtaza9",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "you can use `taskset -pc `pgrep redis-server` to check it.\r\ndid you see the slowlog again after doing that?\r\nor do `redis-cli --intrinsic-latency 100` again.",
                        "createdAt": "2024-06-16T08:01:29Z",
                        "updatedAt": "2024-06-16T08:01:30Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "```\r\ntaskset -c 0-1 redis-server  redis.conf\r\ntaskset -c 2-3 redis-server sentinel.conf\r\n```\r\n\r\nI have used the above commands and here is the result. Now what should be the next step for better optimization. I have used this on all my three instance. Is there any other useful command who can share more related information about it?\r\n\r\n![Screenshot 2024-06-27 at 3 14 47 PM](https://github.com/redis/redis/assets/98938307/a2fa268d-e113-438c-91e9-a4e89feea216)\r\n",
                        "createdAt": "2024-06-27T10:18:04Z",
                        "updatedAt": "2024-06-27T10:18:05Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "irtaza9",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "did you see these slow logs again after doing that?",
                        "createdAt": "2024-06-27T10:24:32Z",
                        "updatedAt": "2024-06-27T10:24:32Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "One of my instance log report - 172.16.179.96\r\n\r\n```\r\nMax latency so far: 1 microseconds.\r\nMax latency so far: 38 microseconds.\r\nMax latency so far: 55 microseconds.\r\nMax latency so far: 74 microseconds.\r\nMax latency so far: 916 microseconds.\r\nMax latency so far: 918 microseconds.\r\nMax latency so far: 2136 microseconds.\r\nMax latency so far: 3903 microseconds.\r\nMax latency so far: 8137 microseconds.\r\nMax latency so far: 14715 microseconds.\r\nMax latency so far: 15160 microseconds.\r\nMax latency so far: 15946 microseconds.\r\n\r\n1705607517 total runs (avg latency: 0.0586 microseconds / 58.63 nanoseconds per run).\r\nWorst run took 271976x longer than the average latency.\r\n```",
                        "createdAt": "2024-06-27T11:14:27Z",
                        "updatedAt": "2024-06-27T11:15:13Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "irtaza9",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "One of my instance log report - 172.29.28.17\r\n\r\n```\r\nMax latency so far: 2 microseconds.\r\nMax latency so far: 3 microseconds.\r\nMax latency so far: 5 microseconds.\r\nMax latency so far: 6 microseconds.\r\nMax latency so far: 8 microseconds.\r\nMax latency so far: 9 microseconds.\r\nMax latency so far: 43 microseconds.\r\nMax latency so far: 119 microseconds.\r\nMax latency so far: 298 microseconds.\r\nMax latency so far: 382 microseconds.\r\nMax latency so far: 650 microseconds.\r\nMax latency so far: 2693 microseconds.\r\nMax latency so far: 3128 microseconds.\r\nMax latency so far: 3135 microseconds.\r\nMax latency so far: 3308 microseconds.\r\n\r\n84215798 total runs (avg latency: 1.1874 microseconds / 1187.43 nanoseconds per run).\r\nWorst run took 2786x longer than the average latency.\r\n```\r\n\r\n",
                        "createdAt": "2024-06-27T11:25:49Z",
                        "updatedAt": "2024-06-27T11:25:50Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "irtaza9",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "One of my instance log report - 172.29.22.8\r\n\r\n```\r\nMax latency so far: 3 microseconds.\r\nMax latency so far: 4 microseconds.\r\nMax latency so far: 10 microseconds.\r\nMax latency so far: 29 microseconds.\r\nMax latency so far: 75 microseconds.\r\nMax latency so far: 868 microseconds.\r\nMax latency so far: 1488 microseconds.\r\nMax latency so far: 1537 microseconds.\r\nMax latency so far: 1674 microseconds.\r\nMax latency so far: 5113 microseconds.\r\n\r\n57988005 total runs (avg latency: 1.7245 microseconds / 1724.49 nanoseconds per run).\r\nWorst run took 2965x longer than the average latency.\r\n```",
                        "createdAt": "2024-06-27T11:29:19Z",
                        "updatedAt": "2024-06-27T11:29:20Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "irtaza9",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@sundb yes these logs are after taskset config.",
                        "createdAt": "2024-06-27T11:30:29Z",
                        "updatedAt": "2024-06-27T11:30:31Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "irtaza9",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                },
                {
                  "body": "PFA, Why all the instances have different connected clients? How can we solve this or optimize the configurations?\r\n\r\n# Client list - 172.16.179.96\r\n\r\n<img width=\"1271\" alt=\"Screenshot 2024-06-15 at 11 13 23 AM\" src=\"https://github.com/redis/redis/assets/98938307/adf3bcda-8aa0-472f-a005-058c722ca4f6\">\r\n\r\n\r\n# Client list - 172.29.28.17\r\n\r\n<img width=\"1433\" alt=\"Screenshot 2024-06-15 at 11 21 57 AM\" src=\"https://github.com/redis/redis/assets/98938307/2352e197-8dc2-4501-acad-6eee3aec08d0\">\r\n\r\n# Client list - 172.29.22.8\r\n\r\n<img width=\"1436\" alt=\"Screenshot 2024-06-15 at 11 23 09 AM\" src=\"https://github.com/redis/redis/assets/98938307/e01e8a1b-1f03-4cfc-a406-eaeca568abff\">\r\n\r\n",
                  "createdAt": "2024-06-15T06:24:26Z",
                  "updatedAt": "2024-06-15T06:24:26Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "irtaza9",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "Hi @irtaza9,\r\nEach sentinel instance should maintain pubsub & cmd channel with each of Redis replicas. \r\nTo resolve which client related to which sentinel, each sentinel has its own unique ID (40 letters) that it prints on startup and a prefix of it being used to identify itself as connected as a client (sentinel-5040549b-cmd, etc).\r\n\r\nCan you check please the cpu consumption of each of the instances and distinct between sentinel and redis.\r\nPlease check also if getting down sentinels makes any difference.\r\n",
                        "createdAt": "2024-06-16T13:30:49Z",
                        "updatedAt": "2024-06-16T13:30:49Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "moticless",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Hello @moticless,\r\n\r\nSorry I was busy, here is the cpu consumption of 1 hour.\r\n\r\n![Screenshot 2024-06-27 at 3 30 51 PM](https://github.com/redis/redis/assets/98938307/1924c827-aaf4-4aaf-8c3b-ddfbdc56123f)\r\n![Screenshot 2024-06-27 at 3 30 28 PM](https://github.com/redis/redis/assets/98938307/a139d0ac-073e-445f-93a3-1e38bc2064b0)\r\n![Screenshot 2024-06-27 at 3 29 57 PM](https://github.com/redis/redis/assets/98938307/b67a7d0a-e2d3-4003-95cc-11e7b05392c3)\r\n\r\n",
                        "createdAt": "2024-06-27T10:32:53Z",
                        "updatedAt": "2024-06-27T10:32:53Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "irtaza9",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Redis CPU:\r\n\r\n![Screenshot 2024-06-27 at 3 34 53 PM](https://github.com/redis/redis/assets/98938307/351c7721-159b-4bf3-ae16-15d2dd17e173)\r\n![Screenshot 2024-06-27 at 3 34 40 PM](https://github.com/redis/redis/assets/98938307/12ce0456-0520-42a0-adc6-d95de2d0f0c0)\r\n![Screenshot 2024-06-27 at 3 34 23 PM](https://github.com/redis/redis/assets/98938307/48aaf4a4-aa0d-4359-8925-c81952fdc9d4)\r\n",
                        "createdAt": "2024-06-27T10:35:36Z",
                        "updatedAt": "2024-06-27T10:35:37Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "irtaza9",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@moticless I have stoped the sentinel service and the number of connected clients down to the half. It was max of 10 and after make the service down it goes to 5",
                        "createdAt": "2024-06-27T11:33:16Z",
                        "updatedAt": "2024-06-27T11:33:17Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "irtaza9",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13357",
            "number": 13357,
            "title": "Last read on given keys",
            "body": "Hello everyone, is there any way to track the most recent read/write operations on certain keys? \r\nWe have an issue where we write keys in Redis, but we're uncertain if anyone will read them. If no one is going to read the keys, we plan to terminate the application responsible for writing.\r\n",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-06-20T02:15:18Z",
            "updatedAt": "2024-06-24T01:33:44Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "lalamini",
              "__typename": "User"
            },
            "category": {
              "name": "General",
              "description": "Chat about anything and everything here",
              "slug": "general"
            },
            "isAnswered": null,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "you can use monitor to listen for keys, or keyspace notification if you're just listening for changes of keys.",
                  "createdAt": "2024-06-20T02:41:45Z",
                  "updatedAt": "2024-06-20T02:41:46Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "This could take a long time, perhaps a day or even a week, so we cannot use the Monitor command.\r\nMoreover, Keyspace notifications only inform me when a value is modified, but not when it is read.",
                  "createdAt": "2024-06-20T02:57:26Z",
                  "updatedAt": "2024-06-20T02:57:26Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "lalamini",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "yes, because in any case, you need to know that the key is being read and notified, and it necessarily comes with more overhead like monitor.\r\n",
                        "createdAt": "2024-06-20T03:22:40Z",
                        "updatedAt": "2024-06-20T03:22:41Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "Thanks for your reply, in product environment, we definitely can not use Monitor command.\r\nThat's why I am here to find a way to achieve the target.",
                        "createdAt": "2024-06-20T03:31:09Z",
                        "updatedAt": "2024-06-20T03:31:10Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "lalamini",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                },
                {
                  "body": "If there is no way to achieve the target, I can contribute the code. Like the slowlog, I will provide a command called accesslog. Users can set ten keys that we want Redis to record access with the mode 'r', 'w', or 'wr', and the recent access time.\r\nIs anyone interested in this?",
                  "createdAt": "2024-06-21T05:33:16Z",
                  "updatedAt": "2024-06-21T05:33:18Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "lalamini",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "IMHO, This feature doesn't make much sense, and i think we don't want to implement it for specific and rare scenarios.\r\nIIUC, you can create a key with expire time when touching it in your business code.\r\nand then you can check if the key exists to know if someone touched it.",
                        "createdAt": "2024-06-21T06:55:12Z",
                        "updatedAt": "2024-06-21T06:55:13Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "If everything goes as planned, everything will be fine. \r\nAnyway, thank you very much!",
                        "createdAt": "2024-06-24T01:33:44Z",
                        "updatedAt": "2024-06-24T01:33:45Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "lalamini",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13349",
            "number": 13349,
            "title": "Unexpected phenomenon when using CAS transaction",
            "body": "Hello everyone, I'm facing unexplainable problem when using CAS transaction, the redis transaction fails and return null message, but when I used `MONITOR` command and capture packet by wireshark, I didn't see any others updating in `watched key`.\r\nHere is the log of `MONITOR` command:\r\n```bash\r\n2024-06-15 14:27:12.811820 [0 172.24.0.1:58706] \"ZADD\" \"watched_key\" \"INCR\" \"-1\" \"127.0.0.1\"\r\n2024-06-15 14:27:12.816681 [0 172.24.0.1:58654] \"ZREM\" \"another_key\" \"127.0.0.1-d9cace95-2a26-4d94-aa24-1a1d2c381a82\"\r\n2024-06-15 14:27:12.823212 [0 172.24.0.1:58674] \"ZSCORE\" \"watched_key\" \"127.0.0.1\"\r\n2024-06-15 14:27:12.831299 [0 172.24.0.1:38920] \"EVALSHA\" \"7726c7be95e2a0ed082ec1da1e26b562f5c8903f\" \"1\" \"2:127.0.0.1-d9cace95-2a26-4d94-aa24-1a1d2c381a82\" \"\\x84\\xa2\\x9f\\x13\\x85\\x1c\\xe9\\\\\\xcc\\a\\x82D\\xa4\\x8a\\xd7B\\xa8\\xbfK\\xfd\\xe2\\x13\\xaaX\" \"1718461637799\"\r\n2024-06-15 14:27:12.831648 [0 lua] \"GET\" \"2:127.0.0.1-d9cace95-2a26-4d94-aa24-1a1d2c381a82\"\r\n2024-06-15 14:27:12.831702 [0 lua] \"DEL\" \"2:127.0.0.1-d9cace95-2a26-4d94-aa24-1a1d2c381a82\"\r\n2024-06-15 14:27:12.831804 [0 172.24.0.1:58650] \"EVALSHA\" \"7726c7be95e2a0ed082ec1da1e26b562f5c8903f\" \"1\" \"0:127.0.0.1-d9cace95-2a26-4d94-aa24-1a1d2c381a82\" \"\\x84\\xa2\\x9f\\x13\\x85\\x1c\\xe9\\\\\\xcc\\a\\x82D\\xa4\\x8a\\xd7B\\xa8\\xbfK\\xfd\\xe2\\x13\\xaaX\" \"1718461637799\"\r\n2024-06-15 14:27:12.831908 [0 lua] \"GET\" \"0:127.0.0.1-d9cace95-2a26-4d94-aa24-1a1d2c381a82\"\r\n2024-06-15 14:27:12.831932 [0 lua] \"DEL\" \"0:127.0.0.1-d9cace95-2a26-4d94-aa24-1a1d2c381a82\"\r\n2024-06-15 14:27:12.831993 [0 172.24.0.1:58650] \"EVALSHA\" \"7726c7be95e2a0ed082ec1da1e26b562f5c8903f\" \"1\" \"1:127.0.0.1-d9cace95-2a26-4d94-aa24-1a1d2c381a82\" \"\\x84\\xa2\\x9f\\x13\\x85\\x1c\\xe9\\\\\\xcc\\a\\x82D\\xa4\\x8a\\xd7B\\xa8\\xbfK\\xfd\\xe2\\x13\\xaaX\" \"1718461637799\"\r\n2024-06-15 14:27:12.832003 [0 lua] \"GET\" \"1:127.0.0.1-d9cace95-2a26-4d94-aa24-1a1d2c381a82\"\r\n2024-06-15 14:27:12.832006 [0 lua] \"DEL\" \"1:127.0.0.1-d9cace95-2a26-4d94-aa24-1a1d2c381a82\"\r\n2024-06-15 14:27:12.904419 [0 172.24.0.1:38900] \"ZSCORE\" \"watched_key\" \"127.0.0.1\"\r\n2024-06-15 14:27:12.905439 [0 172.24.0.1:38900] \"WATCH\" \"watched_key\"\r\n2024-06-15 14:27:12.906484 [0 172.24.0.1:38900] \"ZRANGEBYSCORE\" \"watched_key\" \"-inf\" \"9\" \"LIMIT\" \"0\" \"1\"\r\n2024-06-15 14:27:12.907474 [0 172.24.0.1:38900] \"ZSCORE\" \"watched_key\" \"127.0.0.1\"\r\n2024-06-15 14:27:12.908419 [0 172.24.0.1:38900] \"MULTI\"\r\n2024-06-15 14:27:12.909881 [0 172.24.0.1:38900] \"EXEC\"\r\n2024-06-15 14:27:12.910860 [0 172.24.0.1:38900] \"ZSCORE\" \"watched_key\" \"127.0.0.1\"\r\n2024-06-15 14:27:13.244464 [0 172.24.0.1:35638] \"keys\" \"turn/origin/*\"\r\n2024-06-15 14:27:13.244489 [0 172.24.0.1:35620] \"keys\" \"turn/origin/*\"\r\n2024-06-15 14:27:13.244900 [0 172.24.0.1:35634] \"keys\" \"turn/origin/*\"\r\n2024-06-15 14:27:13.244920 [0 172.24.0.1:35636] \"keys\" \"turn/origin/*\"\r\n2024-06-15 14:27:13.245474 [0 172.24.0.1:35630] \"keys\" \"turn/origin/*\"\r\n2024-06-15 14:27:13.804457 [0 172.24.0.1:58674] \"ZRANGEBYSCORE\" \"another_key\" \"-inf\" \"1718461633\" \"LIMIT\" \"0\" \"100\"\r\n```\r\nplease focus on `MULTI` and `EXEC` and there is no command between them.\r\nthe wireshark shows that command is `QUEUED` successfully.\r\n<img width=\"1619\" alt=\"image\" src=\"https://github.com/redis/rueidis/assets/58434052/610973fa-3af5-4bc0-9040-8522bb7783cb\">\r\n\r\nI'm implementing this function as follow (example):\r\n```go\r\n_ = redis.Dedicated(func(c rueidis.DedicatedClient) error {\r\n\t\t// watch key\r\n\t\tc.Do(ctx, c.B().Watch().Key(queueName).Build())\r\n\t\t// get instance\r\n\t\tredisResult := c.Do(ctx, c.B().Zrangebyscore().Key(queueName).Min(\"-inf\").\r\n\t\t\tMax(strconv.Itoa(9)).Limit(0, 1).Build())\r\n\t\tc.Do(ctx, c.B().Multi().Build())\r\n\t\tc.Do(ctx, c.B().Zadd().Key(queueName).Incr().ScoreMember().ScoreMember(1, \"127.0.0.1\").Build()\r\n\t\t_, err := c.Do(ctx, c.B().Exec().Build()).ToArray()\r\n\t\tif err != nil {\r\n\t\t\tfmt.Println(fmt.Errorf(\"error while increasing request count: %v\", err))\r\n\t\t}\r\n\t\treturn err\r\n\t})\r\n```\r\nIn my application, there is another goroutine which updates this value, but if this is the case, I think `MONITOR` command or wireshark must shows it. \r\nTo debug, I run a python script to print value of `watched key` each 0.001 second and it didn't change while transaction was excuted.\r\n\r\nThank you. 🙏🙏🙏",
            "upvoteCount": 2,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": "RESOLVED",
            "createdAt": "2024-06-17T15:30:37Z",
            "updatedAt": "2024-06-26T01:57:21Z",
            "closedAt": "2024-06-26T01:57:21Z",
            "closed": true,
            "author": {
              "login": "dntam00",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "I could use retry strategy to run the transaction again if I receive an empty response from redis, but it's like a workaround, I really want to understand why redis doens't execute transaction. ",
                  "createdAt": "2024-06-17T15:32:54Z",
                  "updatedAt": "2024-06-17T15:33:38Z",
                  "authorAssociation": "NONE",
                  "author": {
                    "login": "dntam00",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                },
                {
                  "body": "@dangngoctam00 what version are you using?\r\ncan you check the return of\r\n```\r\n\t\tc.Do(ctx, c.B().Multi().Build())\r\n\t\tc.Do(ctx, c.B().Zadd().Key(queueName).Incr().ScoreMember().ScoreMember(1, \"127.0.0.1\").Build()\r\n```\r\nyou can't see them in the monitor if they fails.",
                  "createdAt": "2024-06-18T02:02:11Z",
                  "updatedAt": "2024-06-18T02:02:12Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "hello @sundb , \r\n- Redis version: 7.2.4\r\n- Response of `MULTI` is `OK`, response of `ZADD` is `QUEUED`, response of transaction is `_\\r\\n`, I checked these value through wireshark. \r\n\r\nWhat I'm thinking is this transaction fails because the value of `watched key` was changed, but then it should show in result of `MONITOR` command. ",
                        "createdAt": "2024-06-18T02:34:22Z",
                        "updatedAt": "2024-06-18T02:36:08Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "dntam00",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "are wireshark and monitor simultaneous? Or did you start all over again with wireshark after exec returned nil?",
                        "createdAt": "2024-06-18T02:45:59Z",
                        "updatedAt": "2024-06-18T02:46:00Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@sundb I started wireshark and monitor at the beginning of the test and it's simultaneous, then I mapped the result of these 2 tool by timestamp. I'm in GMT+7 so you will see the timestamp in MONITOR and wireshark differ 7 hours.",
                        "createdAt": "2024-06-18T02:50:44Z",
                        "updatedAt": "2024-06-18T02:57:28Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "dntam00",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "hello @sundb  , I think I have to `UNWATCH` this key to release dedicated connection, excerpted from documentation of redis: https://redis.io/docs/latest/develop/interact/transactions/#watch-explained\r\n<img width=\"998\" alt=\"image\" src=\"https://github.com/redis/rueidis/assets/58434052/9aab0236-3019-4935-afb5-b92d12df33ae\">\r\nit seems like I've misunderstood this sentence:\r\n> Simply all the [WATCH](https://redis.io/docs/latest/commands/watch/) calls will have the effects to watch for changes starting from the call, up to the moment [EXEC](https://redis.io/docs/latest/commands/exec/) is called. You can also send any number of keys to a single [WATCH](https://redis.io/docs/latest/commands/watch/) call\r\n\r\nI supposed `WATCH` would reset the state of key being watched, but it does not because they are 2 different transaction using same connection. \r\nThere is the case I didn't call `EXEC`, my mistake is providing wrong code snippet, sorry 🙏🙏🙏. ",
                        "createdAt": "2024-06-22T07:10:49Z",
                        "updatedAt": "2024-06-22T08:01:14Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "dntam00",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13334",
            "number": 13334,
            "title": "How to monitor detailed command operations initiated by clients connected to Redis starting from Redis itself",
            "body": "By the now ,we often use command:client list and command:slowlog to monitor operations initiated by other clients,but specific client operations cannot be monitored.If the operation initiated by the client does not exceed the slow log limit, the one causing the problem cannot be found.Although command：monitor can show specific operations, it also increases the burden on Redis and has a performance impact. \r\nSo is there a better way to monitor specific client operations?",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-06-11T02:29:15Z",
            "updatedAt": "2024-06-11T02:38:57Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "tentosleep",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "@tentosleep Redis doesn't support recording slowlog for individual users yet, maybe you can get these from a third-party tools.\r\nif a user's command delay doesn't exceed the slow log limit, why should me record it?",
                  "createdAt": "2024-06-11T02:38:56Z",
                  "updatedAt": "2024-06-11T02:38:57Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13332",
            "number": 13332,
            "title": "Any way to replicate redis-cli's cluster create behavior?",
            "body": "Hello,\r\nI'm trying to replicate the `redis-cli --cluster create 127.0.0.1:55432 127.0.0.1:55433 127.0.0.1:55434 --cluster-yes`\r\n(`clusterManagerCommandCreate`) without actually using the CLI. Could anyone tell me if it's achievable only by running commands on the Redis nodes? Thank you!",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-06-10T14:49:45Z",
            "updatedAt": "2024-06-10T14:49:46Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "jirimoravcik",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": []
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13317",
            "number": 13317,
            "title": "Does `GEOSEARCH` take into account Earth's curvature?",
            "body": "Couldn't find this information in the documentations. As title suggested, does `GEOSEARCH` `BYBOX` and `BYRADIUS` take into account Earth's curvature?",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-06-03T20:48:42Z",
            "updatedAt": "2024-06-05T00:58:33Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "feelingsonice",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "AFAIK, the answer is no.\r\nRedis uses `gethash` algorithm, which treats the Earth as a two-dimensional plane.",
                  "createdAt": "2024-06-04T02:42:14Z",
                  "updatedAt": "2024-06-04T02:42:14Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": [
                      {
                        "body": "I found this article https://redis.io/blog/redis-7-geographic-commands/\n\nIn it they very loosely mentioned that the distance calculation uses Haversine, which IIUC,takes into account the curvature?\n\n> Most commands to query Redis Geospatial Indices calculate the distance between two coordinates, so it makes sense to examine how that’s done algorithmically. The Haversine distance is a useful measure, as it takes into account the curvature of the Earth, with more accurate results than a Euclidean distance calculation. \n\nNot sure whether if this implies that the GEOSEARCH queries does the same",
                        "createdAt": "2024-06-04T17:09:02Z",
                        "updatedAt": "2024-06-04T17:09:03Z",
                        "authorAssociation": "NONE",
                        "author": {
                          "login": "feelingsonice",
                          "__typename": "User"
                        }
                      },
                      {
                        "body": "@feelingsonice ooh, you're right, sorry for mistake response.\r\nRedis uses haversin great circle distance formula to calculate distance.",
                        "createdAt": "2024-06-05T00:58:33Z",
                        "updatedAt": "2024-06-05T00:58:33Z",
                        "authorAssociation": "COLLABORATOR",
                        "author": {
                          "login": "sundb",
                          "__typename": "User"
                        }
                      }
                    ]
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13318",
            "number": 13318,
            "title": "Is `GEOSEARCH` `BYBOX` faster than `BYRADIUS`?",
            "body": "My understanding is that `BYRADIUS` is first querying for a bounding box, then doing an additional check on each member to make sure it's within the radius? Is this true? If so, does it imply that `BYBOX` is generally faster than `BYRADIUS`?",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-06-03T22:18:41Z",
            "updatedAt": "2024-06-04T02:39:18Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "feelingsonice",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": [
                {
                  "body": "in theory `BYBOX` will be faster than `BYRADIUS` with same data set.",
                  "createdAt": "2024-06-04T02:39:18Z",
                  "updatedAt": "2024-06-04T02:39:18Z",
                  "authorAssociation": "COLLABORATOR",
                  "author": {
                    "login": "sundb",
                    "__typename": "User"
                  },
                  "replies": {
                    "nodes": []
                  }
                }
              ]
            },
            "labels": {
              "nodes": []
            }
          },
          {
            "url": "https://github.com/redis/redis/discussions/13305",
            "number": 13305,
            "title": "zunionstore optimization in redis ranking accross multiple keys",
            "body": "I am using code [1] to get rankings accross several keys [2] with values [3] in an app [4]. In short, those are rankings for several quizzes for specific clients. Each ranking can contain millions of entries. Thus, I am wondering whether the \"zunionstore\" can be avoid somehow. E.g. I need to calculate rankings accross all quizzes, so I am using zunionstore to sum all rankings into one combined ranking. Then it is easy to apply rank function to get the user actual rank.\r\n\r\nUnfortunately this takes about 0.5 seconds for every million users. So it is the bottleneck here.\r\n\r\nAny suggestions for other way to apply such ranking in redis?\r\n\r\n[1]\r\n\r\n```java\r\nimport io.vertx.core.CompositeFuture;\r\nimport io.vertx.core.Future;\r\nimport io.vertx.core.Promise;\r\nimport io.vertx.redis.client.RedisAPI;\r\nimport io.vertx.redis.client.Response;\r\nimport jakarta.enterprise.context.ApplicationScoped;\r\nimport jakarta.inject.Inject;\r\nimport java.util.ArrayList;\r\nimport java.util.Arrays;\r\nimport java.util.HashMap;\r\nimport java.util.LinkedHashMap;\r\nimport java.util.List;\r\nimport java.util.Map;\r\nimport java.util.UUID;\r\nimport java.util.concurrent.TimeUnit;\r\nimport java.util.concurrent.atomic.AtomicLong;\r\nimport java.util.stream.Collectors;\r\nimport org.jboss.logging.Logger;\r\n\r\n@ApplicationScoped\r\npublic class RankingService {\r\n\r\n  private static final Logger LOGGER = Logger.getLogger(RankingService.class);\r\n\r\n  @Inject RedisAPI redisAPI;\r\n\r\n\r\n  public Future<Map<String, Map<String, Object>>> getCombinedRanking(\r\n      String clientId, List<String> quizIds, int limit, int offset) {\r\n    Promise<Map<String, Map<String, Object>>> promise = Promise.promise();\r\n    String tempCombinedKey =\r\n        clientId + \":temp:combined:ranking:\" + UUID.randomUUID();\r\n    List<String> keys = new ArrayList<>();\r\n    for (String quizId : quizIds) {\r\n      keys.add(clientId + \":quiz:\" + quizId + \":ranking\");\r\n    }\r\n\r\n    List<String> zunionstoreArgs = new ArrayList<>();\r\n    zunionstoreArgs.add(tempCombinedKey);\r\n    zunionstoreArgs.add(String.valueOf(keys.size()));\r\n    zunionstoreArgs.addAll(keys);\r\n    zunionstoreArgs.add(\"AGGREGATE\");\r\n    zunionstoreArgs.add(\"SUM\");\r\n\r\n    AtomicLong startTime = new AtomicLong(System.nanoTime());\r\n\r\n    redisAPI\r\n        .zunionstore(zunionstoreArgs)\r\n        .onComplete(\r\n            ar -> {\r\n              if (ar.succeeded()) {\r\n                long endTime = System.nanoTime();\r\n                long duration = TimeUnit.NANOSECONDS.toMillis(endTime - startTime.get());\r\n                LOGGER.infof(\"zunionstore command took %d ms\", duration);\r\n\r\n                startTime.set(System.nanoTime());\r\n                redisAPI\r\n                    .zrevrange(\r\n                        Arrays.asList(\r\n                            tempCombinedKey,\r\n                            String.valueOf(offset),\r\n                            String.valueOf(offset + limit - 1),\r\n                            \"WITHSCORES\"))\r\n                    .onComplete(\r\n                        ar2 -> {\r\n                          if (ar2.succeeded()) {\r\n                            long endTime2 = System.nanoTime();\r\n                            long duration2 =\r\n                                TimeUnit.NANOSECONDS.toMillis(endTime2 - startTime.get());\r\n                            LOGGER.infof(\"zrevrange command took %d ms\", duration2);\r\n\r\n                            Response response = ar2.result();\r\n                            LOGGER.infof(\"zrevrange response: %s\", response.toString());\r\n                            Map<String, Map<String, Object>> combinedScores = new LinkedHashMap<>();\r\n                            List<Future> rankFutures = new ArrayList<>();\r\n\r\n                            for (int i = 0; i < response.size(); i++) {\r\n                              Response pair = response.get(i);\r\n                              if (pair.size() < 2) {\r\n                                LOGGER.warnf(\"Invalid pair in response: %s\", pair);\r\n                                continue;\r\n                              }\r\n                              String userId = pair.get(0).toString();\r\n                              String scoreString = pair.get(1).toString();\r\n\r\n                              LOGGER.infof(\"User ID: %s, Score String: %s\", userId, scoreString);\r\n\r\n                              try {\r\n                                Double score = Double.parseDouble(scoreString);\r\n                                Map<String, Object> scoreDetails = new HashMap<>();\r\n                                scoreDetails.put(\"score\", score);\r\n                                combinedScores.put(userId, scoreDetails);\r\n\r\n                                Future<Response> rankFuture =\r\n                                    redisAPI.zcount(tempCombinedKey, \"(\" + score, \"+inf\");\r\n                                rankFutures.add(\r\n                                    rankFuture\r\n                                        .onSuccess(\r\n                                            rankResponse -> {\r\n                                              Long rank =\r\n                                                  rankResponse.toLong()\r\n                                                      + 1;\r\n                                              scoreDetails.put(\"rank\", rank);\r\n                                            })\r\n                                        .onFailure(\r\n                                            err -> {\r\n                                              LOGGER.errorf(\r\n                                                  \"Error fetching rank for user %s: %s\",\r\n                                                  userId, err);\r\n                                            }));\r\n                              } catch (NumberFormatException e) {\r\n                                LOGGER.errorf(\r\n                                    \"Error parsing score for user %s: %s\", userId, scoreString, e);\r\n                              }\r\n                            }\r\n\r\n                            CompositeFuture.all(rankFutures)\r\n                                .onComplete(\r\n                                    ar3 -> {\r\n                                      if (ar3.succeeded()) {\r\n                                        long processingEndTime = System.nanoTime();\r\n                                        long processingDuration =\r\n                                            TimeUnit.NANOSECONDS.toMillis(\r\n                                                processingEndTime - startTime.get());\r\n                                        LOGGER.infof(\r\n                                            \"Processing combined ranking took %d ms\",\r\n                                            processingDuration);\r\n\r\n                                        promise.complete(combinedScores);\r\n                                      } else {\r\n                                        promise.fail(ar3.cause());\r\n                                      }\r\n\r\n                                      long cleanupStartTime = System.nanoTime();\r\n                                      redisAPI\r\n                                          .del(Arrays.asList(tempCombinedKey))\r\n                                          .onComplete(\r\n                                              delAr -> {\r\n                                                if (delAr.succeeded()) {\r\n                                                  long cleanupEndTime = System.nanoTime();\r\n                                                  long cleanupDuration =\r\n                                                      TimeUnit.NANOSECONDS.toMillis(\r\n                                                          cleanupEndTime - cleanupStartTime);\r\n                                                  LOGGER.infof(\r\n                                                      \"Cleanup (del) command took %d ms\",\r\n                                                      cleanupDuration);\r\n                                                } else {\r\n                                                  LOGGER.errorf(\r\n                                                      \"Failed to delete temporary key %s: %s\",\r\n                                                      tempCombinedKey, delAr.cause());\r\n                                                }\r\n                                              });\r\n                                    });\r\n                          } else {\r\n                            promise.fail(ar2.cause());\r\n                          }\r\n                        });\r\n              } else {\r\n                promise.fail(ar.cause());\r\n              }\r\n            });\r\n\r\n    return promise.future();\r\n  }\r\n}\r\n```\r\n\r\n\r\n[2]\r\nclient1:quiz:quiz_1:ranking\r\nclient1:quiz:quiz_2:ranking\r\n...\r\n\r\n[3]\r\n```json\r\n[\r\n  {\r\n    \"score\" : 0,\r\n    \"content\" : \"user_0\"\r\n  },\r\n  {\r\n    \"score\" : 10,\r\n    \"content\" : \"user_1\"\r\n  },\r\n  {\r\n    \"score\" : 20,\r\n    \"content\" : \"user_2\"\r\n  },\r\n  {\r\n    \"score\" : 20,\r\n    \"content\" : \"user_3\"\r\n  }\r\n  ...\r\n  ]\r\n```\r\n  \r\n  \r\n  [4]\r\n  Quarkus app with redis (vertx-redis-client-4.5.4).\r\n  \r\n",
            "upvoteCount": 1,
            "authorAssociation": "NONE",
            "locked": false,
            "activeLockReason": null,
            "stateReason": null,
            "createdAt": "2024-05-30T10:38:38Z",
            "updatedAt": "2024-05-30T10:38:38Z",
            "closedAt": null,
            "closed": false,
            "author": {
              "login": "jshandorov1",
              "__typename": "User"
            },
            "category": {
              "name": "Q&A",
              "description": "Ask the community for help",
              "slug": "q-a"
            },
            "isAnswered": false,
            "answerChosenAt": null,
            "answerChosenBy": null,
            "comments": {
              "nodes": []
            },
            "labels": {
              "nodes": []
            }
          }
        ]
      }
    }
  }
}